\documentclass[14pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}

% Required to change the page size to A4
\usepackage[a4paper, top=0.9in, right=0.8in, left=0.8in, bottom=0.9in]{geometry}

% use a different font than the default
%\usepackage{fouriernc}

% increase paragraph spacing
\setlength{\parskip}{1.3em}


\usepackage{amssymb,amsmath,amsthm}
\usepackage{accents}
\usepackage{thmtools}
\usepackage{siunitx}
\usepackage{longtable,booktabs}
\usepackage{centernot}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{nameref,hyperref,cleveref}
\usepackage[dvipsnames]{xcolor}

% Define theorem enviroments
% TODO: change names to match your language
\declaretheoremstyle[
	spaceabove=2em,
	spacebelow=2em,
	headindent=2em,
    notefont=\normalfont\bfseries,
    bodyfont=\normalfont
]{mytheorem}

\declaretheorem[name=Teorema,
refname={teorema,teoremas},
Refname={Teorema,Teoremas},
style=mytheorem]{tm}

\declaretheorem[name=Lema,
refname={lema,lemas},
Refname={Lema,Lemas},
sibling=tm,
style=mytheorem]{lm}

\declaretheorem[name=Definici\'{o}n,
style=mytheorem,
shaded={bgcolor={rgb}{0.9,0.9,0.9}, margin=1em}]{dfn}

\declaretheorem[name=Observaci\'{o}n,
numbered=no]{obs}

\declaretheorem[name=Proposici\'{o}n]{pro}

\declaretheorem[name=Ejemplo,style=mytheorem]{ej}
\DeclareMathOperator{\Ima}{Im}

\begin{document}
	\title{Apuntes de Álgebra Lineal y Geometría}
	\author{Elias Hernandis Prieto}
	\maketitle

\chapter{Espacios vectoriales eucl\'{i}deos y unitarios}

\section{Formas bilineales, simétricas y definidas positivas}

\begin{dfn}[Forma bilineal]
	Una forma bilineal en $V$ es una función $\varphi:V \times V \to \mathbb{R}$ que es lineal en cada uno de sus argumentos.
	\begin{enumerate}
		\item $\forall u, v, w \in V,\ \varphi(u + v, w) = \varphi(u, w) + \varphi(v, w) \land \varphi(u, v + w) = \varphi(u, v) + \varphi(u, w)$.
		\item $\forall u, v \in V, \alpha \in \mathbb{R},\ \varphi(\alpha u, v) = \alpha\varphi(u, v) \land \varphi(u, \alpha v) = \alpha \varphi(u, v)$.
	\end{enumerate}
\end{dfn}

\begin{tm}[Formas bilineales y matrices]
	Sea $V$ un e.v. sobre $K$ y sea $B=\{v_1, \dots, v_n\}$ una base de $V$. Sea $\varphi: V\times V \to \mathbb{R}$ una forma bilineal. Entonces existe una matriz $A \in \mathbb{M}_n (\mathbb{R})$ tal que $\varphi(u, v) = [u]_B^T A [v]_B$. Además, la matriz se construye de la siguiente manera:
	\[
		M_B(\mathbb{\varphi}) = A = \left(\begin{array}{ccc}
		\varphi(v_1, v_1) & \dots & \varphi(v_1, v_n)\\
		\dots & \ddots & \vdots \\
		\varphi(v_n, v_1) & \dots & \varphi(v_n, v_n)
		\end{array}\right)
	\]
	También es cierto el recíproco. Dada una matriz $A \in \mathbb{M}_n(\mathbb{R})$, la función $\varphi : V \times V \to \mathbb{R},\ (u, w) \mapsto [u]_B^T A [v]_B$ es una forma bilineal.
\end{tm}

\begin{tm}[Fórmula del cambio de base para una forma bilineal]
	Sea $V$ un e.v. con bases $B = \{v_1, \dots, v_n\}$ y $B' = \{v_1', \dots, v_n'\}$. Sea $\varphi: V \times V \to \mathbb{R}$ una forma bilineal. Sean $M_B(\varphi)$ y $M_{B'}(\varphi)$ las matrices de la forma bilineal respecto de cada base. Entonces:
	\[
		M_{B'}(\varphi) = M_{BB'}^T \cdot M_B(\varphi) \cdot M_{BB'}
	\]
	donde $M_{BB'}$ es la matriz de cambio de base entre $B$ y $B'$.
\end{tm}

\begin{dfn}[Forma bilineal simétrica]
	Una forma bilineal $\varphi : V \times V \to \mathbb{R}$ es simétrica si $\forall u, v \in V,\ \varphi(u, v) = \varphi(v, u)$.
\end{dfn}

\begin{tm}
	Sea $\varphi : V\times V \to \mathbb{R}$ una forma bilineal, y sea $B = \{v_1, \dots, v_n\}$ una base de $V$. Entonces $\varphi$ es simétrica si y solo si $M_B(\varphi)$ es simétrica.
\end{tm}

\begin{dfn}[Forma bilineal definida positiva]
	Sea $\varphi : V \times V \to \mathbb{R}$ una forma bilineal. Diremos que es definida positiva si $\forall u \in V,\ \varphi(u, u) \geq 0 \land \varphi(u, u) = 0 \iff u = \overrightarrow{0}$.
\end{dfn}

\begin{tm}[Criterio de Sylvester]
	Sea $\varphi: V \times V \to \mathbb{R}$ una forma bilineal \textbf{simétrica}. Sea $B$ una base de $V$ y sea $M_B (\varphi)$ la matriz asociada a $\varphi$. Para $r = 1 \dots n$, definimos la submatriz $M_B(\varphi)_r$ formada por las $r$ primeras filas y las $r$ primeras columnas.
	
	Entonces $\varphi$ es definida positiva $\iff\ \forall r = 1\dots n, \det(M_B(\varphi)_r) > 0$. 
\end{tm}

\section{Normas y productos escalares.}
\begin{dfn}[Producto escalar]
	Sea $V$ un e.v. sobre $\mathbb{R}$. un producto escalar es una forma bilineal, simétrica y definida positiva.
\end{dfn}

\begin{dfn}[Espacio vectorial euclídeo]
	$V$ es un e.v. euclídeo si hay un producto escalar definido en $V$.
\end{dfn}

\begin{dfn}[Norma]
	Dado $V$ un e.v. sobre $K = \mathbb{R, C}$, con $\text{dim}_K V < \infty$. Una función $\rho : V \to \mathbb{R}$ se dice norma si cumple:
	\begin{enumerate}
		\item $\forall u \in V,\ \rho(u) \geq 0$ y además $\rho(u) = 0 \iff u = \overrightarrow{0}$.
		\item $\forall u \in V, \alpha \in  K,\ \rho(\alpha u) = |\alpha|\rho(u)$.
		\item $\forall u, v \in V,\ \rho(u + v) \leq \rho(u) + \rho(v)$.
	\end{enumerate}
\end{dfn}

\begin{tm}
	Sea $\varphi : V\times V \to \mathbb{R}$ un producto escalar. Entonces $\rho : V \to \mathbb{R},\ u \mapsto +\sqrt{\varphi(u, u)}$ es una norma en $V$.
\end{tm}
Para demostrar la tercera propiedad de norma en el teorema anterior utilizamos la desigualdad de Cauchy-Schwarz, $|\varphi(u, v)|^2 \leq \varphi(u,u)\varphi(v, v)$.


\section{Formas sesquilineales y hermíticas}

\begin{dfn}[Forma sesquilineal]
	Sea $V$ un e.v. sobre $\mathbb{C}$. Diremos que $\varphi: V \times V \to \mathbb{C}$ es sesquilineal si
	\begin{enumerate}
		\item $\forall u, v, w \in V,\ \varphi(u + v, w) = \varphi(u, w) + \varphi(v, w) \land \varphi(u, v + w) = \varphi(u, v) + \varphi(u, w)$.
		\item $\forall u, v \in V, \alpha \in \mathbb{C},\ \varphi(\alpha u, v) = \alpha\varphi(u, v) \land \varphi(u, \alpha v) = \bar{\alpha} \varphi(u, v)$.
	\end{enumerate}
	\textbf{Atención:} el escalar sale conjugado si aparece en el segundo argumento.
\end{dfn}

\begin{tm}
	Sea $V$ un e.v. sobre $\mathbb{C}$, $B = \{v_1, \dots, v_n\}$ una base de $V$ y $A \in \mathbb{M}_n(\mathbb{C})$. Entonces la función $\varphi : V \times V \to \mathbb{C},\ (u, v) \mapsto [u]_B^T A \overline{[v]_B}$ es una forma sesquilineal.\footnote{Esto es, tomar las cordenadas de $v$ respecto de $B$ y luego hacer el conjugado.} El recíproco también se cumple, como en el caso real.
\end{tm}

\begin{tm}[Fórmula del cambio de base para una forma sesquilineal]
	Sea $V$ e.v. sobre $\mathbb{C}$, con bases $B = \{v_1, \dots, v_n\}$ y $B' = \{v_1', \dots v_n'\}$. Sea $\varphi$ una forma sesquilineal con matrices $M_B(\varphi)$ y $M_{B'}(\varphi)$ con respecto de cada base. Entonces:
	\[
		M_{B'}(\varphi) = M_{BB'}^T \cdot M_B(\varphi) \cdot \overline{M_{BB'}}
	\]
	donde $M_{BB'}$ es la matriz de cambio de base entre $B$ y $B'$.
\end{tm}

\begin{dfn}[Forma hermítica]
	Sea $\varphi : V \times V \to \mathbb{C}$ una forma sesquilineal. Diremos que $\varphi$ es hermítica si $\forall u,v \in V,\ \varphi(u, v) = \overline{\varphi(v, u)}$.
	
	Diremos que una matriz $A \in \mathbb{M}_n(\mathbb{C})$ es hermítica si $A^T = \overline{A}$ (i.e. $A = \overline{A^T}$). En una matriz hermítica, los coeficientes de la diagonal son todos reales.
\end{dfn}

\begin{tm}
	Sea $B$ una base del e.v. $V$. Sea $\varphi$ una forma sesquilineal. Entonces $\varphi$ es hermítica si y solo si $M_B(\varphi)$ es hermítica.
\end{tm}

En e.v. sobre $\mathbb{C}$ también vale el criterio de Sylvester para averiguar si una forma hermítica es definida positiva.

\begin{dfn}[Espacio vectorial unitario]
	Decimos que $V$ e.v. sobre $\mathbb{C}$ es unitario si existe un producto escalar definido en $V$.
\end{dfn}

\section{Bases ortogonales y ortonormales}

Fijamos $V$ e.v. sobre $K = \mathbb{R, C}$ y un producto escalar $\varphi : V \times V \to K$.

\begin{dfn}[overrightarrowtor unitario]
	diremos que $u \in V$ es unitario si $\varphi(u, u) = 1$.
\end{dfn}

\begin{dfn}[overrightarrowtores ortogonales]
	Dos overrightarrowtores $u, v \in V$ son ortogonales si $\varphi(u, v) = 0$. Dos observaciones:
	\begin{enumerate}
		\item $\overrightarrow{0} \perp u,\ \forall u$
		\item Si existe un $u$ tal que $u \perp v,\ \forall v$, entonces necesariamente $u = \overrightarrow{0}$.
	\end{enumerate}
\end{dfn}

\begin{tm}
	Sean $v_1, \dots v_k \in V$ overrightarrowtores no nulos ortogonales dos a dos ($v_i \perp v_j$ para $i≠j$). Entonces $v_1, \dots v_k$ son linealmente independientes.
\end{tm}

\begin{dfn}[Base ortogonal y ortonormal]
	Diremos que una base de $V$, $B = \{v_1, \dots, v_n\}$ es ortogonal si $v_i \perp v_j$ para $i≠j$. Si además $\lVert v_i \rVert = 1,\ i = 1 \dots n$, diremos que es una base ortonormal (BON).
\end{dfn}

\begin{tm}
	Sea $B = \{v_1, \dots, v_n\}$ una base de $V$. $B$ es ortonormal si y solo si $M_B (\varphi) = I_n$ (si la matriz del producto escalar respecto de $B$ es la identidad).
	
	También al revés: dada $B' = \{w_1, \dots w_n\}$ podemos encontrar $\varphi$ tal que $B'$ es una BON. Basta definir $M_{B'}(\varphi)$ como la identidad. Dicha $\varphi$ es única.
\end{tm}

\begin{tm}[Gram-Schmidt]
	Sea $V$ un e.v. de dimensión $n$ sobre un cuerpo $K$. Sea $\varphi$ un producto escalar. Entonces existe al menos una base de $V$ que es ortonormal respecto a $\varphi$.
\end{tm}

\begin{proof} \textbf{Método de Gram-Schmidt.}
	\begin{enumerate}
		\item Sea $B = \{v_1, \dots, v_n\}$ una base cualquiera de $V$. Definimos $F_1 = \langle v_1\rangle\subsetneq F_2 = \langle v_1, v_2\rangle \subsetneq \dots \subsetneq F_r = \langle v_1, \dots, v_r\rangle$ donde $r < n$.
		\item Sea $\tilde{w}_1 = \frac{v_1}{\lVert v_1\rVert_\varphi}$. Se verifica que $F_1 = \langle v_1\rangle = \langle\tilde{w}_1\rangle$.
		\item Obtendremos $w_2$ a partir de $\tilde{w}_1$: $w_2 = v_2 - \lambda \tilde{w}_1$ tal que $w_2 \perp \tilde{w}_1$. Para ello tiene que darse que $0 = \varphi(w_2, \tilde{w}_1) = \varphi(v_2 - \lambda \tilde{w}_1, \tilde{w}_1) = \varphi(v_2, \varphi{w}_1) - \lambda\varphi(\tilde{w}_1, \tilde{w}_1)$. Tomamos $\lambda = \frac{\varphi(v_2, \tilde{w}_1)}{\varphi(\tilde{w}_1, \tilde{w}_1)}$ y $\tilde{w}_2 = \frac{w_2}{\lVert w_2\rVert_\varphi}$ y queda que $F_2 = \langle v_1, v_2\rangle = \langle \tilde{w}_1, v_2\rangle = \langle \tilde{w}_1, \tilde{w}_2\rangle$.
		\item Supongamos que hemos construido $\tilde{w}_1, \dots, \tilde{w}_r$. Entonces ya tenemos $F_r = \langle \tilde{w}_1, \dots, \tilde{w}_r\rangle$. Para construir $\tilde{w}_{r+1}$ haremos lo mismo. $w_{r+1} = v_{r+1}- \lambda_1\tilde{w}_1 - \dots - \lambda_r\tilde{w}_r$. Obtenemos cada $\lambda_i = \frac{\varphi(v_{r+1}, \tilde{w}_i)}{\varphi(\tilde{w}_i, \tilde{w}_1)}$ y normalizamos para obtener $\tilde{w}_{r+1}$.
		\item  Continuamos hasta que hayamos agotado los overrightarrowtores de $B$. Entonces habremos obtenido la nueva base $B' = \{\tilde{w}_1, \dots, \tilde{w}_n\}$ que es ortonormal por construcción.
	\end{enumerate}
\end{proof}

\section{Aplicaciones adjuntas}
\begin{dfn}[Aplicación adjunta]
	Sean $f, g: V \to V$ aplicaciones lineales. Decimos que $g$ es la adjunta de $f$ si $\forall u, v \in V,\ \varphi(f(u), v) = \varphi(u, g(v))$. Observación: si $g$ es la adjunta de $f$, entonces $f$ es la adjunta de $g$. Denotamos la adjunta como $\tilde{f}$.
\end{dfn}
\begin{tm}[Matrices de adjuntas en BONs]
	Sea $B$ una BON de $V$, $f,\tilde{f}:V \to V$ lineales tal que $\tilde{f}$ es la adjunta de $f$. Entonces $M_B(f) = M_B(\tilde{f})^T$. Si $B$ no es una BON, no tiene por que ocurrir lo anterior.
\end{tm}
\begin{dfn}[Aplicación autoadjunta]
	Sea $f:V \to V$ lineal. $f$ es autoadjunta si $f = \tilde{f}$.
\end{dfn}
\begin{pro}[Existencia y unicidad de la adjunta. Propiedades.]
	Sea $f:V \to V$ lineal. Entonces existe $\tilde{f}$ y es única. Además se verifica lo siguiente:
\end{pro}
\begin{enumerate}
	\item La aplicación identidad es autoadjunta: $\tilde{Id} = Id$.
	\item La adjunta de la adjunta es la aplicación original: $\tilde{\tilde{f}} = f$.
	\item $\widetilde{f + g} = \tilde{f} + \tilde{g}$.
	\item $\widehat{f \circ g} = \tilde{g} \circ \tilde{f}$.
	\item $\widetilde{(f^{-1})} = (\tilde{f})^{-1}$.
\end{enumerate}

\begin{pro}
	Sea $f: V \to V$ lineal. Entonces $\ker f = (\Ima\tilde{f})^\perp$ y $\Ima f = (\ker \tilde{f})^\perp$.
\end{pro}

\begin{pro}
	Sea $f:V \to V$ autoadjunta y sea $W \in V$ un subespacion invariante por $f$ ($f(W) \in W$). Entonces $W^\perp$ es invariante por $f$.
\end{pro}

\begin{pro}
	Sea $f: V \to V$ autoadjunta en $V$ e.v. sobre $K = \mathbb{R, C}$ de $\dim V < \infty$. Entonces todos los autovalores de $f$ son reales.
\end{pro}

\begin{tm}[Diagonalizabilidad de una autoadjunta]
	Sea $V$ un e.v. sobre $K = \mathbb{R, C}$ de dimensión finita. Entonces cualquier $f:V \to V$ autoadjunta es diagonalizable en una BON.
	
	\textbf{Corolario:} cualquier matriz real y simétrica (o compleja y hermítica) es diagonalizable en una BON.
\end{tm}

\section{Proyecciones}
%TODO: esta sección es una mierda

\begin{dfn}[Proyección]
	Una aplicación lineal $f: V \to V$ es una proyección $\iff \ f^2 = f$. Es decir, si $M_B(f) = M_B(f)^2$.
\end{dfn}

\begin{pro}[Clasificación de proyecciones]
	Sea $f:V \to V$ una proyección, entonces ocurre una de las siguientes:
	\begin{enumerate}
		\item $f = Id_V$
		\item $f = \overrightarrow{0}$
		\item $\exists W, W' \subset V : \ V = W \oplus W'$ y $f\vert_W = Id_W \land f\vert_{W'} = \overrightarrow{0}$.
	\end{enumerate}
	Donde $V = W \oplus W'$ significa que $W + W' = V$ y además $W \cap W' = \emptyset$.
\end{pro}

\section{Aplicaciones ortogonales y unitarias}
\begin{dfn}[Aplicación ortogonal o unitaria]
	Decimos que $f: V \to V$ es ortogonal ($K = \mathbb{R}$) o unitaria ($K = \mathbb{C}$) si conserva el producto escalar, es decir, si $\forall u,v \in V,\ \varphi(u, v) = \varphi(f(u), f(v))$.
\end{dfn}

\begin{tm}[Criterio de la norma para aplicaciones ortogonales y unitarias]
	Sea $f: V \to V$ lineal. Entonces $f$ es ortogonal o unitaria $\iff \forall u \in V, \ \lVert f(u)\rVert_\varphi = \lVert u \rVert_\varphi$.
\end{tm}

\begin{tm}[Criterio de la BON para aplicaciones ortogonales y unitarias]
	Sea $f: V \to V$ lineal. Entonces $f$ es ortogonal o unitaria $\iff$ $f$ lleva una BON de $V$ ($B = \{v_1, \dots, v_n\}$) en otra BON de $V$ ($B' = \{f(v_1), \dots, f(v_n)\}$).
\end{tm}

\begin{proof}
	La implicación $\implies$ es evidente, veamos el recíproco. Sea $B = \{v_1, \dots, v_n\}$ una BON y sea $B' = \{f(v_1), \dots, f(v_n)\}$ otra BON. Sean $[u]_B = (\alpha_1, \dots, \alpha_n)$ las coordenadas de $u$ en la base $B$. Comprobamos que las coordenadas de $f(u)$ son las mismas respecto de $B'$: $[f(u)]_{B'} = [f(\alpha_1 v_1 + \dots + \alpha_n v_n)]_{B'} = [\alpha_1 f(v_1) + \dots  +  \alpha_n f(v_n)]_{B'} = (\alpha_1, \dots, \alpha_n)$. Tenemos que ver que $\varphi(u, v) = \varphi(f(u), f(v))$. Utilizando el desarrollo por coordenadas $\varphi(u, v) = [u]_B^T \cdot I \cdot \overline{[v]_B}^T = \alpha_1 \overline{\beta_1} + \dots + \alpha_n \overline{\beta_n}$. Y análogamente $\varphi(f(u), f(v)) = [f(u)]_{B'}^T \cdot I \cdot \overline{[f(v)]_{B'}}^T = \alpha_1 \overline{\beta_1} + \dots + \alpha_n \overline{\beta_n}$. $\varphi(u, v) = \varphi(f(u), f(v))$.
\end{proof}

\begin{tm}[Criterio de la inversa para aplicaciones ortogonales y unitarias]
	Sea $f:V \to V$ un endomorfismo. Entonces $f$ es ortogonal o unitario $\iff f^{-1} = \tilde{f}$.\footnote{Observemos que $f^{-1}$ siempre existe en el caso de que $f$ sea ortogonal/unitaria ya que al llevar $f$ una base en otra, es biyectiva en $V$.}
\end{tm}

\begin{proof}
	($\implies$) Suponemos $f$ ortogonal. Sabemos que $\forall u, v \in V,\ \varphi(u, v) = \varphi(f(u), f(v)) = \varphi(u, \tilde{f}(f(v)))$ es decir, que $\varphi(u, v) - \varphi(u, \tilde{f}(f(v))) = 0$ o mejor dicho, $\varphi(u, v -  \tilde{f}(f(v))) = 0$. Si fijamos $v \in V$ y observamos que la igualdad se cumple $\forall u \in V$, concluímos que $v - \tilde{f}(f(v)) = 0$ pues es ortogonal a todo $u$. Por tanto $v = \tilde{f}(f(v))$. Ahora haciendo variar $v$ tenemos que $\forall v \in V,\ v = \tilde{f}(f(v)) \implies \tilde{f} \circ f = Id \implies \tilde{f} = f^{-1}$.
	
	($\impliedby$) Suponemos $\tilde{f} = f^{-1}$. Entonces $\varphi(f(u), f(v)) = \varphi(u, \tilde{f} \circ f(v)) = \varphi(u, f^{-1} \circ f(v)) = \varphi(u, v)$.
\end{proof}

\begin{tm}
	Sea $f: V \to V,\ \varphi$ produto escalar y sea $B$ una BON de $V$. Entonces $f$ es ortogonal o unitaria $\iff M_B(f^{-1}) = \overline{M_B(f)}^T$. Es decir, podemos comprobar que $f$ es ortogonal verificando $M_B(f) \cdot M_B(f)^T = I$.
\end{tm}

\begin{obs}
	Si $f: V \to V$ es ortofonal o unitaria, entonces $|\det f| = 1$ (el módulo del determinante de $f$).
\end{obs}
\begin{proof}
	Observamos que $f \circ f^{-1} = Id,\ \det f \cdot \det f^{-1} = 1$. En particular, si $f$ es ortogonal o unitaria y fijamos una base $B$ tenemos que: $\det(M_B(f))\cdot \det(M_B(f^{-1})) = \det(M_B(f))\cdot \det(M_B(f)^T) = \det(M_B(f))\cdot \det(M_B(f)) = 1$. En el caso complejo quedaría $\det(M_B(f))\cdot \det(\overline{M_B(f)}) = 1$.
\end{proof}

\begin{pro}
	Sea $f: V \to V$ ortogonal o unitaria, sea $\lambda \in \mathbb{C}$ un autovalor de $f$. Entonces $|\lambda| = 1$.
\end{pro}

\begin{pro}
	Sea $f: V \to V$ ortogonal o unitaria. Si $W \subset V$ es un subespacion invariante por $f$ entonces $W^\perp$ también lo es.
\end{pro}

\begin{tm}[Teorema de clasificación de aplicaiones unitarias]
	Sea $f : V \to V$ unitaria. Entonces $f$ es diagonalizable en una BON y la matriz
	\[
	M_B(f) = \left(\begin{array}{ccc}
	\lambda_1 &\dots &0\\
	\vdots &\ddots &\dots\\
	0 & \dots & \lambda_n
	\end{array}
	\right)
	\]
	con $|\lambda_i| = 1$ para $i = 1, \dots, n$.
\end{tm}

\begin{proof}
	Demostramos por inducción en $\dim V$:
	\begin{itemize}
		\item $\dim V = 1$: cualquier matris es diagonal y al ser $f$ unitaria $|\lambda_1| = 1$.
		\item $\dim V = n$ suponemos el teorema para $n - 1$. Como $f$ es unitaria, $K = \mathbb{C}$, existe un autovalor $\lambda \implies \exists v \in V, v ≠ \vec{0} : f(v) = \lambda v$ y $|\lambda| = 1$. Sea $W = \langle v \rangle,\ \dim W = 1$. Por la proposición anterior como $W^\perp$ es invariante por $f$. Ahora $\dim W^\perp = n - 1$ y por hipótesis de inducción es diagonalizable con $|\lambda_i| = 1$.\footnote{Esto se demuestra análogamente a la demostración para aplicaciones autoadjuntas.}
	\end{itemize}
\end{proof}

%TODO: falta día 6/10

\begin{pro}
	Sea $f: V \to V$ ortogonal con $\dim V \geq 2$. Si $f$ tiene un autovalor $\lambda \in \mathbb{C}\setminus\mathbb{R}$, entonces:
	\begin{enumerate}
		\item $f$ tiene un $W \subset V$ invariante con $\dim V = 2$.
		\item $\exists$ una BON de $W$, $B = \{v_1, v_2\}$ tal que
	\end{enumerate} 
	\[
		M_B(f\vert_W) = \left(
			\begin{array}{cc}
				\cos \alpha & -\sin \alpha\\
				\sin \alpha & \cos \alpha
			\end{array}
		\right)\qquad \alpha \in [0, 2\pi),\ \alpha ≠ 0, \pi
	\]
\end{pro}

\subsection{Clasificación de aplicaciones ortogonales}
\begin{tm}[Clasificación de aplicaciones ortogonales en $\mathbb{R}^2$]
	Sea $f: \mathbb{R}^2 \to \mathbb{R}^2$ ortogonal.
	\begin{enumerate}
		\item Si $f$ tiene autovalores reales $\lambda$:
		\begin{enumerate}
			\item $\lambda = 1$ (doble): $f = I_2$. (Nada.)
			\item $\lambda = -1$ (doble): $f = -I_2$. Tenemos una simetría central.
			\item $\lambda = \pm 1$: existe una BON $B$ en la que $M_B(f) = \left(\begin{array}{cc}
			1 & 0 \\ 0 & -1
			\end{array}\right)$ y $\det M_B(f) = -1$. Tenemos una simetría axial respecto a una recta de vectores fijos $\equiv \ker (f - I_2)$.
		\end{enumerate}
		\item si $f$ no tiene autovalores reales entonces $\exists$ una BON $B = \{u_1, u_2\}$ tal que $M_B(f) = \left(\begin{array}{cc}
			\cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha
			\end{array}\right)$ con $\det M_B(f) = 1$. Tenemos una rotación de ángulo $\alpha$.
	\end{enumerate}
\end{tm}

\begin{tm}[Clasificación de aplicaciones ortogonales en $\mathbb{R}^3$]
	Sea $f: \mathbb{R}^3 \to \mathbb{R}^3$ ortogonal, sea $B$ una BON de $\mathbb{R}^3$ adecuada. Entonces $M_B(f)$ tiene alguna de las siguientes formas:
	\begin{enumerate}
		\item si $\det f = 1$:
		\begin{enumerate}
			\item (nada): $M_B(f) = I_3$
			\item simetría axial respecto de la recta generada por $v_1$: $M_B(f) = \left(\begin{array}{ccc}
			1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & -1
			\end{array}\right)$
			\item rotación de $\alpha$ respectoa  $\langle v_1 \rangle$: $M_B(f) =  \left(\begin{array}{ccc}
			1 & 0 & 0 \\ 0 & \cos \alpha & -\sin \alpha \\ 0 & \sin \alpha & \cos \alpha
			\end{array}\right)$
		\end{enumerate}
		\item si $\det f = -1$:
		\begin{enumerate}
			\item simetría central: $M_B(f) = -I_3$
			\item simetría respecto al plano $\langle v_2, v_3 \rangle$: $M_B(f) = \left(\begin{array}{ccc}
			1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1
			\end{array}\right)$
			\item rotación de $\alpha$ respecto a  $\langle v_1 \rangle$ y simetría respecto al plano $\langle v_2, v_3 \rangle = \langle v_1 \rangle ^\perp$: $M_B(f) =  \left(\begin{array}{ccc}
			1 & 0 & 0 \\ 0 & \cos \alpha & -\sin \alpha \\ 0 & \sin \alpha & \cos \alpha
			\end{array}\right)$
		\end{enumerate}
	\end{enumerate}
\end{tm}

\begin{ej}
	Consideramos $B$ la base estándar y $\varphi$ el productos escalar usual. Sea $f:\mathbb{R}^3 \to \mathbb{R}^3$ dada por
	\begin{align*}
		M_B(f) = \frac{1}{15}\left(\begin{array}{ccc}
		-10 & 5 & -10 \\ -11 & -2 & 10 \\ 2 & 14 & 5
		\end{array}\right)
	\end{align*}
	
	\begin{enumerate}
		\item Primero comprobamos que $f$ es ortogonal viendo que $M_B(f) \cdot M_B(f)^T = I_3$.
		\item Ahora observamos que $f$ no es autoadjunta puesto que $M_B(f)$ no es simétrica. Por tanto estamos en el caso $1c)$ o $2c)$. Para saberlo calculamos el determinante: $\det M_b(f) = 1 \implies$ se trata de una rotación respecto a un eje de vectores fijos.
		\item Calculamos el eje de rotación: $\ker (f  - 1 \cdot I) = \langle (1, -3, 4)\rangle$, el eje es el determinado por el vector unitario $u = \frac{1}{\sqrt{26}} (1, -3, -4)$.
		\item Calculamos el ángulo de rotación
		\begin{enumerate}
			\item Primero tenemos que orientar $\mathbb{R}^3$: fijaremos la orientación de la base $B = \{e_1, e_2, e_3\}$ como positiva. Luego elegimos la orientación del eje arbitrariamente (elegimos el lado desde el cual miramos el plano). Orientamos el eje segín $(1, -3, -4)$.
			\item Para obtener $\alpha$ sabemos que $\text{traza } f$ no depende de la base escogida. Como $\text{traza }f = 1 + 2\cos \alpha$ podemos obtener $\cos \alpha = -\frac{11}{5}, \sin \alpha = \pm \sqrt{1 - \cos^2 \alpha}$.
			\item Para obtener el signo del seno tenemos que encontrar una BON positivamente orientada donde aparezca $u = \frac{1}{\sqrt{26}}(1, -3, -4)$ (normalizado) como primer elemento de la base. Consideramos $\langle u \rangle ^\perp = \{x - 3y - 4z = 0\}$ que tiene una BON que es $\{v = \frac{1}{\sqrt{10}}(3, 1, 0), w = \frac{1}{\sqrt{65}}(2, -6, 5)\}$.
			
			Si cogemos $\tilde{B} = \{u, w, v\}$ vemos que $\det(u, w, v)  < 0$ luego no está positivamente orientada. Podemos o bien cambiar un vector de signo o reordenar para cambiar de signo el determinante. Elegimos $B' = \{u, v, w\}$ que es una BON y está orientada positivamente.
			
			Entonces $M_{B'}(f) = \left(\begin{array}{ccc}
				1 & 0 & 0 \\ 0 & \cos \alpha & -\sin \alpha \\ 0 & \sin \alpha & \cos \alpha
			\end{array}\right)$.
		\end{enumerate}
	\end{enumerate}

	Veamos diferentes formas de obtener el signo del seno una vez obtenida esta nueva BON:
	\begin{itemize}
		\item Podemos hacer una cambio de base: $M_{B' B}M_B(f)M_{B B'} = \left(\begin{array}{ccc}
		1 & \dots & \dots \\ \dots & -11/5 & \dots \\ \dots & +\sqrt{1 - \dots} & \dots
		\end{array}\right)$ No nos hace falta calcular toda la matriz, con solo calcular las posiciones donde aparece el seno ya vemos su signo ($\sin \alpha > 0$).
		
		\item Obserbando que $f(v) = \frac{1}{15}\left(\begin{array}{ccc}
		-10 & 5 & -10 \\ -11 & -2 & 10 \\ 2 & 14 & 5
		\end{array}\right)\left(\begin{array}{c}
		3/\sqrt{10} \\ 1/\sqrt{10} \\ 0
		\end{array}\right) = \frac{1}{3\sqrt{10}}\left(\begin{array}{c}
		-5 \\ -7 \\ 4
		\end{array}\right) = \lambda_1 u + \lambda_2 v + \lambda_3 w$, pero sabemos que [$f(v)] = 0 u - \frac{11}{5} v + \sin \alpha w$ por la segunda columna de $M_{B'}(f)$. Igualando obtenemos $\frac{1}{3\sqrt{10}}\left(\begin{array}{c}
		-5 \\ -7 \\ 4
		\end{array}\right) = -\frac{11}{5}\left(\begin{array}{c}
		3/\sqrt{10} \\ 1/\sqrt{10} \\ 5
		\end{array}\right) + \sin \alpha \left(\begin{array}{c}
		2/\sqrt{65} \\ -6 / \sqrt{65} \\ 5 / \sqrt{65}
		\end{array}\right)$ (nos bastaría con despejar $\sin \alpha w$ y ver el signo del primer coeficiente).
		\item Observamos que $\tilde{\tilde{B}} = \{u, v, f(v)\}$ es una base pues $\alpha = \pi$. (ya hemos descartado una simetría axial). Nos damos cuenta (?) de que $M_{B' \tilde{\tilde{B}}} = \left(\begin{array}{ccc}
		1 & 0 & 0 \\ 0 & 1 & \cos \alpha \\ 0 & 0 & \sin \alpha
		\end{array}\right)$ luego signo $\det M_{B \tilde{\tilde{B}}} = \text{signo }\sin \alpha$. Como el signo de la base no depende del determinante podemos coger los vectores sin normalizar.
		\begin{align*}
			\det \left(\begin{array}{ccc}
			1 & 2 & -5 \\ -3 & -6 & -7 \\ 4 & 5 & 4
			\end{array}\right) > 0 \implies \sin \alpha > 0
		\end{align*}
	\end{itemize}

	Para dar las características de una $f$ de este tipo hay que dar la orientación del eje, el coseno y el signo del seno como mínimo.
\end{ej}

\begin{ej}
	Queremos estudiar la aplicación $h: \mathbb{R}^3 \to \mathbb{R}^3$ dada por la matriz (en la base estándar):
	\begin{align*}
		M_B(h) = \left(\begin{array}{ccc}
		0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0
		\end{array}\right)
	\end{align*}
	\begin{enumerate}
		\item Comprobamos que $h$ es ortonormal, ya que nos damos cuenta de que lleva una BON en otra BON (mirando la matriz).
		\item $\det f = -1$ y además $M_B(f)$ es simétrica luego $f$ es autoadjunta. Por tanto, se trata de una simetría respecto a un plano.
		\item Damos el plano. Es el plano generado por los autovectores de autovalor 1: $\ker ( f - I_3) \equiv$ plano de simetría. Ojo: $\ker(f  + I_3)$ nos daría la recta que se convierte en su simétrica.
	\end{enumerate}
\end{ej}

% ---------------------------------------------------

\chapter{Espacios afines: $\mathbb{A}^n_{\mathbb{R}}$}
\begin{dfn}[Espacio afín]
	Sea $K$ un cuerpo. Un espacio afín sobre $K$ es una terna $(A, \psi, E)$ donde
	\begin{itemize}
		\item $A$ es un conjunto de puntos no vacío
		\item $E$ es un espacio vectorial sobre $K$
		\item $\psi$ es una ción que relaciona $A$ con $E$, $\psi A \times A \to E,\ (P, Q) \mapsto \psi(P, Q) = \overrightarrow{PQ}$ de manera que se cumple:
		\begin{itemize}
			\item para cada $P \in A, \psi_P : A \to E,\ Q \mapsto \psi(P, Q) = \overrightarrow{PQ}$ es una función biyectiva (es decir, convertimos cada posible $P$ en el origen del espacio vectorial).
			\item $\forall P, Q, R \in A,\ \psi(P,Q) + \psi(Q, R) = \psi(P, R)$ (vinculamos todos los diferentes orígenes con un criterio para poder ir saltando de uno en otro).
		\end{itemize}
	\end{itemize}
	Diremos que $\dim A = \dim_K E$.
\end{dfn}

\begin{ej}
	¿Cómo dotar a $\mathbb{A}^2_\mathbb{R}$ de estructura de espacio afín? $\mathbb{A}^2_\mathbb{R} = \{(a_1, a_2) : a_i \in \mathbb{R}\}$. Escogemos $\psi : \mathbb{A}^2_\mathbb{R} \times \mathbb{A}^2_\mathbb{R} \to \mathbb{R}^2,\ ((a,b), (c,d)) \mapsto (c-a, d-b)$.
	
	Hay muchas maneras de escoger $\psi$. Por ejemplo podríamos multiplicar pr una matriz de cambio de base para tener otra. Sin embargo, no veremos otras posibles definiciones de $\psi$.
\end{ej}

\begin{pro}
	Sea $(A, \psi, E)$ un espacio afín, entonces:
	\begin{enumerate}
		\item $\overrightarrow{P Q} = \overrightarrow{0}_E \iff P = Q$
		\item $\forall P, Q \in A,\ \psi(P, Q) = \overrightarrow{P Q} = - \overrightarrow{Q P} = \psi(Q, P)$
		\item $\forall P, Q, R, S \in A,\ \overrightarrow{PQ} = \overrightarrow{RS} \implies \overrightarrow{PR} = \overrightarrow{QS}$.
		% TODO: add a picture
	\end{enumerate}
\end{pro}

\begin{proof}
	Probamos cada una de las proposiciones:
	\begin{enumerate}
		\item $\overrightarrow{PP} = \overrightarrow{PP} + \overrightarrow{PP}; -\overrightarrow{PP} + \overrightarrow{PP} = -\overrightarrow{PP} + \overrightarrow{PP} + \overrightarrow{PP}; \overrightarrow{0}_E = \overrightarrow{PP}$
		\begin{itemize}
			\item $(\implies)$ Por la primera propiedad de $\psi$ tenemos que $\psi_P : A \to E,\ P \mapsto \vec{0}$ es biyectiva. Entonces si $Q ≠ P \implies \psi_P(Q) = \overrightarrow{PQ} + \vec{0}$. 
			\item $(\impliedby)$ Es claro que $P = Q \implies \overrightarrow{PQ} = \vec{0}$.
		\end{itemize}
		\item Basta comprobar que $\overrightarrow{PQ}$ es el opuesto de $\overrightarrow{QP}$ por la suma. Aplicando la segunda propiedad de $\psi$ tenemos que $\overrightarrow{PQ} + \overrightarrow{QP} = \overrightarrow{PP} = \vec{0}$ por la propiedad anterior. Además, el sabemos que el opuesto de un vector en $E$ es único.
		\item Basta probar una implicación pues la otra es análoga. Probaremos $\implies$. $\overrightarrow{PQ} = \overrightarrow{RS};\ \overrightarrow{PQ} = \overrightarrow{PR} + \overrightarrow{RQ}$ y $\overrightarrow{RS} = \overrightarrow{RQ} + \overrightarrow{QS}$ luego $\overrightarrow{PR} + \overrightarrow{RP} = \overrightarrow{RQ} + \overrightarrow{QS} \implies \overrightarrow{PR} = \overrightarrow{QS}$.
	\end{enumerate}
\end{proof}

\begin{dfn}[Variedad lineal]
	Sea $(A, \psi, E)$ un espacio afín. Sean $P \in A, F \in E$ un punto de $A$ y un subespacio de $E$ respectivamente. Definimos la variedad lineal o subespacio afín $\mathcal{L}$ que pasan por el punto $P$ con dirección $F$ como el siguiente subconjunto de $A$:
	\begin{align*}
		\mathcal{L} = P + F := \{Q \in A : \overrightarrow{PQ} \in F\}
	\end{align*}
	Diremos que $\dim P + F = \dim F$.
\end{dfn}

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^2$. Sea $P = (1,0), F = \langle(1,1)\rangle$. Entonces un punto $Q = (x,y) \in A$ está en la variedad lineal si:
	\begin{align*}
		Q \in P + F \iff \overrightarrow{PQ} \in F \iff (x - 1, y) \in F \iff \\
		\exists \lambda \in \mathbb{R} : (x - 1, y) = \lambda(1, 1) \iff \left\lbrace\begin{array}{cc}
			x &= 1 + \lambda \\
			y &= \lambda
		\end{array}\right\rbrace
	\end{align*}
\end{ej}

\begin{pro}
	Sea $P \in A, F \in E, P + F$ una variedad lineal. Sea $R \in P + F$ un punto cualquiera de la variedad lineal, entonces $P + F$ = $R + F$.
\end{pro}

\begin{proof}
	Como se trata de una igualdad de conjuntos, tenemos que probar la doble inclusión, no obstante solamente probaremos una. Veamos que $P + F \subset R + F$. Sea $Q \in A$. Tenemos que ver que $Q \in P + F \implies Q \in R + F$, es decir que $\overrightarrow{RQ} \in F$. Sabemos que $Q \in P + F$ luego $\overrightarrow{PQ} \in F$. También sabemos que $R \in P + F \implies \overrightarrow{PR} \in F$. Ahora bien, $\overrightarrow{RQ} = \overrightarrow{RP} + \overrightarrow{PQ} \in F$ luego $\overrightarrow{RQ} \in F$.
\end{proof}

\begin{dfn}
	Sean $P_1, \dots, P_k \in A$. La variedad lineal generada por $P_1, \dots, P_k$ es la más pequeña que conteine a $P_1, \dots, P_k$.
\end{dfn}

\begin{ej}
	Consideremos el conjunto que solo contiene a $P,\ \{P\}$, donde $P \in \mathbb{A}_\mathbb{R}^2$. Tenemos que $\{P\} = P + \{\vec{0}\}$.
	
	Consideremos ahora $P, Q \in \mathbb{A}_\mathbb{R}^2, P ≠ Q$. La variedad lineal que contiene a ambos puntos: $\{P, Q\} \in P + F$ donde $F = \langle \overrightarrow{PQ}\rangle$.
\end{ej}

\begin{pro}
	Dados $P_1, \dots, P_k \in \mathbb{A}$ con $k \geq 2$ con $P_i ≠ P_j$ para $i≠j$. La variedad lineal más pequeña que contiene a esos puntos es $P_1 + \langle \overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}\rangle$.
\end{pro}

\begin{proof}
	Es claro que la variedad lineal propuesta contiene a todos los puntso $P_1, \dots, P_k$. Vamos a probar que cualquier otra variedad lineal $L$ que contenga a $P_1, \dots, P_k$ cumple $P_1 + \langle \overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}\rangle \subset L$. Podemos suponer que $L = P_1 + F$ para un cierto $F$. Como $P_1 \dots, P_k \in L,\ \overrightarrow{P_1 P_i} \in F$ para $i = 1, \dots, k$. Sea $Q \in P_1 + \langle \overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}\rangle$. Como $\overrightarrow{PQ} \in \langle \overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}\rangle \subset F$ tenemos que $Q \in P_1 + F \in L$.
\end{proof}

\begin{dfn}[Incidencia de variedades lineales]
	Sean $L = P + F, M = Q + G$ dos variedads lineales en $\mathbb{A}$. Entonces puede ocurrir lo siguiente:
	\begin{enumerate}
		\item $L$ y $M$ se cortan si $L \cap M ≠ \emptyset$,
		\item $L$ y $M$ son paralelas si $F \subset G$ o $G \subset F$,
		\item $L$ y $M$ se cruzan si ni se cortan ni son paralelas.	
	\end{enumerate}
\end{dfn}

Sean $L_1 = P + F, L_2 = Q + G$ variedades lineales en $\mathbb{A}$. Entonces puede ocurrir que:
\begin{enumerate}
	\item $L_1 \cap L_2 = \emptyset$ – podemos ampliar la definición de variedad lineal para que incluya el vacío.
	\item $L_2 \cap L_2 ≠ \emptyset$ –  probaremos que $L_1 \cap L_2$ es una variedad lineal.
\end{enumerate}

\begin{pro}
	Dadas dos variedades lineales $L_1$ y $L_2$ definidas como antes, se tiene que $L_1 \cap L_2$ es una variedad lineal. Además si $L_1 \cap L_2 ≠ 0$ y $R \in L_1 \cap L_2$ entonces $L_1 \cap L_2 = R + F \cap G$.
\end{pro}
\begin{proof}
	$L_1 \cap L_2 \supset R + F \cap G$: Sea $S \in R + F \cap G$. Sabemos que $L_1 = P + F, L_2 = Q + G$ y también que $L_1 = R + F, L_2 = R + G$. Tenemos que ver que $\overrightarrow{RS} \in F \land \overrightarrow{RS} \in G$. Esto es cierto porque $\overrightarrow{RS} \in F \cap G$.
	
	$L_1 \cap L_2 \subset R + F \cap G$: si $S \in L_1 \cap L_2$ entonces $\overrightarrow{RS} \in F$ y $\overrightarrow{RS} \in G$ luego $\overrightarrow{RS} \in F \cap G \implies S \in R + F \cap G$.
\end{proof}

Nos preguntamos ¿cuándo sucede que $L_1 \subset L_2$?. Sean $L_1 = P+F, L_2 = Q+G, L_1, L_2 ≠ \emptyset$ y $L_1 \subset L_2$. Entonces $P \in L_2$ luego $L_1 = P + F, L_2 = P + G$. SI $L_1 \subset L_2$, entonces $F \in G$. Sea $S \in L_1$, entonces $\overrightarrow{PS} \in F \implies \overrightarrow{PS} \in G \implies S \in L_2 \implies L_1 \subset L_2$.

\begin{pro}["Suma" de variedades lineales]
	Sean $L_1 = P + F, L_2 = Q + G$ variedades lineals en $\mathbb{A}$. La variedad lineal generada por $L_1 + L_2$ es la variedad lineal más pequeña que contiene a $L_1 + L_2$ y se denota por
	\begin{align*}
		L_1  + L_2 = P + F + G + \langle \overrightarrow{PQ} \rangle
	\end{align*}
\end{pro}

Demostración aburrida que no copié.

\begin{pro}
	Sean $L_1 = P + F, L_2 = Q + G$ dos variedades lineales paralelas. Entonces
	\begin{itemize}
		\item o bien $L_1 \cap L_2 = \emptyset$,
		\item o bien $L_1 \subset L_2$ o $L_2 \cap L_1$.
	\end{itemize}
\end{pro}

\begin{proof}
	Podemos suponer sin pérdida de generalidad que $F \subset G$ (pues $L_1, L_2$ son paralelas). Si $L_1 \cap L_2 = \emptyset$ no hace falta hacer nada. Si $L_1 \cap L_2 ≠ \emptyset$ tenemos que probar que $L_1 \cap L_2$. Como la intersección es no nula, $\exists R \in L_1 \cap L_2 \implies L_1 = R + F \land L_2 = R + G$ y como $F \subset G$ tenemos que $L_1 \in L_2$.
\end{proof}

\begin{pro}[Criterio para decidir cuando $L_1 \cap L_2 ≠ \emptyset$]
	Sean $L_1 = P + F, L_2 = Q + G \in A$. Entonces $L_1 \cap L_2 ≠ \emptyset \iff \overrightarrow{PQ} \in F + G$.
\end{pro}

\begin{proof}
	$(\implies)$ Supongamos $L_1 \cap L_2 ≠ \emptyset$. Sea $R \in L_1 \cap L_2$. Podemos decir que $L_1 = R + F$ y $L_2 = R + G$. $P \in L_1 \implies \overrightarrow{RP} \in F,\ Q \in L_2 \implies \overrightarrow{RQ} \in G$. Entonces, $\overrightarrow{PQ} = \overrightarrow{PR} + \overrightarrow{RQ} \in F + G$.
	
	$(\impliedby)$. Supongamos que $\overrightarrow{PQ} \in F + G$, entonces $\exists v \in F, w \in G$ tales que $\overrightarrow{PQ} \in v + w$. Como $v \in F, \exists A \in L_1 : v = \overrightarrow{PA}$ y como $w \in G,\ \exists B \in L_2 : w = \overrightarrow{QB}$. Luego $\overrightarrow{PQ} = \overrightarrow{PA} + \overrightarrow{PB},\ \overrightarrow{PQ} - \overrightarrow{PA} = \overrightarrow{QB}; \overrightarrow{QA} = \overrightarrow{QB} \implies A = B$ porque $\psi_P$ es biyectiva según la definición de espacio afín.
\end{proof}

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^3$. Sea $L_1 = (1,1,0) + \langle(1,1,1),(1,0,0)\rangle, L_2 = (0,1,0) + \langle (0,1,1),(2,1,2)\rangle$. Como $(1,1,0) - (0,1,0) = (1,0,0) \in \langle (1,1,1),(1,0,0),(0,1,1),(2,1,2)\rangle$ sabemos que $L_1$ y $L_2$ se cortan ($L_1 \cap L_2 ≠ \emptyset$).
\end{ej}

\begin{tm}[Fórmula de Grassmann para variedades lineales]
	Sean $L_1, L_2$ variedades lineales en el espacio afín $(A, \psi, E)$. Entonces,
	\begin{itemize}
		\item asi $L_1 \cap L_2 ≠ \emptyset,\ \dim(L_1 + L_2) = \dim(L_1) + \dim(L_2) - \dim(L_1 \cap L_2)$;
		\item si $L_1 \cap L_2 = \emptyset,\ \dim (L_1 + L_2) = \dim L_1 + \dim L_2 - \dim(F \cap G) + 1$.
	\end{itemize}
\end{tm}

%TODO: faltan ejemplos de grassmann

\section{Sistemas de referencia}
\textbf{Motivación:} hasta ahora no hemos trabajado con coordenadas de puntos. Queremos poder trabajar con coordenadas. Si disponemos de coordenadas vamos a ser capaces de describir las variedades lineales mediante sistemas de ecuaciones lineales.

\begin{dfn}[Independencia afín]
	Sea $(A, \psi, E)$ un espacio afín. Diremos que los puntos $P_1, \dots, P_k \in A,\ k \geq 2$ son afínmente independientes si los overrightarrowtores $\overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}$ son linealmente independientes.
	
	Convenimos en que un solo punto ($k = 1$) siempre es afínmente independiente.
\end{dfn}

\begin{ej}
	Pensemos en $\mathbb{A}^1_\mathbb{R}$. Si cogemos solo un punto $P \in \mathbb{A}^1_\mathbb{R}$ siempre es afínmente independiente. Si cogemos $P, Q \in \mathbb{A}^1_\mathbb{R}$ pueden ocurrir dos cosas. Si $P = Q$ no son afínmente independientes, mientras que si $P ≠ Q$ sí son afínmente independientes. Sin embargo, si cogemos $P, Q, R \in \mathbb{A}_\mathbb{R}^1$ nunca serán afínmente independientes.
	
	Pensemos ahora en $\mathbb{A}^2_\mathbb{R}$. Si cogemos $P, Q, R \in \mathbb{A}_\mathbb{R}^1$, tendremos tres puntos afínmente independientes siempre que no estén alineados.
	
	Vemos que da igual el orden en el que cojamos los puntos
\end{ej}

\begin{pro}
	Sean $P_1, \dots, P_k \in A$, con $k \geq 2$. Entonces $\overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}$ son linealmente independientes $\iff \forall i \in \{i, \dots, k\},\ \overrightarrow{P_i,P_1}, \dots, \overrightarrow{P_i P_{i-1}}, \overrightarrow{P_iP_{i+1}}, \dots, \overrightarrow{P_iP_k}$ son linealmente independientes.
\end{pro}

\begin{proof}
	Basta probar una de las implicaciones. Supongamos $\overrightarrow{P_1 P_2}, \dots, \overrightarrow{P_1 P_k}$ son linealmente independientes. Tenemos que ver que $\lambda_1\overrightarrow{P_i,P_1} + \dots + \lambda_{i-1}\overrightarrow{P_i P_{i-1}} +  \lambda_{i+1}\overrightarrow{P_iP_{i+1}} + \dots + \lambda_k\overrightarrow{P_iP_k} = 0 \implies \lambda_j = 0,\ j = 1, \dots i-1, i+1, \dots, k$. Ahora escribimos cada $\overrightarrow{P_iP_k}$ como $\overrightarrow{P_iP_1} + \overrightarrow{P_1P_j}$: $\lambda_1\overrightarrow{P_i,P_1} + \lambda_2(\overrightarrow{P_iP_1 + P_1P_2}) + \dots + \lambda_{i-1}(\overrightarrow{P_i P_{1}} + \overrightarrow{P_1 P_{i-1}}) +  \lambda_{i+1}(\overrightarrow{P_i P_{1}} + \overrightarrow{P_1 P_{i+1}}) + \dots + \lambda_k(\overrightarrow{P_iP_1} + \overrightarrow{P_1 P_k}) = 0$. Es decir, %TODO
\end{proof}

\begin{obs}
	Si $\dim A = n$, entonces el máximo número de puntos afínmente independientes en $A$ es $n+1$.
\end{obs}

\begin{dfn}[Base afín]
	Diremos que un conjunto de puntos de $A$ son una base afín si son afínmente independientes y la variedad lineal que generan es $A$.
\end{dfn}

\begin{ej}
	Sea $A = \{P\}$. La base afín es $\{P\}$ y $\dim A = 0$.
	
	Sea $\mathbb{A}_\mathbb{R}^n,\ n \geq 1$. Sean $\{P_0, P_1, \dots, P_n \}\  n+1$ puntos afínmente independientes. Generan la variedad lineal $P_0 + \langle\overrightarrow{P_0P_i}\rangle, \ i = 1, \dots, n$, que contiene a cualquier punto de $A$, luego $\{P_0, P_1, \dots, P_n \}$ es una base afín.
\end{ej}

\begin{tm}[Criterios para base afín]
	Sean $P_0, P_1, \dots, P_k \in A$. Entonces las siguentes afirmaciones son equivalentes:
	\begin{enumerate}
		\item $P_0, P_1, \dots, P_k$ son afínmente independientes;
		\item Si existen escalares $\lambda_0, \lambda_1, \dots, \lambda_k \in K$ con $\lambda_0 + \lambda_1 + \dots + \lambda_k = 0$ y tal que  $\forall P \in A$ tenemos $\lambda_0\overrightarrow{PP_0} + \dots + \lambda_k\overrightarrow{PP_k} = \overrightarrow{0}$, entonces se verifica que $\lambda_0 = \lambda_1 = \dots = \lambda_k = 0$;
		\item Para cada $i = 0, \dots k,\ P_i$ no está en la variedad generada por el resto de los puntos –  $P_0, \dots, P_{i-1}, P_{i+1}, \dots P_k$.
	\end{enumerate}
\end{tm}

\begin{proof}
	Es claro que iii) es equivalente a i). Veamos que i) es equivalente a ii).
	
	Probamos i) $\implies$ ii). Supongamos que existen $\lambda_0, \dots, \lambda_k \in K$ con $\lambda_0 + \dots + \lambda_k = 0$ tal que $\forall P \in A$ tenemos $\lambda_0\overrightarrow{PP_0} + \dots + \lambda_k\overrightarrow{PP_k} = \overrightarrow{0}$. Lo que queremos probar es que $\lambda_0 = \dots = \lambda_k = 0$. Tomamos $P = P_0$, por hipótesis tenemos que $\lambda_0\overrightarrow{P_0P_0} + \dots + \lambda_k\overrightarrow{P_0P_k} = \lambda_0\overrightarrow{0} + \dots + \lambda_k\overrightarrow{P_0P_k} = \overrightarrow{0}$. Por hipótesis, $P_0, \dots, P_k$ son afínmente independientes luego $\lambda_1 = \dots = \lambda_k = 0$ necesariamente. Como además $\sum \lambda_i = 0$, tiene que ocurrir que $\lambda_0 = 0$.
	
	Ahora probaremos que ii) $\implies$ i). Tenemos que probar que la única solución de $\mu_1 \overrightarrow{P_0 P_1} + \dots  + \mu_k \overrightarrow{P_0 P_k} = \overrightarrow{0}$ es necesariamente $\mu_1 = \dots = \mu_k = 0$. Hacemos aparecer un $P \in A$: $\mu_1 (\overrightarrow{P_0 P} + \overrightarrow{P P_1}) + \dots  + \mu_k (\overrightarrow{P_0 P} + \overrightarrow{P P_k}) = -(\mu_1 + \dots + \mu_k)\overrightarrow{P P_0} + \mu_1 \overrightarrow{P P_1} + \dots  + \mu_k \overrightarrow{P P_k} = \overrightarrow{0}$. $ -(\mu_1 + \dots + \mu_k) + \mu_1 + \dots + \mu_k = 0$. Ya estamos en las hipótesis de ii), y como ii) se cumple, entonces $\mu_0 = \dots = \mu_k = 0$.
\end{proof}

% 2017/10/30
\subsection{Sistemas de referencia cartesianos}
\begin{dfn}[Referencia cartesiana]
	Un sistema de referencia cartesiano en $A$ es un conjunto formado por un punto $P_0 \in A$ y una base $\{v_1, \dots, v_n\}$ del espacio vectorial subyacente $E$. Denotamos la referencia cartesiana por $\mathcal{R} = \{P_0;\ v_1, \dots, v_n\}$.
\end{dfn}

Sea $\mathcal{R} = \{P_0;\ v_1, \dots, v_n\}$ una referencia cartesiana y sea $Q \in A$. Las coordenadas cartesianas de $Q$ respecto de $\mathcal{R}$ son los $n$ escalares únicos $(\alpha_1, \dots, \alpha_n)$ tales que $\overrightarrow{P_0 Q} = \alpha_1 v_1 + \dots  + \alpha_n v_n$.

\begin{ej}
	Sea $\mathcal{R} = \{(1,1);\ (1,2), \dots, (1,1)\}$, sea $Q = (1, 0)$. Entonces $[Q]_\mathcal{R} = (-1, 1)$. Podemos comprobar que $\overrightarrow{P_0 Q} = (0 -1)$ y que $(0, -1) = \alpha_1 v_1 + \alpha_2 v_2$ con $\alpha_1 = 1,\ \alpha_2 = -1$.
	
	Sean $P_0, P_1, \dots, P_k \in \mathbb{A}_\mathbb{R}^n$ afínmente independientes. Podemos definir $\mathcal{R} = \{P_0;\ \overrightarrow{P_0 P_1}, \dots, \overrightarrow{P_0 P_k}\}$. Y entonces quedan $[P_i]_\mathcal{R} = (0, \dots, 1, \dots, 0)$ (un $1$ en la $i$-ésima coordenada).
\end{ej}

\subsubsection{Ecuaciones de una variedad lineal en una referencia cartesiana} 

\begin{ej}
	Sea $\mathcal{L} = (1,2,1) + \langle 1, 0, 1 \rangle \in \mathbb{A}^3_\mathbb{R}$. ¿Cómo son todos los puntos de $\mathcal{L}$? Basta resolver $(x - 1, y - 2, z - 1) = \lambda (1,0,1)$ luego las ecuaciones paramétricas de $\mathcal{L}$ son
	\begin{align*}
		\left\lbrace\begin{array}{cc}
		x &= 1 + \lambda \\
		y &= 2 \\
		z &= 1 + \lambda
		\end{array}\right\rbrace
	\end{align*}
	
	Sea $\mathcal{R} = \{P_0 = (0,0,0);\ e_1, e_2, e_3\}$. Entonces $[(x,y,z)]_\mathcal{R} = (x, y, z)$. Veamos ahora las coordenadas de $\mathcal{L}$. $(x, y, z) \in \mathcal{L} \iff (x - 1, y - 2, z - 1) \in \langle(1, 0, 1) \rangle \iff$
	\begin{align*}
		\text{Rango }\left(\begin{array}{cc}
		1 & x - 1 \\
		0 & y - 2 \\
		1 & z - 1
		\end{array}\right) = 1 \iff
		\left\lbrace \begin{array}{cc}
			y - 2 &= 0 \\
			z - x &= 0
		\end{array}\right\rbrace
	\end{align*}
\end{ej}

\begin{ej}
	En $\mathbb{A}^3_\mathbb{R}$. Sea $\pi \equiv (1,2,1) + \langle (1,0,1), (0,1,2) \rangle$. Hallemos sus ecuaciones cartesianas respecto de $\mathcal{R} = \{(0,0,0);\ e_1, e_2, e_3\}$. Es decir, tenemos que ver cuando $(x, y, z) \in \pi$.
	\begin{multline*}
	(x,y,z) \in \pi \iff (x-1, y -2, z-1) \in \langle (1,0,1), (0,1,2) \rangle \iff\\
	\text{Rango }\left(\begin{array}{ccc}
	1 & 0 & x - 1 \\
	0 & 1 & y - 2 \\
	1 & 2&  z - 1
	\end{array}\right) = 2 \iff
	z - x - 2y + 4 = 0
	\end{multline*}
\end{ej}

Sean $R = \{P; u_1, \dots, u_n\}$ y $R' = \{Q; w_1, \dots, w_n\}$ dos sistemas de referencia cartesianos en $A$. Sea $X \in A: [X]_R = (x_1, \dots, x_n)$ y $[X]_{R'} = (x'_1, \dots, x_n')$. Supongamos que
\begin{align*}
	w_1 = \sum_{j=1}^{n} \beta_{1j} u_j,\quad \dots \quad w_n = \sum_{j=1}^n \beta_{nj} u_j
\end{align*}
de manera que tendríamos
\begin{align*}
	M_{BB'} = \left(\begin{array}{ccc}
		\beta_{11} & \dots  & \beta_{n1} \\
		  \vdots   & \ddots &   \vdots   \\
		\beta_{1n} & \dots  & \beta_{nn}
	\end{array}
	\right)
\end{align*}

\begin{tm}[Cambios entre referencias cartesianas]
	Sean $R = \{P;\ u_1, \dots, u_n\}$ y $R' = \{Q;\ w_1, \dots, w_n\}$ dos sistemas de referencia cartesianos en $A$. Sea $X \in A: [X]_R = (x_1, \dots, x_n)$ y $[X]_{R'} = (x'_1, \dots, x_n')$. Si llamamos $[Q]_R = (\lambda_1, \dots, \lambda_n)$, entonces la relación entre $[X]_R$ y $[X]_{R'}$ es:
	\begin{align*}
		[X]_R &= [Q]_R + M_{BB'}[X]_{R'} = \\
		\left(\begin{array}{c} x_1 \\ \vdots \\ x_n \end{array}\right) &= 
		\left(\begin{array}{c} \lambda_1 \\ \vdots \\ \lambda_n \end{array}\right) + 
		\left(\begin{array}{ccc}
			\beta_{11} & \dots & \beta_{n1} \\
			\vdots & \ddots & \vdots \\
			\beta_{1n} & \dots & \beta_{nn}
		\end{array}\right)
		\left(\begin{array}{c} x_1' \\ \vdots \\ x_n' \end{array}\right)
	\end{align*}
	donde $M_{BB'}$ es la matriz de cambio de base entre los vectores de las base $B'$ y $B$ de las referencias $R'$ y $R$ respectivamente, es decir la matriz de las coordenadas de los vectores de la referencia $R'$ en función de la referencia $R$ por columnas.
\end{tm}

\subsection{Sistemas de referencia baricéntricos}

Dar unas coordenadas respecto de un sistema de referencia cartesiano muchas overrightarrowes no nos da una idea de las distancias entre los puntos o sus posiciones relativas. Por eso inventamos las referencias baricéntricas.

\begin{dfn}[Referencia baricéntrica]
	Un sistema de referencia baricéntrico es una colección de $n+1$ puntos afínmente independientes en $A$ donde $\dim A = n$. Escribimos $R = \{P_0, P_1, \dots, P_n\}$.
\end{dfn}

¿Cómo se asignan coordenadas respecto a un sistema de referencia baricéntrico?

\begin{tm}
	Sea $P_0, P_1, \dots, P_n$ una base afín de $A$ y sea $Q \in A$. Entonces existen escalares únicos $\lambda_0, \lambda_1, \dots, \lambda_n \in K$ con $\lambda_0 + \lambda_1 + \dots + \lambda_n = 1$ de modo tal que $\forall P \in A$ se tiene que $\overrightarrow{PQ} = \lambda_0 \overrightarrow{P P_0} + \lambda_1 \overrightarrow{P P_1}  + \dots + \lambda_n \overrightarrow{P P_n}$. Diremos que $(\lambda_0, \dots, \lambda_n)$ son las coordenadas baricéntricas de $Q$ respecto de la referencia baricéntrica $\{P_0, P_1, \dots, P_n\}$.
\end{tm}

\begin{proof}
	\textbf{Unicidad.} supongamos existen $\lambda_0, \dots, \lambda_n \in K$ y $\mu_0, \dots, \mu_n \in K$ tales que $\forall P \in A$, se tiene a la vez que $\overrightarrow{PQ} = \lambda_0 \overrightarrow{P P_0} + \lambda_1 \overrightarrow{P P_1}  + \dots + \lambda_n \overrightarrow{P P_n} = \mu_0 \overrightarrow{P P_0} + \mu_1 \overrightarrow{P P_1}  + \dots + \mu_n \overrightarrow{P P_n}$. Restando $\overrightarrow{0} = (\lambda_0 - \mu_0)\overrightarrow{P P_0} + \dots + (\lambda_n - \mu_n)\overrightarrow{P P_n}$ con $(\lambda_0 - \mu_0) + \dots + (\lambda_n - \mu_n) = 0$. Observemos que nos encontramos en las hipótesis del segundo caso del teorema XXXXX, luego $\lambda_i = \mu_i = 0,\ i = 0, \dots, n$.
	
	\textbf{Existencia:} Sea $Q \in A$. $\overrightarrow{P_0 Q} = \alpha_1\overrightarrow{P_0 P_1} + \dots + \alpha_n \overrightarrow{P_0 P_n},\ \alpha_i \in K$. $\overrightarrow{P_0 P} + \overrightarrow{PQ} = \alpha_1 (\overrightarrow{P P_0} + \overrightarrow{P P_1}) + \dots + \alpha_n (\overrightarrow{P_0 P} + \overrightarrow{P P_n})$. De manera que $\overrightarrow{P Q} = (1 - \alpha_1 \dots - \alpha_n)\overrightarrow{P P_0} +  \alpha_1 \overrightarrow{P P_1} + \dots  + \alpha_n \overrightarrow{P P_n}$
\end{proof}

% 2017/10/31
La demostración nos da un método para obtener las coordenadas baricéntricas a partir de las coordenadas cartesianas (parte de la existencia). Es decir, eligiendo una referencia cartesiana adecuada $R' = \{P_0; \overrightarrow{P_0 P_1}, \dots, \overrightarrow{P_0 P_n}\}$, tendríamos las coordenadas $[Q]_{R'} = (\alpha_1, \dots, \alpha_n)$. Y es fácil obtener las coordenadas baricéntricas de $Q$ respecto de $R = \{P_0, \dots, P_n\}$: $[Q]_R = (1 - (\alpha_1 + \dots + \alpha_n), \alpha_1, \dots, \alpha_n)$.

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^2$. Sean $P_0 = (1,0), P_1 = (1, 1), P_2 = (2, 1)$. Comprobamos que los tres puntos son a. i. viendo que los overrightarrowtores $\overrightarrow{P_0 P_1}$ y $\overrightarrow{P_1 P_2}$ son linealmente independientes. Queremos ver las coordenadas de $Q$ respecto de la referencia baricéntrica $R = \{P_0, P_1, P_2\}$. Para ello construimos $R' = \{P_0; \overrightarrow{P_0 P_1}, \overrightarrow{P_0 P_2} \}$. Entonces podemos hallar $[Q]_{R'} = (2, -1)$ y fácilmente $[Q]_R = (0, 2, -1)$.
\end{ej}

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^n$. Si tenemos la referencia baricéntrica $R = \{P_0, \dots, P_n\}$. Las coordenadas $[P_i]_R = (0, \dots, 1, \dots, 0)$, donde el $1$ está en el lugar i-ésimo. Y construyendo la referencia cartesiana asociada $R' = \{P_0; \overrightarrow{P_0 P_1}, \dots, \overrightarrow{P_0 P_n}\}$ las coordenadas $[P_0]_{R'} = (0, \dots, 0)$ y las coordenadas $[P_i]_{R'} = (0, \dots, 1, \dots, 0),\ i = 1, \dots, n$ (un $1$ en el lugar i-ésimo).
\end{ej}

\begin{ej}[Intuición geométrica para coordenadas baricéntricas en la recta]
	En $\mathbb{A}_\mathbb{R}^1$. Sean $R = \{P_0, P_1\},\ R' = \{P_0, \overrightarrow{P_0 P_1}\},\ P_0 ≠ P_1$. Sea $Q \in \mathbb{A}_\mathbb{R}^1$. Las coordenadas $[Q]_R = (\alpha, \beta),\ \alpha + \beta = 1$.
	
	% TODO: add a picture
\end{ej}

\begin{ej}[Intuición geométrica para coordenadas baricéntricas en el plano]
	En $\mathbb{A}_\mathbb{R}^2$. Sean $R = \{P_0, P_1, P_2\},\ R' = \{P_0; \overrightarrow{P_0 P_1}, \overrightarrow{P_0 P_2}\}$. Sea $Q \in \mathbb{A}_\mathbb{R}^2$. Las coordenadas $[Q]_R = (x, y, z),\ x + y + z = 1$.
	
	% TODO: add another picture
	
	$\forall P \in \mathbb{A}_\mathbb{R}^2$ se tiene que $\overrightarrow{P Q} = x \overrightarrow{P P_0} + y \overrightarrow{P P_1} + z \overrightarrow{P P_2}$. Supongamos $P = P_0$, entonces $\overrightarrow{P_0 Q} = x \overrightarrow{P_0 P_0} + y \overrightarrow{P_0 P_1} + z \overrightarrow{P_0 P_2}$. 
	\begin{itemize}
		\item Si $Q \in P_0 + \langle \overrightarrow{P_0 P_1} \rangle$, entonces $z = 0$. Luego decimos que la recta $P_0 + \langle \overrightarrow{P_0 P_1}$ es la recta $z = 0$, y las posiciones de los puntos dentro de dicha recta nos las da la coordenada $y$.
		\item ¿Cuáles serían las ecuaciones baricéntricas de la recta $P_0 + \langle \overrightarrow{P_0 P_2}$? Esa recta sería la $y = 0$, y las diferentes posiciones en esa recta nos las da la coordenada $z$.
		\item Las coordenadas de la recta $P_0 + \langle \overrightarrow{P_1 P_2}\rangle$ son las $x = 0$, y las posiciones dentro de dicha recta nos las da otra vez $z$.
		\item Por último, podemos pensar en la recta paralela a $x=0$ que pasa por $P_0$. En coordenadas cartesianas es fácil ver que esta es la recta $y + x = 0$. Y como en baricéntricas tiene que ser $x + y + z = 1$ tenemos que dicha recta es la $x = 1$.
	\end{itemize}

	Observemos que las coordenadas baricéntricas nos dan una idea de si los puntos están dentro del triángulo. Cualquier punto dentro del triángulo tendrá coordenadas baricéntricas $(x, y, z)$ tal que $x, y, z \leq 1$.
\end{ej}


\begin{dfn}[Baricentro]
	Sean $P_0, \dots, P_m \in A$ puntos afínmente independientes en $A$. Llamamos baricentro de los puntos $P_0, P_1, \dots, P_m$ al puntos cuyas coordenadas baricéntricas respecto a $R = \{P_0, P_1, \dots, P_m\}$ son $(\frac{1}{m+1}, \dots, \frac{1}{m+1})$, (repetimos $m+1$ overrightarrowes).
	
	Nótese que esta referencia baricéntrica es para la variedad lineal $\mathcal{L}$ generada por los puntos $P_0, P_1, \dots, P_m$. En caso de que $\mathcal{L} \subsetneq A$, el resto de coordenadas serían $0$: $(\frac{1}{m+1}, \dots, \frac{1}{m+1}, 0, \dots, 0)$. 
\end{dfn}

\begin{tm}[Criterio de independencia afín respecto de coordenadas baricéntricas]
	Supongamos que $R = \{P_0, P_1, \dots, P_n\}$ es una referencia baricéntrica. Sean $Q_0, Q_1, \dots, Q_r \in A$, con $r \leq n$. Supongamos que $[Q_i]_R = (\alpha_{i0} \alpha_{i1}, \dots, \alpha_{in}),\ i = 0, \dots, r$ y tal que $\sum_{j=0}^{n} \alpha_{ij} = 1$. Los puntos $Q_0, \dots, Q_r$ son afínmente independientes si y solo si
	\begin{align*}
	\text{Rango }\left(\begin{array}{cccc}
	\alpha_{00} & \alpha_{10} & \dots & \alpha_{r0} \\
	\alpha_{01} & \alpha_{11} & \dots & \alpha_{r1} \\
	\vdots & \vdots & \ddots & \vdots \\
	\alpha_{0n} & \alpha_{1n} & \dots & \alpha_{rn} \\
	\end{array}
	\right) = r + 1 \iff\\
	\text{Rango }\left(\begin{array}{ccc}
	\alpha_{11} - \alpha_{01} & \dots & \alpha_{r1} - \alpha_{01} \\
	\vdots & \ddots & \vdots \\
	\alpha_{1n} - \alpha_{0n} & \dots & \alpha_{rn} - \alpha_{0n} \\
	\end{array}
	\right) = r
	\end{align*}
\end{tm}

\begin{proof}
	Con vistas a mirar el rango de la matriz, basta considerar la siguiente matriz
	\begin{align*}
		\text{Rango }\left(\begin{array}{cccc}
		1 & 1 & 1 & 1 \\
		\alpha_{01} & \alpha_{11} & \dots & \alpha_{r1} \\
		\vdots & \vdots & \ddots & \vdots \\
		\alpha_{0n} & \alpha_{1n} & \dots & \alpha_{rn} \\
		\end{array}
		\right) =
		\text{Rango }\left(\begin{array}{cccc}
		1 & 0 & 0 & 0 \\
		\alpha_{01} & \alpha_{11} - \alpha_{01} & \dots & \alpha_{r1} - \alpha_{01} \\
		\vdots & \vdots & \ddots & \vdots \\
		\alpha_{0n} & \alpha_{1n} - \alpha_{0n} & \dots & \alpha_{rn} - \alpha_{0n} \\
		\end{array}
		\right)
	\end{align*}
	Ahora bien, el rango de esta última matriz es igual al rango de la traspuesta del primer elemento más 1:
	\begin{align*}
		= 1 + \text{Rango }\left(\begin{array}{ccc}
		\alpha_{11} - \alpha_{01} & \dots & \alpha_{r1} - \alpha_{01} \\
		\vdots & \ddots & \vdots \\
		\alpha_{1n} - \alpha_{0n} & \dots & \alpha_{rn} - \alpha_{0n} \\
		\end{array}
		\right)
	\end{align*}
	Sabemos que $\forall P \in A,\ \overrightarrow{P Q_0} = \alpha_{00} \overrightarrow{P P_0} + \alpha_{01}\overrightarrow{P P_1} \dots + \alpha_{0n} \overrightarrow{P P_n}$. Análogamente, para $i = 1 \dots r,\ \overrightarrow{P Q_i} = \alpha_{i0} \overrightarrow{P P_0} + \alpha_{i1}\overrightarrow{P P_1} \dots + \alpha_{in} \overrightarrow{P P_n}$. Es decir que podemos escribir $\overrightarrow{P Q_i} - \overrightarrow{P Q_0} = -\overrightarrow{Q_0 Q_i} = (\alpha_{00} - \alpha_{i0})\overrightarrow{P P_0} + \dots + (\alpha_{0n} - \alpha_{in})\overrightarrow{P P_0}$. Tomando $P = P_0$ para $i = i \dots r,\ \overrightarrow{Q_0 Q_i} =  \overrightarrow{0} + (\alpha_{i1} - \alpha_{01})\overrightarrow{P_0 P_1} +  \dots + (\alpha_{in} - \alpha_{0n})\overrightarrow{P_0 P_n}$. Sabemos que $Q_0, \dots, Q_r$ son afínmente independientes $\iff,\ \overrightarrow{Q_0 Q_1}, \dots, \overrightarrow{Q_0 Q_r}$ son linealmente independientes $\iff$
	\begin{align*}
		\text{Rango }\left(\begin{array}{ccc}
		\alpha_{11} - \alpha_{01} & \dots & \alpha_{r1} - \alpha_{01} \\
		\vdots & \ddots & \vdots \\
		\alpha_{1n} - \alpha_{0n} & \dots & \alpha_{rn} - \alpha_{0n} \\
		\end{array}
		\right) = r
	\end{align*} 
\end{proof}

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^3$. Sean $P_0 = (0,0,0), P_1 = (1,0,0), P_2 = (0,1,0), P_3 = (0,0,1)$ puntos afínmente independientes que forman la referencia $R = \{P_0, P_1, P_2, P_3\}$. Queremos saber si los puntos $Q_0 (1, 2, 0), Q_1 = (2, 0, 0), Q_2  = (1,1,1)$ son linealmente independientes.
	
	Una opción sería comprobar si los overrightarrowtores $\overrightarrow{Q_0 Q_1} = (1, -2, 0), \overrightarrow{Q_0 Q_2} = (0, -1, 1)$ son linealmente independientes. Bastaría comprobar
	\begin{align*}
		\text{Rango }\left(\begin{array}{cc}
		1 & 0 \\ -2 & -1 \\ 0 & 1
		\end{array}\right) = 2
	\end{align*}
	
	Otra opción es utilizar el criterio anterior. Es fácil obtener las coordenadas de los puntos $Q_i$ ya que la referencia cartesiana asociada a $R$ es $R' = \{P_0; e_1, e_2, e_3\}$. Luego $[Q_0]_R = (-2, 1, 2, 0), [Q_1]_R = (-1, 2, 0, 0), [Q_2]_R = (-2, 1, 1, 1)$ (lo que hacemos es elegir la primera coordenada para que todas sumen 1). Entonces basta comprobar que
	\begin{align*}
		\text{Rango }\left(\begin{array}{ccc}
		-2 & -1 & -2 \\
		1 & 2 & 1 \\
		2 & 0 & 1 \\
		0 & 0 & 1
		\end{array}\right) = 3
	\end{align*}
\end{ej}

\begin{tm}[Cambios de coordenadas baricéntricas]
	Sean $R_1 = \{P_0, P_1, \dots P_n\},\ R_2 = \{Q_0, Q_1, \dots, Q_n\}$ referencias baricéntricas en $A$. Sea $X \in A$ tal que $[X]_{R_1} = (x_0, x_1, \dots, x_n),\ [X]_{R_2} = (x_0', x_1', \dots, x_n')$ con $\sum_{0}^{n} x_i = \sum_0^n x_i' = 1$. Supongamos que $[Q_i]_{R_1} = (\alpha_{i0}, \alpha_{i1}, \dots, \alpha_{in}),\ i = 0, \dots, n$.
	
	$\forall P \in A$, tenemos que $\overrightarrow{P X} = x_0 \overrightarrow{P P_0} + \dots + x_n \overrightarrow{P P_n}$ y $\overrightarrow{PX} = x_0' \overrightarrow{P Q_0} + \dots + x_n' \overrightarrow{P Q_n} = x_0' (\alpha_{00} \overrightarrow{P P_0} + \dots + \alpha_{0n} \overrightarrow{P P_n}) + x_1' (\alpha_{10} \overrightarrow{P P_0} + \dots + \alpha_{1n} \overrightarrow{P P_n}) + \dots + x_n' (\alpha_{n0} \overrightarrow{P P_0} +  \dots + \alpha_{nn} \overrightarrow{P P_n}) = (x_0' \alpha_{00} + x_1' \alpha_{10} + \dots + x_n' \alpha_{n0})\overrightarrow{P P_0} + \dots + (x_0' \alpha_{0n} + x_1' \alpha_{1n} + \dots + x_n' \alpha_{nn})\overrightarrow{P P_n}$. Con lo que quedarían las coordenadas $x_0 = x_0' \alpha_{00} + x_1' \alpha_{01} + \dots + x_n' \alpha_{0n}, \dots, x_n = x_0' \alpha_{0n} + x_1' \alpha_{1n} + \dots + x_n' \alpha_{nn}$. En forma matricial
	\begin{align*}
		[X]_{R_1} = M_{R_1 R_2}[X]_{R_2} = \left(
			\begin{array}{cccc}
				\alpha_{00} & \alpha_{10} & \dots & \alpha_{n0} \\
				\alpha_{01} & \alpha_{11} & \dots & \alpha_{n1} \\
				\vdots & \vdots & \ddots & \vdots \\
				\alpha_{0n} & \alpha_{1n} & \dots & \alpha_{nn} 
			\end{array}
		\right)\left(
			\begin{array}{c}
				x_0' \\ x_1' \\ \vdots \\ x_n'
			\end{array}
		\right)
	\end{align*} 
	donde la matriz $M_{R_1 R_2}$ es la formada por las coordenadas de los puntos que definen $R_2$ en función de la referencia $R_1$ en columnas. La matriz $M_{R_1 R_2}$ es invertible, por lo tanto el cambio contrario nos lo da $[X]_{R1} = M_{R_1 R_2}^{-1} [X]_{R1}= M_{R_2 R_1} [X]_{R1}$.
\end{tm}

\begin{ej}
	Sea $R_1 = \{(0, 0), (1, 0), (0, 1)\},\ R_2 = \{(1, 2), (2, 3), (1, 3)\}$. Necesitamos las coordenadas baricéntricas de los puntos que definen $R_2$ respecto de $R_1$. Como $R_1$ tiene referencia cartesiana asociada $R' = \{(0,0);\ e_1 = (1, 0), e_2 = (0, 1)\}$. Luego lo único que tenemos que hacer es ajustar la primera coordenada para que todas sumen 1:
	\begin{align*}
		[(1,2)]_{R_1} = (-2, 1, 2)\quad [(2, 3)]_{R_1} = (-4, 2, 3)\quad [(1, 3)]_{R1} = (-3, 1, 3) \\
		[X]_{R1} = M_{R_1 R_2}[X]_{R2} = \left(
			\begin{array}{ccc}
				-2 & -4 & 2 \\
				1 & 2 & 1 \\
				2 & 3 & 3
			\end{array}
		\right)
		\left(
			\begin{array}{c}
				x_0' \\ x_1' \\ x_2'
			\end{array}
		\right)
	\end{align*}
\end{ej}

\begin{ej}
	Aprovechamos el ejemplo anterior para hacer una cambio entre las referencias cartesianas asociadas: $R_1' = \{P_0 = (0, 0); \overrightarrow{P_0 P_1} = (1, 0), \overrightarrow{P_0 P_2} = (0,1)\},\ R_2' = \{Q_0 = (1, 2); \overrightarrow{Q_0 Q_1} = (1, 1), \overrightarrow{Q_0 Q_2} = (0,1)\}$. Buscamos la expresión para el cambio de coordenadas cartesianas: $[\overrightarrow{Q_0 Q_1}]_{B_1} = (1, 1),\ [\overrightarrow{Q_0 Q_2}]_{B_1} = (0, 1),\ [Q_0]_{R_1'} = (1, 2)$:
	\begin{align*}
		\left(
		\begin{array}{c}
		x_1 \\ x_2
		\end{array}
		\right) &= 
		\left(
		\begin{array}{c}
		1 \\ 2
		\end{array}
		\right) + 
		\left(
		\begin{array}{cc}
		1 & 0 \\ 1 & 1
		\end{array}
		\right)
		\left(
		\begin{array}{c}
		x_1'\\ x_2'
		\end{array}
		\right) \\
		\left(
		\begin{array}{c}
		x_1' \\ x_2'
		\end{array}
		\right) &= 
		\left(
		\begin{array}{c}
		-1 \\ -1
		\end{array}
		\right) + 
		\left(
		\begin{array}{cc}
		1 & 0 \\ -1 & 1
		\end{array}
		\right)
		\left(
		\begin{array}{c}
		x_1 \\ x_2
		\end{array}
		\right)
	\end{align*}
\end{ej}

\subsubsection{Ecuaciones de una variedad lineal en una referencia baricéntrica}

Veremos las ecuaciones baricéntricas de una variedad lineal a través de un ejemplo.

\begin{ej}
	En $\mathbb{A}^3_\mathbb{R}, L = (1, 2, 1) + \langle (1, 0, 1), (0, 1, 2)\rangle,\ R = \{P_0 (0, 0, 0), P_1 = (1, 0, 0), P_2 = (0, 1, 0), P_3 = (0, 1, 1)\}$. Buscamos la ecuación baricéntrica de $L$. Sea $X \in \mathbb{A}^3_\mathbb{R}, [A]_R = (x_0, x_1, x_2, x_3), x_0 + x_1 + x_2 + x_3 = 1$. Tomamos 3 puntos afínmente independientes en $L$: $A_1 = (1, 2, 1), A_2 = (4, 0, 0), A_3 = (2, 2, 2)$. Sabemos que $X \in L \iff A_1, A_2, A_3 y X$ no son afínmente independientes. Utilizaremos el criterio para decidir cuando dichos puntos son afínmente independientes para coordenadas baricéntricas: $A_1, A_2, A_3, X$ afínmente independientes $\iff$
	\begin{align*}
		\text{Rango } \left(
			\begin{array}{cccc}
				-3 & -3 & -5 & x_0 \\
				1  & 4 & 2 & x_1 \\
				2  & 0 & 2 & x_2 \\
				1  & 0 & 2 & x_3 \\
			\end{array}
		\right) = 3 \to 
		\left(\begin{array}{ccc|c}
		-3 & -3 & -5 & x_0 \\
		0  & 9 & 1 & 3x_1 + x_0 \\
		0  & 0 & -10 & 8x_0 + 6x_1 + 9x_2 \\
		0  & 0 & 0 & 4x_0 + 3x_1 + 2x_2 + 5x_3 \\
		\end{array}\right)
	\end{align*}
	Y obtenemos las ecuaciones baricéntricas de $L:$
	\begin{align*}
		L \equiv \left\lbrace\begin{array}{cc}
		4x_0 + 3x_1 + 2x_2 + 5x_3 &= 0\\
		x_0 + x_1 + x_2 + x_3 &= 0
		\end{array}\right\rbrace
	\end{align*}
\end{ej}

\section{Afinidades o aplicaciones afines}
Buscamos funciones que transformen un espacio afín en otro que lleven variedades lineales en otras variedades lineales.

\begin{dfn}
	Sean $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ espacios afines. Diremos que una función $f: A_1 \to A_2$ es una afinidad o aplicación afín si existe una aplicación lineal $\tilde{f} : E_1 \to E_2$ tal que $\forall P, Q \in A_1, \overrightarrow{f(P) f(Q)} = \tilde{f}(\overrightarrow{PQ})$.
\end{dfn}

Esto quiere decir que
% TODO: adjuntar dibujo

\begin{obs}
	Si $f : A_1 \to A_2$ es afín, entonces $f$ lleva la recta $P + \langle\overrightarrow{PQ}\rangle$ en la recta $f(P) + \langle\overrightarrow{f(P) f(Q)}\rangle$.
	
	\begin{proof}
		Si $S \in P + \langle\overrightarrow{PQ}\rangle$, ¿tenemos que $f(S) \in f(P) + \langle\overrightarrow{f(P) f(Q)}\rangle$?
		
		$S \in P + \langle\overrightarrow{PQ}\rangle \implies \overrightarrow{PS} = \alpha(\overrightarrow{PQ}),\ \alpha \in \mathbb{R}$. Observemos que $\overrightarrow{f(P) f(S)} \in \langle \overrightarrow{f(P) f(Q)}\rangle$ es equivalente a decir $\overrightarrow{f(P) f(S)} \in \langle \tilde{f}(\overrightarrow{f(P) f(Q)})\rangle$ que a su vez es equivalente a decir $\tilde{f}(\overrightarrow{PS}) = \alpha(\overrightarrow{PQ}) \in \langle \tilde{f}(\overrightarrow{f(P) f(Q)})\rangle$.
	\end{proof}
\end{obs}

%TODO: falta interpretación geométrica de la aplicación afín

\begin{ej}[Ejemplos de afinidades]
	¿Existe alguna aplicación afín? Veamos algunos ejemplos
	\begin{enumerate}
		\item $Id_A : A \to A,\ P \mapsto P$ con $(A, \psi, E)$. Tenemos que encontrar una aplicación lineal $\tilde{f} : E \to E$ tal que $\forall P, Q \in A, \overrightarrow{Id_A(P) Id_A(Q)} = \tilde{f}(\overrightarrow{PQ})$. Sí existe, de hecho es $\tilde{f} = Id_E$.
		\item Sean $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$. Consideramos la función constante: sea $S \in A_2, f: A_1 \to A_2, P \mapsto f(P) = S, \forall P \in A_1$. ¿Es $f$ afín?, es decir, ¿existe $\tilde{f} : E_1 \to E_2: \forall P, Q \in A, \overrightarrow{f(P)f(Q)} = \tilde{f}(\overrightarrow{PQ})$? Basta tomar $\tilde{f} = 0_{E_1 E_2} : E_1 \to E_2,\ u \mapsto \vec{0},\ \forall u \in E_1$. Es decir, cogemos la aplicación lineal constante entre $E_1$ y $E_2$. 
		\item \textbf{Traslaciones.} Fijamos $(A, \psi, E)$. Sea $u \in E$. Definimos $T_u : A \to A, P \mapsto T_u(P) = Q$ si $\overrightarrow{PQ} = u$. Observemos que $Q$ es único porque una vez fijado $P$, la función $\psi$ del espacio afín es biyectiva por definición.
		
		¿Es $T_u$ una afinidad? ¿Existe una aplicación lineal $\tilde{f} : E \to E$ tal que $\forall P, S \in A,\ \overrightarrow{\tilde{f}(P)\tilde{f}(S)} = \tilde{f}(\overrightarrow{PS})$? Basta tomar $\tilde{f} = Id_E$ porque $\overrightarrow{P f(P)} = \overrightarrow{S f(S)} = u$ y, por tanto, $\overrightarrow{PS} = \overrightarrow{f(P) f(S)}$ (por las propiedades de espacio afín).
	\end{enumerate}
\end{ej}

\begin{obs}
	Todas las traslaciones son afinidades con $\tilde{f} = Id_E$. En particular, la afinidad constante es una translación de vector $u = \vec{0}$.
\end{obs}

\begin{pro}
	Sea $f: A_1 \to A_2$ afín y sea $\tilde{f} : E_1 \to E_2$ su aplicación lineal asociada. Sea $L = P + F$ una variedad lineal en $A_1$. Entonces $f(L) = \{f(Q) : Q \in L\} = f(P) + \tilde{f}(L)$ que es una variedad lineal.
\end{pro}

\begin{proof}
	Tenemos que probar la doble inclusión puesto que se trata de una igualdad de conjuntos.
	
	¿$f(L) \subset f(P) + \tilde{f}(F)$? Sea $T \in f(L) \implies \exists Q \in L : f(Q) = T$. ¿$T \in f(P) + \tilde{f}(F)$? ¿$\overrightarrow{f(P) T} \in \tilde{f}(F)$? ¿$\overrightarrow{f(P) f(Q)} \in \tilde{f}(F)$? ¿$\tilde{\overrightarrow{PQ}} \in \tilde{f}(F)$? Sí porque $Q \in L \impliedby \overrightarrow{PQ} \in F$.
	
	¿$f(L) \supset f(P) + \tilde{f}(F)$? Sea $S \in f(P) + \tilde{f}(F) \implies \overrightarrow{f(P)S} \in \tilde{f}(F)$. ¿$S \in f(L)$? ¿$\exists Q \in L : f(Q) = S$? Como $\overrightarrow{f(P)S} \in \tilde{f}(F) \implies \exists v \in F : \overrightarrow{f(P)S} = \tilde{f}(v)$. Como $v \in F, \exists Q \in L : \overrightarrow{PQ} = v$. Es decir que $\overrightarrow{f(P)S} = \tilde{f}(v)$ equivale a decir $\overrightarrow{f(P)S} = \tilde{f}(\overrightarrow{PQ}) = \overrightarrow{f(P)f(Q)} \implies S = f(Q)$, por las propiedades de espacio afín.
\end{proof}

\begin{pro}
	Sean $f, g : A_1 \to A_2$ dos afinidades. Supongamos que $\exists P \in A_1 : f(P) = g(P)$ y que sus aplicaciones lineales asociadas coinciden $\tilde{f} = \tilde{g}$. Entonces $f = g$.
\end{pro}

\begin{proof}
	Para comprobar que dos funciones son iguales, tenemos que probar que toman el mismo valor en todos los puntos, es decir, que $\forall Q \in A_1, f(Q) = g(Q)$.
	Sabemos que, dado $Q \in A_1$, $\overrightarrow{f(P)f(Q)} = \tilde{f}(\overrightarrow{PQ}) = \tilde{g}(\overrightarrow{PQ}) = \overrightarrow{g(P)g(Q)} = \overrightarrow{f(P)g(Q)} \implies f(P) = g(Q)$ por las propiedades del espacio afín.
\end{proof}

\begin{tm}
	Sean $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ dos espacios afines. Sea $\varphi : E_1 \to E_2$ una aplicación linea, y sean $P \in A_1, Q \in A_2$. Entonces existe una única aplicación afín $f: A_1 \to A_2$ tal que $\tilde{f} = \varphi$ y $f(P) = Q$.
\end{tm}

Este teorema nos permite generar muchos ejemplos de aplicaciones afines, ya que dando una base para cada espacio vectorial subyacente obtenemos una aplicación lineal $\varphi$ y fijando la imagen de un punto obtenemos una aplicación afín.

\begin{proof}
	Por la proposición anterior, si $f: A_1 \to A_2$ existe, es necesariamente única.
	
	Definimos $f: A_1 \to A_2, P \mapsto f(P) = Q, S \mapsto f(S)$. Tenemos que decidir cómo definimos $f(S)$. Veamos que $\varphi(\overrightarrow{PS}) = w = \overrightarrow{QT} \in E_2$ para algún $T \in E_2$ que es único por definición de espacio afín. Entonces para que $\varphi(\overrightarrow{PS}) = \overrightarrow{f(P)f(S)} = \overrightarrow{Q f(S)}$ sabemos que tiene que ser $f(S) = T$. Una vez definida $f$, tenemos que comprobar que es afín, es decir que $\forall R, S \in A_1, \overrightarrow{f(R) f(S)} = \varphi(\overrightarrow{RS})$. $\overrightarrow{f(R) f(S)} = \overrightarrow{f(R) f(P)} + \overrightarrow{f(P) f(S)} = -\overrightarrow{f(P) f(R)} + \overrightarrow{f(P) f(S)} = -\varphi(\overrightarrow{PR}) + \varphi(\overrightarrow{PS}) = \varphi(\overrightarrow{PS} - \overrightarrow{PR}) = \varphi(\overrightarrow{RS})$.
\end{proof}

\begin{tm}
	Dados $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ espacios afines, dados $P_0, P_1, \dots, P_N \in A_1$ que formen una base afín en $A_1$, y dados $Q_0, Q_1, \dots, Q_n \in A_2$ puntos arbitrarios, existe una única afinidad $f : A_1 \to A_n$ tal que $f(P_i) = Q_i$ para $i = 0, 1, \dots, n$.
\end{tm}

\begin{proof}
	Definimos $\varphi : E_1 \to E_2, \overrightarrow{P_0 P_i} \mapsto \varphi(\overrightarrow{P_0 P_i}) = \overrightarrow{Q_0 Q_i}$ para $i = 0, \dots, n$. Tal aplicación lineal existe y es única porque $\overrightarrow{P_0 P_1}, \dots, \overrightarrow{P_0 P_n}$ es una base de $E_1$. Ahora definimos $f : A_1 \to A_2, P_0 \mapsto Q_0$. Por el teorema anterior, existe una única $f$ afín tal que $f(P_0) = Q_0$ y que $\tilde{f} = \varphi$.
	
	Veamos ahora que $f(P_i) = Q_i$ para $i = 1, \dots, n$. Como $f$ es afín, $\forall P_i,\ i = 1, \dots, n,\ \overrightarrow{f(P_0) f(P_i)} = \varphi(\overrightarrow{P_0 P_i}) = \overrightarrow{Q_0 Q_i},\ i = 1, \dots, n$. Pero $\overrightarrow{Q_0 Q_i} = \overrightarrow{f(P_0) Q_i} \implies f(P_i) = Q_i,\ i = 1, \dots, n$. 
\end{proof}

Vista la gran variedad de afinidades que podemos encontrar con estos métodos, bucaremos la relación entre $f$ y $\tilde{f}$.

\begin{tm}[Relaciones entre una afinidad y su aplicación lineal]
	Se dan las siguientes:
	\begin{enumerate}
		\item Si $f: A_1 \to A_2$ afín y $g : A_2 \to A_3$ afín, entonces $g \circ f : A_1 \to A_3$ es afín y $\widetilde{g \circ f} = \tilde{g} \circ \tilde{f}$. Ojo cuidao que el gorro de las aplicaciones lineales asociadas no tiene que ver con el gorro de las aplicaciones adjuntas.
		\item Supongamos $f: A_1 \to A_2$ afín. Entonces $f$ es inyectiva $\iff \tilde{f}$ es inyectiva.
		\item Igualmente, si $f: A_1 \to A_2$ afín, $f$ es sobreyectiva $\iff \tilde{f}$ es sobreyectiva.
		\item $f: A_1 \to A_2$ afín es biyectiva $\iff \tilde{f}$ es biyectiva.
		\item Si $f$ es biyectiva y afín, entonces su inversa $f^{-1}$ es afín y $\widetilde{f^{-1}} = \tilde{f}^{-1}$.
	\end{enumerate}
\end{tm}

\begin{proof}
	Probaremos algunas de las propiedades:
	\begin{enumerate}
		\item Tenemos que probar que $\exists \varphi : E_1 \to E_2$ lineal tal que $\forall P, Q \in A,\\ \overrightarrow{(g \circ f)(P)(g \circ f)(Q)} = \varphi (\overrightarrow{PQ})$. $\overrightarrow{(g \circ f)(P)(g \circ f)(Q)} = \overrightarrow{g(f(P)) g(f(Q))} = \tilde{g}(\overrightarrow{f(P)f(Q)}) = \tilde{g}(\tilde{f}(\overrightarrow{PQ})) = \tilde{g} \circ \tilde{f}(\overrightarrow{PQ})$ que es lineal, luego basta tomar $\varphi = \tilde{g} \circ \tilde{f}$.
		\item Veamos ($\implies$). Sabemos que si $P ≠ Q \implies f(P) ≠ f(Q)$. Veamos si $\tilde{f}$ es inyectiva. Supongamos que $\exists v \in E_1, v≠\vec{0}_{E_1} : \tilde{f} (v) = \vec{0}_E$. Como $v ≠ \vec{0}_{E_1}$ tienen que existir $S, T \in A_1, S ≠ T : \overrightarrow{ST} = v$. $\overrightarrow{f(S)f(T)} = \tilde{f}(\overrightarrow{ST}) = \tilde{f}(v) = \vec{0}_{E_1}$. Pero no puede ocurrir que $\overrightarrow{f(S)f(T)} = \vec{0}_{E_1}$, es decir que $f(S) = f(T)$ y como $f$ inyectiva es necesario que $S = T$, luego en realidad $v = \vec{0}_{E_1}$.
		\item Probaremos ($\impliedby$). Suponemos $\tilde{f}$ sobreyectiva, es decir, que $\forall w \in E_2, \exists v \in E_1 : \tilde{f}(v) = w$. Tenemos que probar que $\forall S \in A_2, \exists Q \in A_1 : f(Q) = S$. $\exists T \in A_2 : T \in \text{Im} f$, s decir que $\exists P \in A_1$ con $f(P) = T$. $\overrightarrow{TS} \in E_2 \implies \exists u \in E_1 : \tilde{f}(u) = \overrightarrow{TS}$. Podemos suponer que $u = \overrightarrow{PQ}$ para algún $Q \in A$. $\tilde{f}(u) = \tilde{\overrightarrow{PQ}} = \overrightarrow{f(P)f(Q)}$ pero como $f(u) = \overrightarrow{TS} = \overrightarrow{f(P)S}$ tenemos que $\overrightarrow{f(P) f(Q)} = \overrightarrow{F(P) S} \implies f(Q) = S$.
		\item Es consecuencia de 2 y 3.
		\item Sabemos que $f$ es afín y biyectiva $\implies f^{-1} : A_2 \to A_1$ está definida. Tenemos que ver si $f^{-1}$ es afín. Hipótesis: $f$ biyectiva $\implies \tilde{f}$ biyectiva $\implies \exists \tilde{f}^{-1} : E_2 \to E_1$ que es lineal. Para ver si $f^{-1}$ es afín tenemos que encontrar $\varphi : E_2 \to E_1$ tal que $\forall T, S \in A_2,\ \overrightarrow{f^{-1}(T)f^{-1}(S)} = \varphi(\overrightarrow{TS})$. Basta comprobar que $\forall T, S \in A_2, \overrightarrow{f^{-1}(T)f^{-1}(S)} = \tilde{f}(\overrightarrow{TS})$.
		
		Sean $T, S \in A_2$. $\overrightarrow{f^{-1}(T)f^{-1}(S)} = \overrightarrow{PQ}$ si $f(P) = T$ y $f(Q) = S$. Sabemos que como $f$ es afín $\overrightarrow{f(P) f(Q)} = \tilde{f}(\overrightarrow{PQ})$ pero también sabemos que $\overrightarrow{f(P) f(Q)} = \overrightarrow{TS}$. Ahora bien, $\tilde{f}(\overrightarrow{PQ}) = \overrightarrow{TS} \implies \tilde{f}^{-1} (\overrightarrow{TS}) = \overrightarrow{PQ} \implies f^{-1}$ es afín y $\widetilde{f^{-1}} = \tilde{f}^{-1}$.
	\end{enumerate}
\end{proof}

\begin{dfn}[Afinidad isomorfismo]
	Se dice que $f : A_1 \to A_2$ es un isomorfismo si es biyectiva y su inversa $f^{-1} : A_2 \to A_1$ es afín.
\end{dfn}

\begin{tm}
	Por lo visto anteriormente, $f$ afín es un isomorfismo $\iff f$ es biyectiva $\iff \tilde{f}$ es biyectiva.
\end{tm}

\begin{dfn}[Espacios afines isomorfos]
	Se dice que dos espacios afines $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ son isomorfos (como espacios afines) si existe una aplicación afín $f: A_1 \to A_2$ que sea isomorfismo.\footnote{No tienen que ser todas las afinidades isomorfismo, con que encontremos una que lo sea vale. Tampoco por que dos espacios afines sean isomorfos, toda afinidad entre ellos será un isomorfismo.}
\end{dfn}

\begin{tm}
	Dos espacios afines $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ son isomorfos $\iff \exists f : A_1 \to A_1$ isomorfismo $\iff \exists \tilde{f} : E_1 \to E_2$ isomorfismo de espacios vectoriales $\iff \dim E_1 = \dim E_2 \iff \dim A_1 = \dim A_2$.
\end{tm}

\subsection{Afinidades en coordenadas cartesianas}
Sean en esta sección $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ espacios afines y $f : A_1 \to A_2$ una afinidad. Sean $R_1 = \{P;\ v_1, \dots, v_n\}, R_2 = \{Q;\ w_1, \dots, w_n\}$ referencias cartesianas de los espacios $A_1$ y $A_2$ respectivamente.

\textbf{Objetivo:} nos proponemos encontrar expresiones que nos conviertan las coordenadas de un punto $S \in A_1$ en las coordenadas de la imagen de ese punto en $A_2$:
\begin{align*}
	[S]_{R_1} = (x_1, \dots, x_n) \longmapsto [f(S)]_{R_2} = (y_1, \dots, y_n)
\end{align*}

Recordemos que por definición $[S]_{R_1} = (x_1, \dots, x_n) \iff \overrightarrow{PS} = x_1 v_1  + \dots + x_n v_n$. Análogamente $[f(S)]_{R_2} = (y_1, \dots, y_n) \iff \overrightarrow{Q f(S)} = y_1 w_1 + \dots + y_n w_n$. Como $f$ es afín, $\tilde{f}$ es lineal y por tanto $\overrightarrow{f(P)f(S)} = \tilde{f}(\overrightarrow{PS}) = x_1 \tilde{f}(v_1) + \dots + x_n \tilde{f}(v_n)$. También tenemos que $\overrightarrow{f(P)f(S)} = \overrightarrow{f(P)Q} + \overrightarrow{Qf(S)} = x_1 \tilde{f}(v_1) + \dots + x_n \tilde{f}(v_n) \implies \overrightarrow{Q f(S)} = \overrightarrow{Q f(P)} + x_1 \tilde{f} (v_1) + \dots + x_n \tilde{f} (v_n)$. Expandimos $\overrightarrow{Q f(P)}$ en su expresión en coordenadas: $\overrightarrow{Q f(S)} = \lambda_1 w_1 + \dots + \lambda_n w_n + x_1 \tilde{f} (v_1) + \dots + x_n \tilde{f} (v_n)$. La expresión en coordenadas de la segunda mitad de la derecha de la igualdad se corresponde con $M_{R_2 R_1}(\tilde{f})$, luego, si $[f(P)]_{R_2} = (\lambda_1, \dots, \lambda_n)$:
\begin{align*}
	[S]_{R_2} &= [f(P)]_{R_2} + M_{R_2 R_1}(\tilde{f}) [X]_{R_1} \\
	\left(
	\begin{array}{c}
	y_1 \\ y_2 \\ \vdots \\ y_n
	\end{array}
	\right) &= 
	\left(
	\begin{array}{c}
	\lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n
	\end{array}
	\right) + M_{R_2 R_1}\left(
	\begin{array}{c}
	x_1 \\ x_2 \\ \vdots \\ x_n
	\end{array}
	\right)
\end{align*}

\subsection{Ejemplos de tipos de afinidades}

\subsubsection{Traslaciones}
Dado un espacio afín $(A, \psi, E)$ y un vector $u \in E$
\begin{align*}
	T_u :\ &A \longrightarrow A \\
	&P \longmapsto Q \text{ si } \overrightarrow{PQ} = u
\end{align*}
con $\tilde{T_u} = Id_{E_1} \implies T_u$ es biyectiva y por tanto un isomorfismo.

Fijada una referencia cartesiana en $A$: $R = \{P_0;\ u_1, \dots, u_n\},\ \dim A = n$, vamos a escribir la expresión de $T_u$ en coordenadas. $[T_u(P_0)]_R = [Q]_R = [\overrightarrow{P_0 Q}]_B = (a_1, \dots, a_n)$. $T_u(P_0) = Q \iff \overrightarrow{P_0 Q} = a_1 u_1 + \dots + a_n u_n$.
\begin{align*}
	\left(\begin{array}{c}
	y_1 \\ y_2 \\ \vdots \\ y_n
	\end{array}
	\right) = 
	\left(
	\begin{array}{c}
	a_1 \\ a_2 \\ \vdots \\ a_n
	\end{array}
	\right) + I_n \left(
	\begin{array}{c}
	x_1 \\ x_2 \\ \vdots \\ x_n
	\end{array}
	\right)
\end{align*}

\begin{pro}
	Sea $f: A_1 \to A_1$ afín. Si $\tilde{f} = Id_{E_1}$ entonces $f$ es una traslación.
\end{pro}

\begin{proof}
	Sea $P \in A_1$ y sea $Q = f(P)$ Si $f$ es una traslación entonces necesariamente $f = T_{\overrightarrow{PQ}}$. ¿Cómo probamos esto último? Vemos que, como $T_{\overrightarrow{PQ}}$ es una afinidad, basta con la imagen y una aplicación lineal para definirla. Estos son $T_{\overrightarrow{PQ}}(P) = Q$ y $\widetilde{T_{\overrightarrow{PQ}}} = Id_E$. En el caso de $f$, tenemos $\tilde{f} = Id_E$ y $f(P) = Q$ luego $f = T_{\overrightarrow{PQ}}$.
\end{proof}

\begin{tm}[Propiedades de las traslaciones]
	Son las siguientes:
	\begin{enumerate}
		\item Dados $P, Q \in A$, existe una única traslación $T_u : A \to A$ tal que $T_u(P) = Q$.
		\item Sean $u, w \in E$. Sean $T_u, T_w$ las traslaciones de vectores $v$ y $w$ respectivamente. Entonces la composición $T_u \circ T_w = T_{u + w}$.\footnote{Este ``entonces'' dice dos cosas: 1. la composición de traslaciones es una traslación y 2. la composición de dos traslaciones es la traslación de vector suma.}
		\item $\forall u \in E, T_u$ es biyectiva $\implies T_u^{-1} : A \to A$ es afín y $T_u^{-1} = T_{-u}$.
	\end{enumerate}
\end{tm}

\begin{proof}
	Probamos cada propiedad:
	\begin{enumerate}
		\item Definimos $f: A \to A$ tal que $f(P) = Q, \tilde{f} = Id_E \implies f = T_u$.
		\item Primero vemos que $T_u \circ T_w$ es una traslación: $\widetilde{T_u \circ T_w} = \tilde{T_u} \circ \tilde{T_w} = Id_E \circ Id_E = Id_E \implies T_u \circ T_w$ es una traslación. Ahora veamos cuál es el vector traslación: sea $P ºin A, T_u \circ T_w (P) = T_u (T_w (P)) = T_u (Q) = S$ donde $\overrightarrow{PQ} = w$ y $\overrightarrow{QS} = u$. $T_{u + w}(P) = T$ si $\overrightarrow{PT} = u + w$. Pero además sabemos que $\overrightarrow{PQ} + \overrightarrow{QS} = \overrightarrow{PS}$, luego $T = S$. Como hemos visto que $T_u \circ T_w (P) = T_{u + w}(P)$ y $T_u \circ T_w$ es traslación, concluimos que $T_u \circ T_w = T_{u + w}$ es una traslación de vector $u + w$.
	\end{enumerate}	
\end{proof}

\subsubsection{Proyecciones}
\begin{dfn}
	Diremos que una finidad $f : A \to A$ es una proyección si $f^2 = f$.
\end{dfn}

Objetivo: estudiar el efecto geométrico de $f$ en $A$.
\begin{pro}
	Sea $f: A \to A$ una proyección. Entonces $f$ está en alguno de los siguientes casos:
	\begin{enumerate}
		\item $f: A \to A$ es contante, es decir $\text{Im } f = \{Q\}, Q \in A$,
		\item $f: A \to A$ es la identidad $f = Id_A$,
		\item $f$ es una proyección sobre una variedad lineal $P + F$ de puntos fijos $G$ donde $F = \ker(\tilde{f} - Id_E); G = Ker(\tilde{f})$ y $\dim P + F \geq 0$.
	\end{enumerate}
\end{pro}

\begin{proof}
	Sabemos que $f^2 = f \implies \tilde{f}^2 = \tilde{f} \implies \tilde{f}$ es una proyección $\implies m_{\tilde{f}}(x) \vert x^2 - x = x(x - 1)$.  
	\begin{enumerate}
		\item Si $m_{\tilde{f}}(x) = x \implies \tilde{f} = 0$ (aplicación lineal 0).
		\item Si $m_{\tilde{f}}(x) = (x - 1) \implies \tilde{f} = Id_E \implies f = T_u$ para algún $u \in E$. Como $f^2 = f \implies T_u \circ T_u = T_u$, tenemos que $T_{2u} = T_u$ luego $2u = u \implies u = \vec{0}_E$. Entonces $f = T_{\vec{0}} \implies f = Id_A$.
		\item Si $m_{\tilde{f}}(x) = x(x - 1)$ entonces los autovalores de $\tilde{f}$ son $0, 1$. Podemos escribir$E$ como la suma directa de la dirección de la proyección más el espacios sobre el que se proyecta: $E = \ker \tilde{f} \oplus \ker (\tilde{f} - Id_E) = G + F$, respectivamente.
	\end{enumerate}
\end{proof}

¿Qué podemos decir de $f$?
\begin{pro}
	$f$ tiene algún punto fijo.
\end{pro}

\begin{proof}
	Veamos que $\exists P \in A : f(P) = P$. Sea $S \in A$. Sabemos que $f^2(S) = f(S)$ luego $f(f(S)) = f(S); f(P) = P$.
\end{proof}

Pero, ¿cuáles son dichos puntos fijos?
\begin{pro}
	$\Lambda = \{\text{puntos fijos de } f\} = P + F$.
\end{pro}

\begin{proof}
	Veamos que $\Lambda \subset P + F$. Sea $T \in \Lambda \implies f(T) = T$. ¿$T \in P + F$? Tenemos que ver si $\overrightarrow{PT} \in F$, es decir si $\tilde{f}(\overrightarrow{PT}) = \overrightarrow{PT}$. Sabemos, porque $f$ es afinidad, que $\tilde{f}(\overrightarrow{PT}) = \overrightarrow{f(P)f(T)} = \overrightarrow{PT}$ pues $T$ y $P$ son puntos fijos por $f$. Ahora veamos que $P + F \subset \Lambda$. Sea $Q \in P + F$, ¿$Q \in \Lambda$?. Para verlo tenemos que comprobar si $f(Q) = Q$. Sabemos que $Q \in P + F \implies \overrightarrow{PQ} \in F \implies \tilde{f}(\overrightarrow{PQ}) = \overrightarrow{PQ}$. Pero también, como antes, $\tilde{f}(\overrightarrow{PQ}) = \overrightarrow{f(P) f(Q)} = \overrightarrow{P f(Q)}$ por ser $P$ punto fijo. Entonces como $\overrightarrow{PQ} = \overrightarrow{P f(Q)}$ tenemos que $Q$ es un punto fijo luego $Q \in \Lambda$.
\end{proof}

\begin{pro}
	Para $S \in A$, $f(S) = (S + G) \cap (P + F)$
\end{pro}
\begin{proof}Tenemos que probar dos cosas:
	
	Veamos primero que $\forall S \in A,\ (S + G) \cap (P + F)$ es  (único) punto. Comprobamos que la intersección es no vacía porque $\overrightarrow{SP} \in F \oplus G = E$. Probemos que $\vert (S + G) \cap (P + F)\vert = 1$. $(S + G) \cap (P + F) = \text{ un punto } + F \cap G = \text{ un punto } + \vec{0}$.
	
	Veamos ahora que $\forall S \in A, f(S) \in (S + G) \cap (P + G)$. Para ello comprobamos que $f(S) \in P + F$ porque $f(S)$ es un punto $\implies \overrightarrow{P f(S)}$ es un vector fijo. Y comprobamos también que $f(S) \in S + G$. ¿$\overrightarrow{S f(S)} \in G$? ¿$\tilde{f}(\overrightarrow{S f(S)}) = \vec{0}_E$? $\tilde{f}(\overrightarrow{S f(S)}) = \overrightarrow{f(S) f(f(S))} = \overrightarrow{f(S) f(S)} = \vec{0}_E$. Entonces $f(S) \in P + F$ y $f(S) \in S + G$ luego $f(S) \in (P + F) \cap (S + G)$. 
\end{proof}

Si $f$ tiene dos puntos fijos distintos entonces $\tilde{f}$ tiene un autovalor que vale 1. ¿Si $\tilde{f}$ tiene un autovalor que vale 1 entonces $f$ tiene puntos fijos? No! Veamos por ejemplo las traslaciones. Si $\tilde{f}$ tiene un autovalor que vale 1 y $f$ tiene una punto fijo, entonces $f$ en realidad tiene toda una variedad lineal de puntos fijos. Observación: $f$ puede tener un (único) punto fijo y $\tilde{f}$ no tener un autovalor que valga 1 (como es el caso de las homotecias que veremos más adelante).

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^3$. Sea $R = \{P_0 = (0,0,0); e_1 = (1,0,0), e_2 = (0,1,0), e_3 = (0,0,1)\}$. Calculemos la expresión en coordenadas de la proyección sobre la recta $L = (1,1,1) + \langle (1,0,1) \rangle$ con dirección $G = \langle (0,1,1), (1,1,0) \rangle$.
	
	Buscamos un sistema de referencia en el que sea fácil expresar la proyección: $R' = \{Q_0 = (1,1,1); u_1 = (1,0,1), u_2 = (0,1,1), u_3 = (1,1,9)\}$. Sea $B' = \{u_1, u_2, u_3\}$ la base formada por los vectores de nuestra nueva referencia cartesiana. Por la fórmula de la expresión en coordenadas cartesianas de una afinidad:
	\begin{align*}
		[X]_{R'} &= [f(Q_0)]_{R'} + M_{B'}(f) [X]_{R'} \\
		\left(\begin{array}{c}
		x' \\ y' \\ z'
		\end{array}\right) &=
		\left(\begin{array}{c}
		0 \\ 0 \\ 0
		\end{array}\right) + 
		\left(\begin{array}{ccc}
		1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0
		\end{array}\right)
		\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right)
	\end{align*}
	
	Pero en realidad queremos la expresión de las coordenadas respecto de $R$. Para obtenerlas hacemos un cambio de coordenadas:
	\begin{align*}
		[X]_R &= [Q_0)]_R + M_{BB'} [X]_{R'} \\
		\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right) &=
		\left(\begin{array}{c}
		1 \\ 1 \\ 1
		\end{array}\right) + 
		\left(\begin{array}{ccc}
		1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 1 & 0
		\end{array}\right)
		\left(\begin{array}{c}
		x' \\ y' \\ z'
		\end{array}\right)
	\end{align*}
	Pero entonces tendríamos que hacer dos cambios. Para calcular las coordenadas de la imagen de un punto $P$ por $f$ respecto de $R$ ($[f(P)]_R$), tendríamos que hallar $[P]_{R'}$ haciendo un cambio, posteriormente sustituir las coordenadas obtenidas en la primera expresión para obtener $[f(P)]_{R'}$ y finalmente volver a sustituir en la segunda expresión para obtener $[f(P)]_R$.
	
	Este método requiere muchos cálculos por lo que buscamos otra solución. Definimos $R'' = \{Q_0 = (1,1,1); e_1, e_2, e_3\}$. Entonces tendríamos la siguiente expresión para las coordenadas de $f(X)$ cuando $X$ está expresado en términos de $R''$:
		\begin{align*}
	[f(X)]_{R''} &= [f(Q_0)]_{R'} + M_{B'B}^{-1}M_{B'}(f)M_{B'B} [X]_{R''} \\
	\left(\begin{array}{c}
	x'' \\ y'' \\ z''
	\end{array}\right) &=
	\left(\begin{array}{c}
	0 \\ 0 \\ 0
	\end{array}\right) + M_{B'B}^{-1}
	\left(\begin{array}{ccc}
	1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0
	\end{array}\right) M_{B'B}
	\left(\begin{array}{c}
	x'' \\ y'' \\ z''
	\end{array}\right)
	\end{align*}
	Aquí utilizamos $M_{B'B} = M_{B'B''}$ pues $B''$ y $B$ tienen la misma base para su espacio vectorial. Ahora transformamos la expresión para que las coordenadas de $X$ y $f(X)$ estén escritas respecto de $R$. Sabemos que $R''$ y $R$ son en realidad la misma referencia solo que con un origen distinto, luego podemos cambiar entre ellas sumando un punto al que llamaremos $(a, b, c)$:
		\begin{align*}
	[f(X)]_R &= (a,b,c) + M_{BB'}^{-1}M_{B'}(f)M_{BB'} [X]_R \\
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right) &=
	\left(\begin{array}{c}
	a \\ b \\ c
	\end{array}\right) + M_{BB'}^{-1}
	\left(\begin{array}{ccc}
	1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0
	\end{array}\right) M_{BB'}
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right)
	\end{align*}
	Para obtener $(a, b, c)$ forzamos que $(a,b,c) + M_B({\tilde{f}})[Q_0]_R = [Q_0]_R$. Así nos queda la expresión:
	\begin{align*}
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right) &=
	\left(\begin{array}{c}
	1/2 \\ 0 \\ 1/2
	\end{array}\right) +
	\left(\begin{array}{ccc}
	1/2 & -1/2 & 1/2 \\ 0 & 0 & 0 \\ 1/2 & -1/2 & 1/2
	\end{array}\right)
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right)
	\end{align*}
\end{ej}

\paragraph{Simetrías}

\begin{dfn}
	Diremos que una afinidad $f: A \to A$ es una simetría si $f^2 = Id_A$.
\end{dfn}

Tenemos como objetivo dar una interpretación el efecto geométrico de $f$ en $A$.

\begin{obs}
	$f^2 = Id_A \implies \widetilde{f^2} = \widetilde{Id_A} = Id_E$  y además $\tilde{f}^2 = \widetilde{f^2}$ luego $\tilde{f}^2 = Id_E$ y $\tilde{f}$ es a su vez una simetría.
\end{obs}

\begin{obs}
	Una simetría siempre tiene puntos fijos.
\end{obs}
% poner un dibujito de por qué el baricentro entre Q y f(Q) es un punto fijo
\begin{proof}
	Sea $T \in A$. Vamos a probar que el punto medio entre $T$ y $f(T)$ es un punto fijo por $f$.
	\begin{itemize}
		\item Si $T = f(T)$: $T$ ya es un punto fijo.
		\item Si $T ≠ f(T)$: entonces $T$ y $f(T)$ determinan una recta $L$ y forman una referencia baricéntrica de la recta que generan $R' = \{T, f(T)\}$.  Si $Q$ es el punto medio de $T$ y $f(T) \implies \forall P \in L,\, \overrightarrow{PQ} = 1/2 \overrightarrow{PT} + 1/2\overrightarrow{Pf(T)} \implies \tilde{f}(\overrightarrow{PQ}) = 1/2 \tilde{f}(\overrightarrow{PT}) + 1/2\tilde{f}(\overrightarrow{Pf(T)});\ \overrightarrow{f(P)f(Q)} = 1/2 \overrightarrow{f(P) f(T)} + 1/2 \overrightarrow{f(P f^2(T))} = 1/2 \overrightarrow{f(P)f(T)} + 1/2 \overrightarrow{f(P)T}$. ¿Es $Q = f(Q)$? La recta $L = T + \langle \overrightarrow{Tf(T)}\rangle = f(T) + \langle \overrightarrow{f(T) T}\rangle$.
	\end{itemize}
\end{proof}

\begin{tm}
	Sea $f: A \to A$ una simetría. Entonces
	\begin{enumerate}
		\item o bien $f = Id_A$;
		\item o bien $f$ es una simetría central (y por tanto tiene un único punto fijo);
		\item o bien $f$ es una simetría respecto a una variedad lineal de puntos fijos $P + F$, con $n > \dim F \geq 1$.
	\end{enumerate}
\end{tm}

\begin{proof}
	Como $f^2 = Id_A \implies \tilde{f}^2 = Id_E \implies m_{\tilde{f}}(x) \vert x^2 - 1$.
	\begin{enumerate}
		\item Si $m_{\tilde{f}}(x) = x - 1 \implies \tilde{f} = Id_E \implies f = T_u$ para algún $u \in E$. Como $f^2 = Id_A \implies T_2u = Id_A = T_{\vec{0}} \implies 2u = \vec{0} \implies u = \vec{0}$, lo que prueba el primer caso.
		
		\item Si $m_{\tilde{f}}(x) = x + 1 \implies \tilde{f} = - Id_E$. ¿Qué efecto tiene $f$ sobre los puntos de $A$? Sabemos que $f$ tiene al menos un punto fijo. Sea $Q$ un punto fijo de $f$. ¿Puede $f$ tener más puntos fijos? No porque si los tuviera $\tilde{f}$ tendría un autovector con autovalor $1$. $\overrightarrow{QP} \to \tilde{f}(\overrightarrow{QP}) = - \overrightarrow{QP}$. $\overrightarrow{f(Q)f(P)} = \overrightarrow{Qf(P)}$, lo que prueba? el segundo caso.
		
		\item Si $m_{\tilde{f}}(x) = (x - 1)(x + 1)$ entonces $\ker (f - Id_E) = F$ es un subespacio con $1 \leq \dim F < n$ formado por los vectores fijos de $\tilde{f}$. Como $f$ tiene al menos un punto fijo $Q$ decidimos que $Q + F$ es una variedad lineal de puntos fijos ($Q + F$ es de hecho el conjunto de puntos fijos de $f$). Y esto prueba el tercer caso.
	\end{enumerate}
\end{proof}

Efecto geométrico de $f$:
% TODO: poner un dibujito

\begin{ej}
	Calcula la expresión analítica en coordenadas de la simetría (en $\mathbb{A}_\mathbb{R}^2$) respecto a la recta $(1, 0) + \langle (-1, 2) \rangle$ con dirección $(0, 1)$.
	
	Fijamos la referencia habitual: $R = \{(0,0,0); e_1, e_2\}$. Para facilitarnos las cosas definimos la referencia auxiliar $R' = \{(1,0); e_1 = (-1,2), u_2(0, 1)\}$. Respecto de esta referencia:
	\begin{align*}
		\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right) =
		\left(\begin{array}{c}
		0 \\ 0
		\end{array}\right) + 
		\left(\begin{array}{cc}
		1 & 0 \\ 0 & -1
		\end{array}\right)
		\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right)
	\end{align*}
	Ahora necesitamos las coordenadas en términos de $R$. Para obtenerlas utilizamos el método expuesto anteriormente. Sea $R'' = \{(1,0); e_1, e_2\}$:
	\begin{align*}
	\left(\begin{array}{c}
	x'' \\ y''
	\end{array}\right) = 
	\left(\begin{array}{cc}
	1 & 0 \\ -4 & -1
	\end{array}\right)
	\left(\begin{array}{c}
	x'' \\ y''
	\end{array}\right)
	\end{align*}
	Y cambiando de nuevo a $R$ (tenemos que hallar $(a, b)$):
	\begin{align*}
	\left(\begin{array}{c}
	1 \\ 0
	\end{array}\right) &= 
	\left(\begin{array}{c}
	a \\ b
	\end{array}\right) + 
	\left(\begin{array}{cc}
	1 & 0 \\ -4 & -1
	\end{array}\right)
	\left(\begin{array}{c}
	1 \\ 0
	\end{array}\right) \implies
	\left(\begin{array}{c}
	a \\ b
	\end{array}\right) = 
	\left(\begin{array}{c}
	0 \\ 4
	\end{array}\right) \\
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) &= 
	\left(\begin{array}{c}
	0 \\ 4
	\end{array}\right) + 
	\left(\begin{array}{cc}
	1 & 0 \\ -4 & -1
	\end{array}\right)
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
	\end{align*}
\end{ej}

\paragraph{Homotecias}
\begin{dfn}
	Una afinidad $f: A \to A$ es una \textbf{homotecia} si $\tilde{f} = r Id_E$ con $r ≠ 0, 1$. En tal caso diremos que $f$ es una homotecia de razon $r$.
\end{dfn}

\begin{obs}
	\begin{itemize}
		\item Si $r= 0$ entonces $f$ es constante.
		\item Si $r = 1$ entonces $f$ es una traslación.
	\end{itemize}
\end{obs}

Sea $R = \{ P; u_1, \dots, u_n\}$ donde $B= \{u_1, \dots, u_n\}$ y sea $f: A \to A$ una homotecia de razón $r$.
\begin{align*}
	\left(\begin{array}{c}
	x_1 \\ \vdots \\ x_n
	\end{array}\right) \mapsto
	\left(\begin{array}{c}
	a_1 \\ \vdots \\ a_n
	\end{array}\right) + 
	\left(\begin{array}{ccc}
	r & \dots & 0 \\ \vdots & r & \vdots \\ 0 & \dots & r
	\end{array}\right)
	\left(\begin{array}{c}
	x_1 \\ \vdots \\ x_n
	\end{array}\right)
\end{align*}

\begin{pro}
	$f$ tiene un único punto fijo.
\end{pro}

\begin{proof}
	Podemos reescribir la ecuación de arriba para comprobar que nos queda un sistema escalonado con solución única para $r ≠ 1$:
	\begin{align*}
		\left(\begin{array}{cccc}
		r-1 & 0 & \dots & 0 \\
		0 & r-1 &  & 0 \\
		\vdots & & \ddots & \vdots \\
		0 & \dots & 0 & r - 1
		\end{array}\right)\left(\begin{array}{c}
		x_1 \\ \vdots \\ x_n
		\end{array}\right) = \left(
			\begin{array}{c}
			a_1 \\ \vdots \\ a_n
			\end{array}
		\right), \qquad x_i = \frac{-a_1}{r-1}
	\end{align*}
\end{proof}

Sea $Q$ el punto fijo de $f$ al que llamamos centro de la homotecia. Definimos $R' = \{Q; u_1, \dots, u_n\}$. Entonces nos queda una expresión en coordenadas sencilla de $f$:
\begin{align*}
\left(\begin{array}{c}
x_1' \\ \vdots \\ x_n'
\end{array}\right) \mapsto
\left(\begin{array}{ccc}
r & \dots & 0 \\ \vdots & r & \vdots \\ 0 & \dots & r
\end{array}\right)
\left(\begin{array}{c}
x_1' \\ \vdots \\ x_n'
\end{array}\right)
\end{align*}

% TODO:	poner un dibujito de una homotecia

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^2$. Sea $R = \{(0,0); e_1, e_2\}$ la referencia cartesiana estándar. Calcula la expresión en coordenadas de la homotecia de centro $(2,1)$ y razón $r = 2$.
	\begin{align*}
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) \mapsto
	\left(\begin{array}{c}
	a \\ b
	\end{array}\right) + 
	\left(\begin{array}{cc}
	2  & 0 \\ 0 & 2
	\end{array}\right)
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
	\end{align*}
	Ahora imponemos que $(2, 1)$ es un punto fijo:
	\begin{align*}
	\left(\begin{array}{c}
	2 \\ 1
	\end{array}\right) =
	\left(\begin{array}{c}
	a \\ b
	\end{array}\right) + 
	\left(\begin{array}{cc}
	2  & 0 \\ 0 & 2
	\end{array}\right)
	\left(\begin{array}{c}
	2 \\ 1
	\end{array}\right) \implies \left(\begin{array}{c}
	a \\ b
	\end{array}\right) = \left(\begin{array}{c}
	-2 \\ -1
	\end{array}\right)
	\end{align*}
	Luego la expresión en coordenadas de la homotecia es:
		\begin{align*}
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) \mapsto
	\left(\begin{array}{c}
	-2 \\ -1
	\end{array}\right) + 
	\left(\begin{array}{cc}
	2  & 0 \\ 0 & 2
	\end{array}\right)
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
	\end{align*}
	
\end{ej}

\subsection{Expresión en coordenadas baricéntricas de una afinidad}
Sea $f: A_1 \to A_2$ una afinidad. Sean $R_1 = \{P_0, P_1, \dots, P_n\},\ R_2 = \{Q_0, Q_1, \dots, Q_m\}$ referencias baricéntricas de $A_1$ y $A_2$, respectivamente. Si $X \in A_1,\ [X]_{R_1} = (x_0, x_1, \dots, x_n)$, queremos dar una expresión que relacione $[X]_{R_1}$ con $[f(X)]_{R_2} = (y_0, y_1, \dots, y_m)$.

Sabemos que $[X]_{R_1} = (x_0, x_1, \dots, x_n) \iff \forall P \in A_1, \overrightarrow{PX} = x_0 \overrightarrow{P P_0} + x_1 \overrightarrow{P P_1} + \dots  + x_n \overrightarrow{P P_n}$. Y análogamente, $[f(X)]_{R_2} = (y_0, y_1, \dots, y_m) \iff \forall Q \in A_2, \overrightarrow{QX} = y_0 \overrightarrow{Q Q_0} + y_1 \overrightarrow{Q Q_1} + \dots  + y_m \overrightarrow{Q Q_m}$. Ahora aplicamos $\tilde{f}$ a la expresión obtenida para $\overrightarrow{PQ}$: $\tilde{f}(\overrightarrow{PQ}) = x_0 \tilde{f}(\overrightarrow{P P_0}) + \dots + x_n\tilde{f}(\overrightarrow{P P_n}); \tilde{f}(\overrightarrow{PQ}) = x_0 \overrightarrow{f(P) f(P_0)} + \dots + x_n \overrightarrow{f(P) f(P_n)}$. Ahora $\overrightarrow{f(P)Q} + \overrightarrow{Qf(X)} = x_0 (\overrightarrow{f(P)Q} + \overrightarrow{Qf(P_0)}) + \dots + x_n (\overrightarrow{f(P) Q} + \overrightarrow{Qf(P)})$ para un $Q \in A_2$. Restando $\overrightarrow{f(P) Q}$ queda: $\overrightarrow{Q f(X)} = x_0 \overrightarrow{Q f(P_0)} + x_1 \overrightarrow{Qf(P_1)} + \dots + x_n\overrightarrow{Q f(P_n)}$. Pero podemos escribir cada $\overrightarrow{Qf(P_i)}$ como combinación lineal de $\overrightarrow{QQ_0}, \overrightarrow{QQ_1}, \dots \overrightarrow{Q Q_m}$. De hecho si $[P_0]_{R_2} = (\alpha_{00}, \alpha_{01}, \dots, \alpha_{0m})$, y en general $[P_i]_{R_2} = (\alpha_{i0}, \alpha_{i1}, \dots, \alpha_{im})$ para $i = 0, \dots, n$, tenemos que $\overrightarrow{Qf(X)} = x_0(\alpha_{00}\overrightarrow{Q Q_0} + \alpha_{01} \overrightarrow{Q Q_1} + \dots + \alpha_{0m} \overrightarrow{Q Q_m}) + \dots + x_n(\alpha_{n0}\overrightarrow{Q Q_0} + \alpha_{n1}\overrightarrow{Q Q_1} + \dots + \alpha_{nm} \overrightarrow{Q Q_m}) = (x_0\alpha_{00} + x_1\alpha_{10} + \dots + x_n\alpha_{n0})\overrightarrow{Q Q_m} + \dots + (x_0\alpha_{nm})\overrightarrow{Q Q_m}$. Y por la expresión que habíamos dado $\overrightarrow{Q X}$, lo anterior es igual a $y_0\overrightarrow{Q Q_0} + y_1 \overrightarrow{Q Q_1} + \dots + y_m \overrightarrow{Q Q_m}$. En forma matricial:
\begin{align*}
\left(\begin{array}{cccc}
\alpha_{00} & \alpha_{10} & \dots & \alpha_{n0} \\
\alpha_{01} & \alpha_{11} &  & \alpha_{n1} \\
\vdots & \vdots & & \vdots \\
\alpha_{0m} & \alpha_{1m} & \dots & \alpha_{nm}
\end{array}\right)
\left(
\begin{array}{c}
x_0 \\x_1 \\ \vdots \\ x_n
\end{array}\right) = \left(
\begin{array}{c}
y_0 \\y_1 \\ \vdots \\ y_n
\end{array}\right)
\end{align*}

\begin{ej}
	Sea $R = \{P_0 = (0,0); e_1 = (1,0), e_2 = (0,1)\}$ una referencia cartesiana en $\mathbb{A}_\mathbb{R}^2$. Sea $f:\mathbb{A}_\mathbb{R}^2 \to \mathbb{A}_\mathbb{R}^2$ la homotecia de razón 2 y centro $(2,1)$. Respecto de $R$ tenemos:
	\begin{align*}
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) \mapsto
	\left(\begin{array}{c}
	-2 \\ -1
	\end{array}\right) +
	\left(\begin{array}{cc}
	2 & 0 \\ 0 & 2
	\end{array}\right)
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
	\end{align*}
	
	Sea $R' = \{P_0 = (0,0), P_1 = (1,0), P_2 = (0,1)\}$ con $\overrightarrow{P_0 P_1} = (1,0),\ \overrightarrow{P_0 P_2} = (0,1)$. Queremos obtener la expresión en coordenadas respecto a $R'$.
\end{ej}

% --------------------------------------------------------

\chapter{Espacio afín euclídeo}
\begin{dfn}
	Sea $(A, \psi, E)$ un espacio afín. Una distancia es una función:
	\begin{align*}
		d:A \times A \to \mathbb{R}_{\geq 0}
	\end{align*}
	que cumple las siguientes propiedades:
	\begin{enumerate}
		\item $\forall P, Q \in A,\ d(P, Q) \geq 0$ y en particular $d(P, Q) = 0 \iff P = Q$
		\item $\forall P, Q \in A,\ d(P, Q) = d(Q, P)$
		\item Desigualdad triangular: $\forall P, Q, R \in A,\ d(P, Q) \leq d(P, R) + d(R, Q)$
	\end{enumerate}
	Un espacio que tiene una distancia definida se llama espacio métrico.
\end{dfn}

\begin{ej}Para $P, Q \in A$:
	\begin{align*} 
		d(P, Q) = \begin{cases}
		0 &\text{ si } P = Q \\
		1 &\text{ si } P ≠ Q
		\end{cases}
	\end{align*}
	Esta función de distancia cumple las tres propiedades pero distorsiona completamente la noción que tenemos de espacio.
\end{ej}

\begin{ej}
	Si $\lVert \cdot \rVert : E \to \mathbb{R}$ es una norma en $E$, entonces podemos definir una distancia en $A$ inducida por la norma:
	\begin{align*}
		d_{\lVert \cdot \rVert}(P, Q) = \lVert \overrightarrow{PQ} \rVert
	\end{align*}
	Esta distancia cumple las tres propiedades como consecuencia de las tres propiedades de una norma.
\end{ej}

\begin{dfn}
	Diremos que un espacio afín es euclídeo si la distancia definida en $A$ proviene de una norma euclídea en $E$, es decir una norma que proviene de un producto escalar.
\end{dfn}

Estas son las normas que nos interesan por lo que a partir de ahora supondremos que $(A, \psi, E)$ es un espacio afín euclídeo.

\begin{tm}[Pitágoras]
	Sean $P, Q, R \in A$. Si $\overrightarrow{PQ} \perp \overrightarrow{PR}$, entonces $d(Q, R)^2 = d(P,Q)^2 + d(P, R)^2$.\footnote{En caso de haber ortogonalidad, la desigualdad triangular se convierte en igualdad. Y solo en ese caso.}
\end{tm}

\begin{dfn}[Ortogonalidad en el caso afín]
	Sean $L = P + F, M = Q + G$ dos variedades lineales en $A$. Diremos que $L \perp M$, ($L$ y $M$ son ortogonales) $\iff F \perp G$.
\end{dfn}

\begin{dfn}[Distancia entre dos variedades lineales]
	Sean $L_1, L_2$ variedades lineales en $A$. Definimos
	\begin{align*}
		d(L_1, L_2) = \inf \{ d(P, Q): P \in L_1, Q \in L_2\}
	\end{align*}
\end{dfn}

\begin{ej}
	Sea $P \in A, Q + F = L$. Queremos encontrar $d(\{P\}, L)$.
	\begin{itemize}
		\item Si $P \in L \implies d(\{P\}, L) = 0$.
		\item ¿Qué ocurre si $P \not \in L$.
	\end{itemize}

Sea $f: A \to A$ la proyección ortogonal sobre $L$ con dirección $F^\perp$. Sea $Q = f(P)$. Queremos probar que $d(\{P\}, L) = \lVert \overrightarrow{PQ}\rVert$. Para ello basta ver que $\lVert \overrightarrow{PQ} \rVert$ es la norma más pequeña que podemos obtener tomando puntos en $L$. Sea $R \in L$. $\overrightarrow{RQ} + \overrightarrow{QP} = \overrightarrow{RP}$. Como $\overrightarrow{RQ} \perp \overrightarrow{QP}$ utilizamos el teroema de Pitágoras. $\lVert\overrightarrow{RP}\rVert^2 = \lVert\overrightarrow{RQ}\rVert^2 + \lVert\overrightarrow{QP}\rVert^2 \implies d(P, Q) = \lVert \overrightarrow{QP} \rVert \leq d(R, P),\ \forall R \in L$. $\lVert \overrightarrow{QP}\rVert$ da el ínfimo y como $Q \in L$ entonces tenemos el mínimo. 
\end{ej}

\section{Isometrías}
\begin{dfn}
	Sean $(A_1, \psi_1, E_1), (A_2, \psi_2, E_2)$ euclídeos\footnote{Valdría decir métricos en lugar de euclídeos, pero solo vamos a trabajar el caso euclídeo.}. Diremos que una afinidad $f:A_1 \to A_2$ es una isometría si $\forall P, Q \in A_1,\ d(P, Q) = d(f(P), f(Q))$.
\end{dfn}

\begin{obs}
	$f: A_1 \to A_2$ es isometría $\iff \tilde{f}: E_1 \to E_2$ es ortogonal.
\end{obs}
\begin{proof}
	$d(P, Q) = d(f(P), f(Q)); \lVert \overrightarrow{PQ} \rVert = \lVert \overrightarrow{f(P) f(Q)}\rVert = \lVert\tilde{f}(\overrightarrow{PQ})\rVert \implies \tilde{f}$ es ortogonal.
\end{proof}

\begin{dfn}[Referencia ortonormal]
	Sea $(A, \psi, E)$ un espacio afín euclídeo. Diremos que una referencia cartesiana de $A$, digamos $R = \{P; u_1, \dots, u_n\}$, es ortonormal si $\{u_1, \dots, u_n\}$ es una BON en $E$.
	
	Abreviamos referencia ortonormal por RON.
\end{dfn}

\subsection{Clasificación de isometrías en $\mathbb{A}_\mathbb{R}^2$}

\begin{tm}
	Sea $f \mathbb{A}_\mathbb{R}^2 \to \mathbb{A}_\mathbb{R}^2$ una isometría. Entonces en una RON positivamente orientada adecuada, $R = \{P; u_1, u_2\},\ B = \{u_1, u_2\}$, $M_B(f)$ es alguna de las siguientes:
	\begin{enumerate}
		\item $M_B(f) = \left(\begin{array}{cc}
		1 & 0 \\ 0 & 1
		\end{array}\right) \implies f$ es una traslación de vector $[u]_B = (a, b)$. $f$ tiene expresión en coordenadas $\left(\begin{array}{c}
		x \\ y
		\end{array}\right) \mapsto \left(\begin{array}{c}
		a \\ b
		\end{array}\right) + 
		\left(\begin{array}{cc}
		1 & 0 \\ 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right)$
		\item $M_B(f) = \left(\begin{array}{cc}
		\cos \alpha & -\sin \alpha \\ \sin \alpha & \cos \alpha
		\end{array}\right)$, para $\alpha \in (0, 2\pi)$. $f$ es una rotación trasladada. $f$ tiene expresión en coordenadas $\left(\begin{array}{c}
		x \\ y
		\end{array}\right) \mapsto \left(\begin{array}{c}
		c \\ d
		\end{array}\right) + 
		\left(\begin{array}{cc}
		\cos \alpha & - \sin \alpha \\ \sin \alpha & \cos \alpha
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right)$
		\item $M_B(f) = \left(\begin{array}{cc}
		1 & 0 \\ 0 & -1
		\end{array}\right)$. $f$ es una simetría respecto a una recta con posible deslizamiento. La expresión en coordenadas de $f$ respecto a $R$ es $\left(\begin{array}{c}
		x \\ y
		\end{array}\right) \mapsto \left(\begin{array}{c}
		c \\ d
		\end{array}\right) + \left(\begin{array}{cc}
		1 & 0 \\ 0 & -1
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right)$
	\end{enumerate}
\end{tm}

En el \textbf{caso 2}, queremos ver, como siempre, si $f$ tiene puntos fijos, pues si tiene un punto fijo y centramos nuestra referencia en dicho punto, $f$ se comporta como una aplicación ortogonal. Para hallar los puntos fijos tenemos que resolver el sistema:
\begin{align*}
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) = \left(\begin{array}{c}
	c \\ d
	\end{array}\right) + 
	\left(\begin{array}{cc}
	\cos \alpha & - \sin \alpha \\ \sin \alpha & \cos \alpha
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) \to 
		\left(\begin{array}{cc}
	\cos \alpha - 1 & - \sin \alpha \\ \sin \alpha & \cos \alpha - 1
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) = \left(\begin{array}{c}
	-c \\ -d
	\end{array}\right)
\end{align*}

Miramos el determinante: $(\cos \alpha - 1)^2 = \sin^2 \alpha = \cos^2 + 1 - 2 \cos\alpha = \sin^2 \alpha = 2 - 2\cos \alpha = 2(1 - \cos \alpha) ≠0$ si $\alpha ≠ 0$. Si $\alpha \in (0, 2 \pi)$ siempre hay un único punto fijo, $C = a,b)$ donde $x = a, y = b$ es la única solución del sistema.Fijamos ahora la referencia $R' = \{C = (a,b); u_1, u_2\}$. La expresión en coordenadas de $f$ respecto de la nueva referencia $R'$ es:
\begin{align*}
		\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right) \mapsto \left(\begin{array}{c}
	0 \\ 0
	\end{array}\right) + 
	\left(\begin{array}{cc}
	\cos \alpha & - \sin \alpha \\ \sin \alpha & \cos \alpha
	\end{array}\right)\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right)
\end{align*}
pues hemos forzado que $[C]_{R'} = (0,0)$ de manera que ahora $f$ se comporta como una aplicación lineal. En el caso particular $\alpha = \pi,\ f$ es una simetría central con centro en el punto $C$.

Para el \textbf{caso 3} repetimos el proceso e intentamos encontrar puntos fijos
\begin{align*}
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right) = \left(\begin{array}{c}
	c \\ d
	\end{array}\right) + 
	\left(\begin{array}{cc}
	1 & 0 \\ 0 & -1
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) \to 
	\left(\begin{array}{cc}
		0 & 0 \\ 0 & -2
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) = \left(\begin{array}{c}
	-c \\ -d
	\end{array}\right)
\end{align*}
Observamos que solo hay puntos fijos si $c = 0$. Podemos reescribirlo:
\begin{align*}
\left(\begin{array}{c}
x \\ y
\end{array}\right) \mapsto \left(\begin{array}{c}
c \\ 0
\end{array}\right) + \left(\begin{array}{c}
0 \\ d
\end{array}\right) + 
\left(\begin{array}{cc}
1 & 0 \\ 0 & -1
\end{array}\right)\left(\begin{array}{c}
x \\ y
\end{array}\right)
\end{align*}
para darnos cuenta de que podemos mirar $f$ como la composición de una traslación y una simetría axial. Estudiamos la simetría axial y obtenemos que la recta de puntos (recta de simetría) fijos son aquellos que tienen coordenada $y = d/2$. Es decir que si $Q \in \{y = d/2\}$ en $R' = \{Q; u_1, u_2\}$ la expresión de la simetría es
\begin{align*}
	\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right) \mapsto 
	\left(\begin{array}{cc}
	1 & 0 \\ 0 & -1
	\end{array}\right)\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right)
\end{align*}
En conjunto, la afinidad $f$ lo que hace es trasladar el vector resultado de aplicar la simetría por el vector $(c, 0)$, que es paralelo a la recta de simetría. Es decir que $f$ puede ser una simetría con respecto a una recta, o una simetría con deslizamiento paralelo a la recta de simetría.

\begin{ej}
	En $\mathbb{A}_\mathbb{R}^2$, con la distancia euclídea usual. Fijamos $R = \{0 = (0,0); e_1, e_2\}$ que es una RON. Clasificar
	\begin{align*}
		f : \mathbb{A}_\mathbb{R}^2& \to \mathbb{A}_\mathbb{R}^2 \\
		\left(\begin{array}{c}
		x \\ y
		\end{array}\right) &\mapsto
		\left(\begin{array}{c}
		1 - \sqrt{2} \\ \sqrt{2} - 1
		\end{array}\right) + \left(\begin{array}{cc}
		-1/3 & \frac{2\sqrt{2}}{3} \\
		\frac{2\sqrt{2}}{3} & 1/3
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right)
	\end{align*}
	
	Comprobamos que $f$ es una isometría ya que $M_B(\tilde{f}) M_B(\tilde{f}) = I_2$ y $B$ (la base formada por los vectores de $R$) es una BON.
	
	Vemos que $\det \tilde{f} = -1$ por lo que puede tratarse de una simetría ortogonal respecto de una recta con un posible deslizamiento. Tenemos que ver si existe dicho deslizamiento o no. Sabemos que en una cierta RON $R' = \{P; u_1, u_2\}$ la isometría tiene la forma dada anteriormente (ver los casos de la clasificación). En dicha base, separamos el deslizamiento de la simetría y en la parte de la simetría podemos obtener la dirección de la recta de simetría haciendo $\ker (\tilde{f} - I_2) = \langle (\frac{1}{\sqrt{3}}, \sqrt{\frac{2}{3}})\rangle$ (no hacemos ningún cambio de base, tomamos $M_B(\tilde{f})$ para hacer el núcleo).
	
	Quedan por calcular la recta de simetría (solo tenemos la dirección) y el vector deslizamiento. Tenemos varias opciones:
	\begin{enumerate}
		\item Podemos expresar el vector deslizamiento como combinación lineal de el vector director de la recta y uno ortogonal a este (es decir, respecto de la referencia $R'$):$\left(\begin{array}{c}
		1 - \sqrt{2} \\ \sqrt{2} - 1
		\end{array}\right) = a\left(\begin{array}{c}
		\frac{1}{\sqrt{3}} \\ \sqrt{\frac{2}{3}}
		\end{array}\right) + b\left(\begin{array}{c}
		\sqrt{\frac{2}{3}} \\ \frac{-1}{\sqrt{3}}
		\end{array}\right)$. Obtenemos $a = \sqrt{3}, b = -\sqrt{3}$. Entonces podemos escribir:
		\begin{align*}
			\left(\begin{array}{c}
			x \\ y
			\end{array}\right) &\mapsto
			\sqrt{3}\left(\begin{array}{c}
			\frac{1}{\sqrt{3}} \\ \sqrt{\frac{2}{3}}
			\end{array}\right) + \left[-\sqrt{3}\left(\begin{array}{c}
			\sqrt{\frac{2}{3}} \\ \frac{-1}{\sqrt{3}}
			\end{array}\right)+\left(\begin{array}{cc}
			-1/3 & \frac{2\sqrt{2}}{3} \\
			\frac{2\sqrt{2}}{3} & 1/3
			\end{array}\right)\left(\begin{array}{c}
			x \\ y
			\end{array}\right)\right]
		\end{align*}
		donde vemos que el vector deslizamiento es $\sqrt{3}\left(
		\frac{1}{\sqrt{3}}, \sqrt{\frac{2}{3}}\right)$ y la recta de simetría son los puntos fijos de la parte que queda dentro de los corchetes, es decir $\frac{2\sqrt{2}}{3}x - \frac{2}{3}y = -1$.
		
		\item Tenemos la dirección de la recta de simetría y podemos ver dibujándolo que dado $Q$ un punto cualquiera, el punto medio entre $f(Q)$ y $Q$ está en la recta de simetría a pesar de cualquier deslizamiento paralelo a la recta de simetría. Si cogemos un $Q$ en la propia recta de simetría, su imágen $f(Q)$ también estará en la recta. Con un punto medio podemos obtener la recta de simetría pues tenemos la dirección. Por ejemplo veamos el punto medio entre $(0,0)$ y $f(0,0) = (1 - \sqrt{2}, \sqrt{2} - 1)$. $M_{Qf(Q)} = (\frac{1 - \sqrt{2}}{2}, \frac{\sqrt{2} - 1}{2})$ que pertenece a la recta de simetría, luego la recta tiene ecuación $(\frac{1 - \sqrt{2}}{2}, \frac{\sqrt{2} - 1}{2}) + \langle u_r\rangle $.
	\end{enumerate}
	
	\hrule
	Esto concuerda con lo que hubiéramos obtenido de estudiar los puntos fijos de $f$ sin pensar en la referencia $R'$.
	\begin{align*}
	\left(\begin{array}{c}
	1 - \sqrt{2} \\ \sqrt{2} - 1
	\end{array}\right) + \left(\begin{array}{cc}
	-1/3 & \frac{2\sqrt{2}}{3} \\
	\frac{2\sqrt{2}}{3} & 1/3
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) = 
	\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
	\end{align*}
	Estudiando el sistema de arriba llegamos a la conclusión de que no tiene solución, es decir que no hay puntos fijos. Esto pasa porque el vector del deslizamiento no es ortogonal al vector director de la recta de simetría, cosa que podemos comprobar fácilmente (el vector deslizamiento es $(1 - \sqrt{2}, \sqrt{2} - 1)$).
\end{ej}

\subsection{Clasificación de isometrías en $\mathbb{A}_\mathbb{R}^3$}

\begin{tm}
	Sea $f:\mathbb{A}_\mathbb{R}^3 \to \mathbb{A}_\mathbb{R}^3$ una isometría. Si fijamos una RON adecuada $R = \{P; u_1, u_2, u_3\}$, donde $B = \{u_1, u_2, u_3\}$ es una BON, la expresión en coordenadas de $f$ es de la siguiente forma
	\begin{align*}
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right) \mapsto
	\left(\begin{array}{c}
	a_1 \\ a_2 \\ a_3
	\end{array}\right) + 
	M_B(\tilde{f})\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right)
	\end{align*}
	donde $M_B(\tilde{f})$ es  alguna de las siguientes:
	\begin{enumerate}
		\item Si $\det \tilde{f} = 1$
		\begin{enumerate}
			\item $M_B(\tilde{f}) = I_3$. Se trata de una traslación $f = T_u, u \in \mathbb{R}^3$.
			\item $M_B(\tilde{f}) = \left(\begin{array}{ccc}
			1 & 0 & 0 \\
			0 & -1 & 0 \\
			0 & 0 & -1
			\end{array}\right)$ (caso especial $\alpha = \pi$).
			\item $M_B(\tilde{f}) = \left(\begin{array}{ccc}
			1 & 0 & 0 \\
			0 & \cos \alpha & - \sin \alpha \\
			0 & \sin \alpha & \cos \alpha
			\end{array}\right),\, \alpha \in (0, 2\pi)$. Si $a_1 = 0$ entonces siempre hay una recta de puntos fijos, que es el eje de giro, de la forma $(0, b, c) + \langle (1, 0, 0) \rangle$ para $b$ y $c$ constantes. Estamos ante una \textbf{rotación} de $\alpha$ al rededor del eje dado.\footnote{Lo que está ocurriendo es que hacemos una rotación al rededor del la recta de vector director $u_1$ (el de la base) y luego componemos con un deslizamiento de vector $(0, b, c)$ que es perpendicular al eje de giro.} Si $a_1 ≠ 0$ entonces no hay puntos fijos pero reescribimos la expresión de $f$ como composición de una traslación paralela al eje $u_1$ con una rotación deslizada como antes, lo que se llama \textbf{movimiento helicoidal}:
			\begin{align*}
				\left(\begin{array}{c}
				x \\ y \\ z
				\end{array}\right) \mapsto
				\left(\begin{array}{c}
				a_1 \\ 0 \\ 0
				\end{array}\right) + 
				\left(\begin{array}{c}
				0 \\ a_2 \\ a_3
				\end{array}\right) + 
				\left(\begin{array}{ccc}
				1 & 0 & 0 \\
				0 & -1 & 0 \\
				0 & 0 & -1
				\end{array}\right)\left(\begin{array}{c}
				x \\ y \\ z
				\end{array}\right)
			\end{align*}
		\end{enumerate}
		\item Si $\det \tilde{f} = -1$
		\begin{enumerate}
			\item $M_B(\tilde{f}) = \left(\begin{array}{ccc}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & -1
			\end{array}\right)$ Solo hay puntos fijos si $a_1 = a_2 = 0$. En ese caso los puntos fijos son el plano de simetría $z = a_3 / 2$ por lo que estamos ante una \textbf{simetría respecto a un plano}.\footnote{En realidad lo que pasa es que hacemos simetría con respecto al plano $\langle u_1, u_2 \rangle$ y luego trasladamos por el vector $a_3$ que es perpendicular a dicho plano. El resultado de esto es una simetría pero respecto al plano $\langle u_1, u_2 \rangle$ desplazado por $a_3 / 2$ en la dirección perpendicular al plano.} Si $a_1 ≠ 0 \lor a_2 ≠ 0$ entonces no hay puntos fijos, reescribimos como en otras ocasiones:
			\begin{align*}
				\left(\begin{array}{c}
				x \\ y \\ z
				\end{array}\right) \mapsto
				\left(\begin{array}{c}
				a_1 \\ a_2 \\ 0
				\end{array}\right) + 
				\left(\begin{array}{c}
				0 \\ 0 \\ a_3
				\end{array}\right) + 
				\left(\begin{array}{ccc}
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & -1
				\end{array}\right)\left(\begin{array}{c}
				x \\ y \\ z
				\end{array}\right)
			\end{align*}
			Así, $f$ es el resultado de una \textbf{simetría respecto a un plano con un deslizamiento} con un vector paralelo al plano.
			\item $M_B(\tilde{f}) = -I_3$ (caso especial $\alpha = \pi$).
			\item $M_B(\tilde{f}) = \left(\begin{array}{ccc}
			-1 & 0 & 0 \\
			0 & \cos \alpha & - \sin \alpha \\
			0 & \sin \alpha & \cos \alpha
			\end{array}\right),\ \alpha \in (0, 2\pi)$. Si $\alpha ≠ 0$ hay siempre un único punto fijo $Q$. Fijando $R' = \{Q; u_1, u_2, u_3\}$ que tiene la misma BON que $R$, la expresión de $f$ en coordenadas es
			\begin{align*}
				\left(\begin{array}{c}
				x' \\ y' \\ z'
				\end{array}\right) \mapsto
				\left(\begin{array}{c}
				0 \\ 0 \\ 0
				\end{array}\right)  + 
				\left(\begin{array}{ccc}
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & -1
				\end{array}\right)\left(\begin{array}{c}
				x' \\ y' \\ z'
				\end{array}\right)
			\end{align*}
			Tenemos una \textbf{rotación compuesta con una siemtría respecto a un plano}, donde el eje de rotación es $Q + \langle u_3 \rangle$ (lo podemos obtener del núcleo de la parte lineal) y el plano de simetría es $Q + \langle u_1, u_2 \rangle$.
		\end{enumerate}
	\end{enumerate}
\end{tm}

% 2017/11/28
\begin{ej}
	En $\mathbb{A}_\mathbb{R}^3$ fijamos $R = \{(0,0,0); e_1, e_2, e_3\}$, la RON estándar. Se pide estudiar el efecto geométrico de
	\begin{align*}
		f: A& \to A \\
		\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right) &\mapsto
		\left(\begin{array}{c}
		2 \\ 0 \\ 0
		\end{array}\right)  + 
		\left(\begin{array}{ccc}
		0 & 1 & 0 \\
		1 & 0 & 0 \\
		0 & 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right)
	\end{align*}
	
	Comprobamos que $f$ es una isotmetría porque $\tilde{f}$ lleva una BON en otra BON. Observamos que $\def \tilde{f} = -1$. Como $\tilde{f}$ es simétrica sabemos que $\tilde{f}$ es autoadjunta, por tanto no estamos en el caso de una rotación. Tenemos que decidir si estamos ante una simetría respecto a un plano, con o sin deslizamiento. Para ver si hay deslizamiento podemos ver si hay puntos fijos:
	\begin{align*}
		\left(\begin{array}{c}
		2 \\ 0 \\ 0
		\end{array}\right)  =
		\left(\begin{array}{ccc}
		-1 & 1 & 0 \\
		1 & -1 & 0 \\
		0 & 0 & 0
		\end{array}\right)\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right)
	\end{align*}
	Vemos que no hay puntos fijos (las dos primeras ecuaciones son incompatibles), por lo que se trata de una simetría respecto a un plano con deslizamiento.
	
	Calculamos la dirección del plano de simetría: $\ker (\tilde{f} - I_3) = \{ x = y\} = \langle (1,1,0), (0,0,1) \rangle$.\footnote{Efectivamente, la dirección del plano no nos queda ortogonal a la dirección de la traslación dada por $(2,0,0)$, por lo que no nos pueden quedar puntos fijos. Solo en caso de que la dirección de la traslación que componemos con la simetría sea perpendicular al plano nos quedará otra simetría respecto aun plano ($x = a_3 /2$).} Una vez tenemos la dirección solo nos hace falta un punto del plano. Observamos que el punto medio entre cualquier punto y su imagen por $f$ tiene que estar en el plano de simetría. Como habitualmente cogemos el punto $A = (0,0,0), f(A) = (2,0,0), M_{A f(A)} = (1,0,0) \in \pi$ plano de simetría.\footnote{Este plano de simetría del que hablamos es el plano con dirección obtenida del núclro de $\tilde{f}$ pero ya trasladado por la parte ortogonal de la traslación que componemos con la simetría (la parte de $(2,0,0)$ que es ortogonal a $\langle (1,1,0), (0,0,1)\rangle$).} Por tanto el plano de simetría es $\pi \equiv (1,0,0) + \langle (1,1,0), (0,0,1)\rangle$.
	
	Ahora hallamos el vector deslizamiento que podemos obtener de $\overrightarrow{P f(P)}$ donde $P \in \pi$. Elegimos $P = (1,0,0)$ por comodidad y obtenemos $\overrightarrow{P f(P)} = (1,1,0)$ que efectivamente es paralelo al plano.
	
	
	\hrule
	
	Otra manera de hacer el problema es, una vez sabido que estamos ante una simetría con deslizamiento, ver que parte del vector de la traslación que componemos con la simetría (el vector $(2,0,0)$) es paralela al plano de simetría y que parte es ortogonal (con plano de simetría nos referimos al de $\tilde{f}$ que pasa por el origen):
	\begin{align*}
		(2,0,0) = u + w,\ u \in \langle (1,1,0), (0,0,1)\rangle,\ w \in \langle (1,1,0), (0,0,1)\rangle^\perp \\(2,0,0) = \alpha (1,1,0) + \beta(0,0,1) + \gamma (1, -1, 0) \implies\\
		\alpha = \gamma = 1,\ \beta = 0 \implies \text{ podemos escribir f como} \\
		\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right) \mapsto
		\left(\begin{array}{c}
		1 \\ 1 \\ 0
		\end{array}\right) +
		\left(\begin{array}{c}
		1 \\ -1 \\ 0
		\end{array}\right) + 
		\left(\begin{array}{ccc}
		0 & 1 & 0 \\
		1 & 0 & 0 \\
		0 & 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right)
	\end{align*}
	Vemos que los puntos fijos de la segunda parte nos dan el plano de simetría (haciendo la cuneta sale $\{-x + y = -1\}$). Y el vector de la primera parte es, en efecto, el vector deslizamiento (paralelo a la dirección del plano de simetría) que habiamos obtenido con el otro método.
\end{ej}

\begin{ej}
		En $\mathbb{A}_\mathbb{R}^3$ fijamos $R = \{(0,0,0); e_1, e_2, e_3\}$, la RON estándar. Se pide estudiar el efecto geométrico de
	\begin{align*}
	f: A& \to A \\
	\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right) &\mapsto
	\left(\begin{array}{c}
	4 \\ -2 \\ -4
	\end{array}\right)  + 
	\frac{1}{15}
	\left(\begin{array}{ccc}
	-10 & 5 & -10 \\
	-11 & -2 & 10 \\
	2 & 14 & 5
	\end{array}\right)\left(\begin{array}{c}
	x \\ y \\ z
	\end{array}\right)
	\end{align*}
	
	Como siempre tenemos que comprobar que $f$ es una isometría (nos lo creemeos porque ya lo hemos hecho muchas veces). Calculamos $\def \tilde{f} = 1$. Además como la matríz de $\tilde{f}$ no es autoadjunta sabemos que no estamos en el caso de una traslación. Tenemos que decidir si estamos en el caso de una rotación respecto a un eje o en el caso de un movimiento helicoidal. Podríamos ver si hay puntos fijos pero preferimos utilizar otro método porque corremos mucho riesgo de confundirnos
	
	Calculamos, sabiendo que tiene que salir una recta, la dirección del eje de rotación $\ker (\tilde{f} - I_3) = \land (1, -3, -4)\rangle$. Mirando el vector de la traslación que está compuesta con la rotación vemos que no hay puntos fijos, puesto que para que los hubiera el vector de la traslación, $(4, -2, -4)$, tendría que ser ortogonal a la dirección del eje. Como no lo es, no hay puntos fijos. Reescribimos el vector traslación como suma de un vector paralelo al eje y un vector ortogonal al eje:
	\begin{align*}
		(4, -2, -4) = \alpha(1, -3, -4) + \beta w,\ w \perp \langle (1, -3, -4) \rangle \implies \alpha = 1, w = (3, 1, 0) \\
		\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right) \mapsto
		\left(\begin{array}{c}
		1 \\ -3 \\ -4
		\end{array}\right) +
		\left(\begin{array}{c}
		3 \\ 1 \\ 0
		\end{array}\right) + 
		\frac{1}{15}
		\left(\begin{array}{ccc}
		-10 & 5 & -10 \\
		-11 & -2 & 10 \\
		2 & 14 & 5
		\end{array}\right)\left(\begin{array}{c}
		x \\ y \\ z
		\end{array}\right)
	\end{align*}
	Como antes ya hemos escrito $f$ como una composición de una rotación respecto de un eje trasladado por un vector ortogonal al del eje de $\tilde{f}$ (la segunda parte) y un deslizamiento en una dirección paralela al eje (la primera parte). De los puntos de la segunda parte obtenemos el eje de rotación $\{ -5x + y - 2z = -0, 32y - 24z = -8 \}$. Y también tenemos el vector deslizamiento $(1, -3, -4)$.
	
	Ahora tenemos que calcular el ángulo de rotación. Como en el caso lineal sabemos por que la traza es un invariante respecto de los cambios de base que $1 + 2\cos \alpha = \text{Traza } M_B(\tilde{f}) = \frac{-7}{15} \to \cos \alpha = \frac{-11}{15}, \sin \alpha = \pm \sqrt{1 - \frac{121}{225}}$. Por último averiguamos el signo del seno. Para poder hacerlo, orientamos el eje según el vector $u_1 = (1, -3, -4)$. Para saber el signo del seno, construimos la matriz que tiene los vectores por columnas ($u_1, u_1^\perp, \tilde{f}(u_1^\perp)$. Cogemos $u_1^\perp = (1, 3, 0)$:
	\begin{align*}
		\det \left(\begin{array}{ccc}
		1 & 3 & 2 \\
		-3 & 1 & -6 \\
		-4 & 0 & 5
		\end{array}\right) > 0 \implies \sin \alpha > 0
	\end{align*}
\end{ej}

% 2017.11.30
\chapter{Cónicas}
\section{Formas cuadráticas}
\begin{dfn}[Forma cuadrática]
	Sea $V$ un espacio vectorial sobre $\mathbb{R}$. Entonces una forma cuadrática $Q$ en $V$ es una función
	\begin{align*}
		Q : V \to \mathbb{R}
	\end{align*}
	que tiene dos propiedades:
	\begin{enumerate}
		\item $\forall \lambda \in R, \forall u \in V,\ Q(\lambda u) = \lambda^2 Q(u)$
		\item La función
		\begin{align*}
			\varphi_Q : V \times V &\to \mathbb{R}\\
			 \varphi_Q (u, v) &\mapsto \frac{1}{2} (Q(u + v) - Q(u) - Q(v))
		\end{align*}
		es una forma bilineal simétrica.
	\end{enumerate}
\end{dfn}

\begin{ej}
	Sea $V = \mathbb{R}^2$. Definimos
	\begin{align*}
		Q : \mathbb{R}^2 &\to \mathbb{R}^2 \\
		(x, y) &\mapsto x^2 + y^2
	\end{align*}
	Veamos que $Q$ es una forma cuadrática:
	\begin{enumerate}
		\item Es claro que $Q(\lambda(x,y)) = \lambda^2Q(x,y)$
		\item Tenemos que comprobar que
		\begin{align*}
			\varphi_Q : \mathbb{R}^2 \times \mathbb{R}^2 &\to \mathbb{R} \\
			((x, y), (x', y')) &\mapsto \frac{1}{2}((x + x', y + y') - Q(x, y) - Q(x', y'))
		\end{align*}
		es una forma bilineal simétrica. $\frac{1}{2}((x + x', y + y') - Q(x, y) - Q(x', y')) = 1/2((x + x')^2 + (y + y')^2 - (x^2 + y^2) - (x'^2 + y'^2) = 1/2(2xx' + 2yy')) = xx' + yy'$ que es una forma bilineal simétrica porque es el producto escalar estándar.
	\end{enumerate}
\end{ej}

¿Cómo describimos todas las formas cuadráticas?

\begin{obs}
	Sea $\varphi V \times V \to \mathbb{R}$ una forma bilineal. Usando $\varphi$ podemos definir una forma cuadrática $Q$ en $V$ del siguiente modo:
	\begin{align*}
		Q : V &\to \mathbb{R} \\
		u &\mapsto Q(u) = \varphi (u, u)
	\end{align*}
	Veamos que $Q$ es una forma cuadrática:
	\begin{enumerate}
		\item Por definición, $Q(\lambda u) = \varphi (\lambda u, \lambda u)$ y como $\varphi$ es bilineal, $\varphi (\lambda u, \lambda u) = \lambda^2 \varphi (u, u)$.
		\item $\varphi_Q : V \times V \to \mathbb{R}$ tiene que ser bilineal simétrica. Por definición $\varphi_Q (u, v) = 1/2(\varphi (u + v, u + v) - \varphi(u,u) - \varphi(v,v))$. Ojo: no hemos pedido que $\varphi$ sea simétrica, por lo que al expandir la expresión anterior nos queda: $\varphi_Q(u,v) = 1/2(\varphi(u, v) + \varphi(v, u))$. Tenemos que comprobar que eso es bilineal y simétrico. Es bilineal porque $\varphi$ es bilineal. Y también es simétrica porque si intercambiamos $u, v$ sigue quedando lo mismo.
	\end{enumerate}

	Luego podemos obtener formas cuadráticas a partir de una forma bilineal.
\end{obs}

Supongamos ahora que fijamos una base $B = \{u_1, \dots, u_n\}$ de $V$. Dada la forma bilineal $\varphi$ anterior se tiene que $\forall u,v, \varphi(u,v) = [u]_B^T M_B(\varphi) [v]_B$. Esto quiere decir que $Q(u) = \varphi (u, u) = [u]_B^T M_B(\varphi) [u]_B$ es una combinación lineal de monomios de grado 2. Veamos un ejemplo.

\begin{ej}
	Sean $V = \mathbb{R}^2,\ B = \{e_1, e_2\}$ la base habitual y sea $\varphi_1 : V \times V \to \mathbb{R}$ donde 
	\begin{align*}
		M_B(\varphi_1) = \left(\begin{array}{cc}
		1 & 2 \\ 0 & 1
		\end{array}\right)
	\end{align*}
	Entonces
	\begin{align*}
		Q(x, y) = (x, y) \left(\begin{array}{cc}
			1 & 2 \\ 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right) = x^2 + 2xy + y^2
	\end{align*}
	Ahora, por ser $Q$ una forma cuadrática, podemos definir una forma bilineal asociada $\varphi_Q$. ¿Es esa forma bilineal la misma $\varphi_1$ de la que hemos partido? No, porque $\varphi_Q$ tiene que ser simétrica y $\varphi_1$ no lo es. ¿A qué forma bilineal da lugar $Q$?
	\begin{align*}
		\varphi_Q : \mathbb{R}^2 \times \mathbb{R}^2 & \to \mathbb{R} \\
		(u, v) &\mapsto \frac{1}{2} (\varphi_1(u, v) + \varphi_1(v, u))
	\end{align*}
	por la ecuación a la que hemos llegado antes.
	
	Observemos que $\varphi_Q$ vuelve a inducir la forma cuadrática $Q$ otra vez.
\end{ej}

\paragraph{Resumen} Las formas bilineales en $V$ inducen formas cuadráticas en $V \to \mathbb{R}$. Por otro lado, dada una forma cuadrática, siempre podemos producir una forma bilineal simétrica en $V$. Si la forma bilineal de la que partimos es simétrica, el viaje es de ida y vuelta y vuelvo a la forma bilineal de partida. Si no es simétrica, llego a una forma cuadrática que tiene asociada una forma bilineal que sí es simétrica. Es decir, que podemos descartar todas las formas bilineales no simétricas porque tenemos otras formas bilineales que inducen las mismas formas cuadráticas que esas y además son simétricas. \textbf{Existe una correspondencia biyectiva entre formas bilineales simétricas y formas cuadráticas.}

\begin{dfn}[Expresión en cordenadas de una forma cuadrática]
	Sea $B = \{v_1, \dots, v_n\}$ una base de $V$, $Q: V \to \mathbb{R}$ una forma cuadrática. Sea $\varphi_Q$ la única forma bilineal simétrica inducida por $Q$. Sea $u \in V$ con lo cual tenemos $[u]_B = (x_1, \dots, x_n)$,
	\begin{align*}
		Q(u) = \varphi_Q(u, u) = [u]_B^T M_B(\varphi_Q)[u]_B = (x_1, \dots, x_n)M_B(\varphi_Q)\left(\begin{array}{c}
		x_1 \\ \vdots \\ x_n
		\end{array}\right) = \sum_{1 \leq i, j \leq n} b_{ij} x_i x_j
	\end{align*}
\end{dfn}

Nos proponemos encontrar cambios de coordenadas que permitan escribir una forma cuadrática sin términos cruzados (de modo que solo aparezcan sumas de monomios en una variable).

\begin{tm}
	Sea $V$ un e. v. euclídeo de dimensión $n$ sobre $\mathbb{R}$ y sea $Q: V \to \mathbb{B}$ una forma cuadrática. Entonces existe una base ortonormal de $B$ respecto de la cual la expresión en coordenadas de $Q$ no tiene términos cruzados.
\end{tm}

\begin{proof}
	Fijamos una BON, $B = \{v_1, \dots, v_n\}$ de $V$. Sea $\varphi$ la forma bilineal simétrica que determina $Q$. Si expresamos $Q$ en coordenadas, $Q(u) = [u]_B^T M_B(\varphi)[u]_B$, nos damos cuenta de que la matriz ha de ser diagonal. Entonces
	\begin{enumerate}
		\item (Vamos a hacer la trampa de pensar que $M_B(\varphi)$ es la matriz de una aplicación lineal. Sea $f: V \to V$ la aplicación lineal cuya matriz respectoa $B$ es $M_B(\varphi)$, es decir $M_B(f) = M_B(\varphi)$. Como $B$ es una BON entonces $f$ es autoadjunta y por tanto existe una BON, $B'$ tal que $M_B(f) = \left(\begin{array}{ccc}
		\lambda_1 & 0 & 0 \\
		0 & \ddots & 0 \\
		0 & 0 & \lambda_n
		\end{array}\right)$. Entonces si queremos calcular $M_B(f) = M_B(\varphi)$:
		\begin{align*}
			M_B(f) = M_{BB'}M_{B'}(f)M_{B'B} = M_{B'B}^TM_{B'}(f)M_{B'B}
		\end{align*}
		qué coño pone aquí?.
	\end{enumerate}
\end{proof}


%TODO: copiar apuntes del 2017/12/4

% resumen de lo de ayer
\begin{dfn}[Forma normal de una forma cuadrática]
	Dada una forma $Q: V \to \mathbb{R}$, diremos que $Q$ está en forma canónica respectoa una base $B$ de $V$ si su expresión en coordenadas respecto a esa base no tiene productos cruzados.
	
	Es decir, la matriz de la forma bilineal simétrica asociada a $Q$ respecto a $B$ es diagonal, o lo que es lo mismo, para $u \in V$ con $[u]_B = (x_1, \dots, x_n)$ se tiene que $Q(u) = a_1x_1^2 + \dots + a_nx_n^2 = \sum_{i = 1}^n a_i x_i^2$.
\end{dfn}

\begin{dfn}[Rango de una forma cuadrática]
	Dada $Q$ en forma canónica, diremos que el rango de $Q$ es el número de términos no nulos en la expresión anterior.
\end{dfn}

\begin{dfn}[Índices de inercia de una forma cuadrática]
	Sea $Q$ una forma cuadrática en forma canónica:
	\begin{itemize}
		\item Diremos que el índice de inercia positivo es el número de coeficientes $a_i$ positivos.
		\item Diremos que el índice de inercia negativo es el número de coeficientes $a_i$ negativos.
	\end{itemize}
\end{dfn}

\begin{tm}[de Steiner]
	Sea $V$ un e.v. sobre $\mathbb{R}$ con dimesión $n$ y sea $Q: V \to \mathbb{R}$ una forma cuadrática. Supongamos que $Q$ se encuantra en forma canónica respecto a una base $B=\{v_1, \dots, v_n\}$, es decir,
	\begin{align}
		\label{steiner1}
		Q(u) = [u]_B^T M_B(\varphi)[u]_B = M_1 x_1^2 + \dots + M_l x_l^2,\qquad l \leq n
	\end{align}
	donde $\varphi$ es la forma bilineal simétrica asociada a $Q$. Sea $B' = \{w_1, \dots, w_n\}$ otra base respecto de la cual $Q$ también tiene forma canónica, es decir:
	\begin{align}
		\label{steiner2}
		Q(u) = [u]_{B'}^T M_{B'}(\varphi) [u]_{B'} = \lambda_1 x_1^2 + \dots + \lambda_k x_k^2,\qquad k \leq n
	\end{align}
	Entonces $l = k$, es decir el rango de $Q$ es el mismo en ambas bases, y los índices de inercia positivo y negativo de \eqref{steiner1} y de \eqref{steiner2} coinciden.
\end{tm}

\begin{dfn}[Forma normal de una forma cuadrática]
	Dada $Q$ en forma canónica, se dice que $Q$ tiene forma normal respecto a $B$ si los coeficientes no nulos en la expresión de la forma canónica tienen valor absoluto igual a 1.
\end{dfn}

Para obtener una forma normal basta con hacer el cambio de cariable $z_i = \sqrt{|a_i|}x_i,\ i = 1, \dots, n$ y reordenando si hace falta queda:
\begin{align*}
	Q(u) = z_1^2 + \dots + z_l^2 - z_{l + 1}^2 + \dots + (-z)_s^2,\ s \leq n
\end{align*}
% end: resumen de lo de ayer

\subsection{Método de Gauss para encontrar una base respecto a la cual una forma cuadrática dada tenga forma canónica}

Primero veremos algunos ejemplos y luego sacaremos la fórmula general.

\begin{ej}
	Fijamos $B =  \{u_1, u_2, u_3\}$. Sea $Q(u) = x_1^2 + 3x_3^2 - 4x_1x_2 - 4x_1x_3 + 4x_2x_3$ donde $[u]_B = (x_1, x_2, x_3)$. Priemo escribimos $M_B(Q)$ que no es otra cosa que la matriz de la forma bilineal simétrica que induce esta forma canónica:
	\begin{align*}
		M_B(Q) = \left(\begin{array}{ccc}
		1 & -2 & -2 \\
		-2 & 0 & 2 \\
		-2 & 2 & 3
		\end{array}\right)
	\end{align*}
	
	Si utilizaramos el método de los autovalores, $\det(M_B(Q) - \lambda I) = -\lambda(\lambda^2 - 4\lambda - 9)$. Tenemos suerte de que ya nos sale casi factorizado y vemos claramente que un autovalor es $0$, por lo tanto tiene rango 2. Podemos aproximar los autovalores e incluso averiguar los índices de inercia ya que nos queda un autovalor positivo y otro negativo, es decir que ambos índices de inercia son 1.
	
	No obstante, el método anterior no ayuda para obtener la base que hace que $Q$ esté en forma canónica por lo que utilizaremos el método de Gauss.
	
	Tenemos $Q(u) = x_1^2 + 3x_3^2 - 4x_1x_2 - 4x_1x_3 + 4x_2x_3$. Nos fijamos en el primer término que aparece al cuadrado. Completamos cuadrados para obtener $Q(u) = (x_1 - 2(x_1 + x_3))^2 - (2(x_2 + x_3))^2 + 4x_2x_3 + 3x_3^2 = (x_1 - 2x_2 - 2x_3)^2 - 4x_2^2 - 4x_3^2 - 8x_2x_3 + 3x_2$. Ahora repetimos la misma operación pero fijándonos en el siguiente término que aparece al cuadrado: $(x_1 - 2x_2 - 2x_3)^2 - 4x_2^2 - 4x_3^2 - 8x_2x_3 + 3x_2 = (x_1 - 2x_2 - 2x_3)^2 - (2x_2 + x_3)^2 + 0$. Ahora hacemos el cambio de variable $x_1' = x_1 - 2x_2 - 2x_3,\ x_2' = 2x_2 + x_3,\ x_3' = x_3$, de manera que el cambio de base nos lo da:
	\begin{align*}
		M_{B'B}[x] = [x'] \qquad
		\left(\begin{array}{ccc}
		1 & -2 & -2 \\
		0 & -2 & 1 \\
		0 & 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x_1 \\ x_2 \\ x_3
		\end{array}\right) = \left(\begin{array}{c}
		x_1' \\ x_2' \\ x_3'
		\end{array}\right)
	\end{align*}
	Pero queremos obtener la base $B'$ para lo que calculamos la inversa que tiene como vectores columna los vectores de la nueva base, $M_{B'B}^{-1} = M_{BB'}$:
	\begin{align*}
		M_{BB'} = \left(\begin{array}{ccc}
		1 & 1 & 1 \\
		0 & 1/2 & -1/2 \\
		0 & 0 & 1
		\end{array}\right)
	\end{align*}
	Por tanto, $B' = \{(1, 0, 0), (1, 1/2, 0), (1, -1/2, 1)\}$. Y efectivamente, haciendo el cambio adecuado para obtener $M_{B'}(Q)$\footnote{Recordamos que se trata de un cambio de base entre formas bilineales.}:
	\begin{align*}
		\left(\begin{array}{ccc}
		1 & 0 & 0 \\
		1 & 1/2 & 0 \\
		1 & -1/2 & 1
		\end{array}\right)\left(\begin{array}{ccc}
		1 & -2 & -2 \\
		-2 & 0 & 2 \\
		-2 & 2 & 3
		\end{array}\right)\left(\begin{array}{ccc}
		1 & 1 & 1 \\
		0 & 1/2 & -1/2 \\
		0 & 0 & 1
		\end{array}\right) = \left(\begin{array}{ccc}
		1 & 0 & 0 \\
		0 & -1 & 0 \\
		0 & 0 & 0
		\end{array}\right) = M_{B'}(Q)
	\end{align*}
	que efectivamente tiene rango 2 y los índices de inercia que esperábamos.
\end{ej}

\begin{tm}[Fórmula general para el cambio de variable en el caso en el que aparecen términos al cuadrado en la expresión de $Q$]
	\begin{align}
		Q(x_1, \dots, x_n) = ax_1^2 + x_1(\sum_{i = 2}^{n} a_i x_i) + Q_1(x_2, \dots, x_n) = a(x_1 + \frac{1}{2a}\sum_{i = 2}^{n} a_ix_i)^2 + Q_2(x_2, \dots, x_n),\, a≠0
	\end{align}
\end{tm}

Pero, ¿qué ocurre si no parece ningún término al cuadrado, sino que solamente aparecen productos cruzados?:

\begin{ej}
	Consideramos ahora $Q(u) = 3x_1x_2 + 2x_1x_3 + x_2x_3$ con
	\begin{align*}
		M_B(Q) = \left(\begin{array}{ccc}
		0 & 3/2 & 1 \\
		3/2 & 9 & 1/2 \\
		1 & 1/2 & 0
		\end{array}\right)
	\end{align*}
	Utilizaremos la siguiente fórmula que permite expresar un producto como una diferencia de cuadrados:
	\begin{align}
		\label{prod2sqdiff}
		a\cdot b = \frac{(a + b)^2 - (a - b)^2}{4}
	\end{align}
	Así podemos escribir $Q(u) = 3(x_1 + \frac{1}{3}x_3)(x_2 + \frac{2}{3}x_3) - \frac{2}{3}x_3^2$. Ahora utilizando $\eqref{prod2sqdiff}$ (identificamos cada paréntesis con $a$ y $b$, respectivamente) nos queda:
	\begin{align*}
		Q(u) = \frac{3}{4}(x_1 + \frac{1}{3}x_3 + x_2 + \frac{2}{3}x_3)^2 - \frac{3}{4}(x_1 + \frac{1}{3} - (x_2 + \frac{2}{3}x_3))^2 - \frac{2}{3}x_3^2 = \\
		\frac{3}{4}(x_1 + x_2 + x_3)^2 - \frac{3}{4}(x_1 - x_2 - \frac{1}{3}x_3)^2 - \frac{2}{x}x_3^2
	\end{align*}
	Como antes hacemos el cambio de variable $x_1' = x_1 + x_2 + x_3,\ x_2' = x_1 - x_2 - \frac{1}{3}x_2'$ y obtenemos la expresión para el cambio de base que nos da la nueva base:
	\begin{align*}
		\left(\begin{array}{ccc}
		1 & 1 & 1 \\ 1 & -1 & -1/3 \\ 0 & 0 & 1
		\end{array}\right)\left(\begin{array}{c}
			x_1 \\ x_2 \\ x_3
		\end{array}\right) = 
		\left(\begin{array}{c}
		x_1' \\ x_2' \\ x_3'
		\end{array}\right) \\
		M_{BB'} = \left(\begin{array}{ccc}
		1/2 & 1/2 & 1/3 \\ 1/2 & -1/2 & -2/3 \\ 0 & 0 & 1
		\end{array}\right)
	\end{align*}
	Así tenemos $B' = \{(1/2, 1/2, 0), (1/2, -1/2, -2/3), (0, 0, 1)\}$, $Q(u) = \frac{3}{4}(x_1')^2 - \frac{3}{4}(x_2')^2 - \frac{2}{3}(x_3')^2$. El rango nos sale 3 y los índices de inercia positivo y negativo nos salen 1 y 2, respectivamente. El cambio de base es análogo al caso anterior.
\end{ej}


\begin{tm}[Fórmula general cuando no hay términos al cuadrado]
	\begin{multline}
		Q(u) = ax_1x_2 + x_1\sum_{i = 3}^{n}\alpha_ix_i + x_2\sum_{i = 3}^{n}\beta_ix_i + Q_1(x_3, \dots, x_n) = \\
		a(x_1 + \frac{1}{a}\sum_{i = 3}^{n} \beta_i x_i)(x_2 + \frac{1}{a}\sum_{i = 3}^{n}\alpha_i x_i) + Q_2(x_3, \dots, x_n)
	\end{multline}
\end{tm}

La idea es iterar eligiendo esta fórmula o la anterior en función de si tenemos cuadrados o no.


\section{Ecuaciones y elementos geométricos de las cónicas}
Fijados el plano afín euclídeo y una RON $R = \{O = (0,0); e_1, e_2\}$, veamos las ecuaciones canónicas de las cónicas en las que los ejes de las cónicas corresponden a ejes paralelos a los del sistema de referencia.

\begin{dfn}[Circunferencia]
	\begin{align*}
		(x - c_1)^2 + (y - c_2)^2 = r^2
	\end{align*}
	donde el centro es $(c_1, c_2)$ y el radio es $r$.
\end{dfn}

\begin{dfn}[Elipse]
	El conjunto de puntos del plano cuya suma de distancias a dos puntos es constante:
	\begin{align*}
		\frac{(x - c_1)^2}{a^2} + \frac{(y - c_2)^2}{b^2} = 1
	\end{align*}
	donde $F_1 (-c, 0), F_2 = (c, 0)$ son los focos, el centro $(c_1, c_2)$ es el punto medio entre los focos $C = \frac{1}{2}F_1 + \frac{1}{2}F_2$, el eje principal es la recta que pasa por dos focos, el eje secundario es la recta ortogonal al eje principal que pasa por el centro. Se tiene que $a^2 = b^2 + c^2$.
\end{dfn}

\begin{dfn}[Hipérbola]
	\begin{align*}
		\frac{(x - c_1)^2}{a^2} - \frac{(y - c_2)^2}{b^2} = 1
	\end{align*}
	Centro $(c_1, c_2) = \frac{1}{2}F_1 + \frac{1}{2}F_2$ y ejes iguales que en la elipse. Sin embargo, en este caso se tiene que $c^2 = a^2 + b^2$. Además tenemos las asíntotas. Despejando la $y$ obtenemos una ecuación de la que podemos tomar un límite:
	\begin{align*}
		y = \pm \sqrt{\frac{b^2}{a^2}x^2 - b^2} \\
		\lim\limits_{x \to \infty} \frac{+\sqrt{\frac{b^2}{a^2}x^2 - b^2}}{\lambda x} = 1 \implies \lambda = \frac{b}{a} \\
		\lim\limits_{x \to -\infty} \frac{+\sqrt{\frac{b^2}{a^2}x^2 - b^2}}{\lambda x} = 1 \implies \lambda = \frac{-b}{a}
	\end{align*}
	Con lo que tenemos las pendientes de las dos asíntotas que pasan por el centro.
\end{dfn}

\begin{dfn}[Parábola]
	Conjunto de puntos del plano que equidistan de un punto (foco) y una recta.
	\begin{align*}
		(y - v_2)^2 = \pm 2p(x - v_1)^2 \\
		(x - v_1)^2 = \pm 2p(y - v_2)^2 
	\end{align*}
	donde el foco $F = (p/2, 0)$, la directriz es la recta $x = -\frac{p}{2}$, el eje principal es ortogonal a la principal y pasa por $F$. El eje secundario es ortogonal al principal y pasa por el vértice. El vértice $V = (v_1, v_2)$ es el punto medio entre el foco y el punto donde la directriz interseca al eje principal.
\end{dfn}

\begin{obs}
	En las cónicas nunca aparece un término cruzado $xy$ al desarrollar la ecuación canónica.
\end{obs}

¿Qué ocurre cuando tenemos una ecuación que no se corresponde con ninguna de las anteriores?

\subsection{Determinación del tipo de una cónica}
Fijados el plano afín euclídeo y la RON estándar $R = \{(0,0); e_1, e_2\}$. Dada una ecuación del tipo
\begin{align*}
	Ax^2 + By^2 + Cxy + Dx + Ey + F = 0, \ A, B, C, D, E, F \in \mathbb{R}
\end{align*}
queremos hacer un cambio de variable que haga que desaparezca el término cruzado sin deformar la figura, es decir buscando una referencia en la que la ecuación esté en forma canónica.

Podemos escribir la parte principal (parte cuadrática $Ax^2 + By^2 + Cxy$) de la ecuación en forma matricial:
\begin{align*}
	(x, y)\left(\begin{array}{cc}
	A & C/2 \\ C/2 &B
	\end{array}\right)\left(\begin{array}{c}
	x \\ y
	\end{array}\right)
\end{align*}
y haciendo un cambio de RON que permite escribir la forma cuadrática en forma canónica: $R' = \{(0,0); v_1, v_2\}$
\begin{align*}
	(x', y')\left(\begin{array}{cc}
	\lambda_1 & 0 \\ 0 & \lambda_2
	\end{array}\right)\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right) \\
	M_{BB'}\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right) = \left(\begin{array}{c}
	x \\ y
	\end{array}\right)
\end{align*}
El cambio de base nos lo da una matriz ortonormal en $\mathbb{R}^2$. Ahora bien, el cambio de variable no es único por tanto le vamos a pedir a $M_{BB'}$ que tenga $\det M_{BB'} = 1$. De esta manera nos aseguramos de que el cambio que hemos hecho corresponde únicamente a una rotación. Ahora ya tenemos una ecuación que no tiene términos cruzados ya que la estamos viendo desde un sistema de referencia que tiene ejes paralelos a los de la figura:
\begin{align*}
	\lambda_1 x'^2 + \lambda_2 y'^2 + (D, E)\left(\begin{array}{c}
	x \\ y
	\end{array}\right) + F = 0
\end{align*}
Ahora terminamos de hacer el cambio para que toda la ecuación esté en la misma RON utilizando $M_{BB'}[x]_R = [x]_{R'}$:
\begin{align*}
	\lambda_1 x'^2 + \lambda_2 y'^2 + (D, E)M_{BB'}\left(\begin{array}{c}
	x' \\ y'
	\end{array}\right) + F = 0
\end{align*}
De esta manera podemos obtener una ecuación sin términos cruzados que se corresponde a la ecuación canónica de una cónica trasladada:
\begin{align}
	\label{conicas2}
	\lambda_1 x'^2 + \lambda_2 y'^2 + b_1x' + b_2y' + F = 0
\end{align}
Ahora solo nos queda completar cuadrados (hacer una traslación) sin alterar distancias para llegar a una forma canónica. Distinguimos los siguientes casos
\begin{enumerate}
	\item $\lambda_1 \cdot \lambda_2 > 0$ que es el mismo que el signo de $\det\left(\begin{array}{cc}
	A & C/2 \\ C/2 & B
	\end{array}\right)$. Podemos suponer de hecho que $\lambda_1, \lambda_2 > 0$ pues si nos salieran los dos negativos solo habría que cambiar todo lo anterior para que queden positivos jajajajjajaja xd lol. En este caso nos encontramos ante una \textbf{cónica de tipo elíptico}. Reescribimos la ecuación \eqref{conicas2} para poder hacer el cambio apropiado:
	\begin{align*}
		\lambda_1(x' + \frac{b_1}{2\lambda_1})^2 + \lambda_2(y' + \frac{b_2}{2\lambda_2})^2 - \frac{b_1^2}{4\lambda_1} - \frac{b_2^2}{4\lambda_2} + F = 0 \\
		\left(\begin{array}{c}
		x'' \\ y''
		\end{array}\right) = \left(\begin{array}{c}
		\frac{b_1}{2\lambda_1} \\ \frac{b_2}{2\lambda_2}
		\end{array}\right) + \left(\begin{array}{cc}
		1 & 0 \\ 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right) \\
		\frac{(x'')^2}{1/\lambda_1} + \frac{(y'')^2}{1/\lambda_2} = d
	\end{align*}
	Así hemos hecho la traslación correspondiente que nos ha dado la RON $R'' = \{C= (c_1, c_2); v_1, v_2\}$. Ahora distinguimos diferentes casos para $d$:
	\begin{enumerate}
		\item Si $d > 0$ estamos ante una elipse (o una circunferencia si $\lambda_1 = \lambda_2$),
		\item si $d = 0$ estamos ante un punto
		\item si $d < 0$ entonces estamos ante el conjunto vacío de puntos.
	\end{enumerate}

	\item $\lambda_1 \cdot \lambda_2 < 0$. Como anteriormente, sin pérdida de generalidad podemos suponer que $\lambda_1 > 0, \lambda_2 < 0$ (si fuera al revés multiplicaríamos toda la ecuación por $-1$). En esta caso nos encontramos ante una \textbf{cónica de tipo hiperbólico}. Completanto cuadrados en la ecuación \eqref{conicas2} nos queda una ecuación que ya está en la referencia $R'' = \{C = (c_1, c_2); v_1, v_2\}$ y es de la forma:
	\begin{align*}
		\lambda_1(x'')^2 + \lambda_2(y'')^2 = d
	\end{align*}
	\begin{enumerate}
		\item Si $d ≠ 0$ nos encontramos ante una hipérbola,
		\item si $d = 0$ estamos ante una hipérbola degenerada, es decir dos rectas que se cortan en el centro.
	\end{enumerate}

	\item $\lambda_1 \cdot \lambda_2 = 0$. Podemos suponer que solo uno de los dos es 0 pues si los dos fueran 0, no aparecería parte cuadrática al principio. Suponemos, como anteriormente que $\lambda_1 = 0,\ \lambda_2 ≠ 0$. Estamos ante una \textbf{cónica de tipo parabólico}. En esta ocasión la ecuación de la que partimos \eqref{conicas2} no tiene término en $x^2$ por lo que nos queda de la siguiente forma
	\begin{align*}
		\lambda_2(y')^2 + b_1x' + b_2y' + F = 0
	\end{align*}
	Como antes, completamos cuadrados de una forma un poco chunga: supongamos $b_1 ≠ 0$:
	\begin{align*}
		\lambda_2(y' + \frac{b_2}{2\lambda_2}) + b_1(x' + \frac{F}{b_1} - \frac{b_2^2}{4\lambda_2b_1}) = 0
	\end{align*}
	donde
	\begin{align*}
		y'' = y' + \frac{b_2}{2\lambda_2} \qquad x'' = (x' + \frac{F}{b_1} - \frac{b_2^2}{4\lambda_2b_1}
	\end{align*}
	de manera que podemos escribir la ecuación en función de $R''$:
	\begin{align*}
		\left(\begin{array}{c}
		x'' \\ y''
		\end{array}\right) = \left(\begin{array}{c}
		\frac{F}{b_1} - \frac{b_2^2}{4\lambda_2b_1} \\ \frac{b_2}{2\lambda_2}
		\end{array}\right) + \left(\begin{array}{cc}
		1 & 0 \\ 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right) \\
		(y'')^2 = \frac{-b_1}{\lambda_2} x''
	\end{align*}
	supongamos ahora que $b_1 = 0$:
	\begin{align*}
		\lambda_2 (y')^2 + b_2y' + F = 0
	\end{align*}
	En esta ecuación podemos ver que estamos ante una ecuación de segundo grado por lo que podemos discutir ante qué situación estamos viendo las soluciones que puede tomar:
	\begin{enumerate}
		\item Si no existen soluciones para $y'$, entonces tampoco va a haber soluciones para la $x'$ luego estamos ante el vacío.
		\item Si hay una solución, estaremos ante una recta horizontal doble.
		\item Si hay dos soluciones, estaremos ante dos rectas horizontales.
	\end{enumerate}
	Vamos a verlo completando cuadrados:
	\begin{align*}
		\lambda_2(y' + \frac{b_2}{2\lambda_2})^2 + F - \frac{b_2^2}{4\lambda_2} = 0
	\end{align*}
	donde
	\begin{align*}
		x'' = x' \qquad y'' = y' + \frac{b_2}{2\lambda_2}
	\end{align*}
	Obtenemos pues la ecuación en función de la RON $R''$ que tiene forma canónica:
	\begin{align*}
		(y'')^2 = \frac{-c}{\lambda_2}
	\end{align*}
	\begin{enumerate}
		\item Si $c= 0$ tenemos una solución doble $y'' = 0$ (la recta horizontal).
		\item Si $c < 0$ entonces tenemos dos soluciones $y'' = \pm \sqrt{\frac{-c}{\lambda_2}}$ que nos dan dos rectas horizontales (y por tanto paralelas).
		\item Si $c > 0$ entonces no hay soluciones luego estamos ante el conjunto vacío.
	\end{enumerate}
\end{enumerate}

\begin{obs}
	Si en lugar de utilizar este método, utilizamos el método de Gauss para encontrar un cambio ortogonal (que sí modifica las distancias), es posible que no obtengamos una cónica con las mismas dimensiones que la que nos han dado. Pero seguro que obtendremos una cónica del mismo tipo que la dada pues el tipo depende de los índices de inercia que no varían. \textbf{Un cambio de variable (tipo Gauss) no altera el tipo de cónica que es.}
\end{obs}

\begin{obs}
	Tampoco es posible que si tenemos por ejemplo, una parábola no degenerada, al hacer un cambio con el método de Gauss obtengamos dos rectas paralelas (una parábola degenerada). Esto es porque un cambio de base es en realidad un isomorfismo y no puede ocurrir que un isomorfismo lleve una curva en dos rectas pues sabemos que un isomorfismo respeta la estructura de espacio afín y lleva rectas en rectas. \textbf{Un cambio de variable (tipo Gauss) no lleva una cónica no degenerada en una degenerada ni viceversa.}
\end{obs}

\begin{ej}
	Dada la ecuación $3x^2 - 10xy + 3y^2 + 14x - 2y + 3 = $ respecto de la RON $R = \{(0,0); e_1, e_2\}$ decidir de qué tipo de cónica se trata y dar sus elementos geométricos.
	
	Sea $B = \{e_1, e_2\}$ y veamos $M_B(Q)$, la matriz para la parte principal o cuadrática:
	\begin{align*}
		M_B(Q) = \left(\begin{array}{cc}
		3 & -5 \\ -5 & 3
		\end{array}\right)
	\end{align*}
	Obtenemos sus autovalores haciendo $\det (M_B(Q) - \lambda I_2) = 0$ y nos quedan $\lambda_1 = 8, \lambda_2 = -2$ por lo que se trata de una cónica de tipo hiperbólico. Para obtener la BON respecto de la cual la parte cuadrática tiene forma canónica hacemos $\ker (M_B(Q) - 8I_2) = \langle u_1 = (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})\rangle$ y $\ker (M_B(Q) + 2I_2) = \langle u_2 = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})\rangle$ con lo que obtenemos $B' = \{u_1, u_2\}$ y por extensión $R' = \{(0,0); u_1, u_2\}$ y $M_{BB'}$ que nos da el cambio. Nótese que $\det M_{BB'} = 1$ por lo que no necesitamos hacer ningún cambio (estamos ante un cambio de base que nos da una rotación que es lo que buscábamos). Ahora en esta nueva base
	\begin{align*}
		8(x')^2 - 2(y')^2 + (14, -2)\left(\begin{array}{cc}
		\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
		-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
		\end{array}\right)\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right) + 3 = 0 \\
		8(x')^2 - 2(y')^2 + 8\sqrt{2}x' + 6\sqrt{2}y' + 3 = 0
	\end{align*} 
	completamos cuadrados y llegamos al siguiente cambio de base que nos da la traslación
	\begin{align*}
		8(x' + \frac{\sqrt{2}}{2})^2 - 2(y' - \frac{3\sqrt{2}}{2})^2 + 8 = 0 \\
		2(y'')^2 - 8(x'')^2 = 8 \qquad \to \qquad
		\frac{(y'')^2}{4} - \frac{(x'')^2}{1} = 1 \\
		\left(\begin{array}{c}
		x'' \\ y''
		\end{array}\right) = \left(\begin{array}{c}
		\frac{\sqrt{2}}{2} \\ -\frac{3\sqrt{2}}{2}
		\end{array}\right) + \left(\begin{array}{cc}
		1 & 0 \\ 0 & 1
		\end{array}\right)\left(\begin{array}{c}
		x' \\ y'
		\end{array}\right) = \left(\begin{array}{c}
		\frac{\sqrt{2}}{2} \\ -\frac{3\sqrt{2}}{2}
		\end{array}\right) + \left(\begin{array}{cc}
		\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
		-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
		\end{array}\right)\left(\begin{array}{c}
		x \\ y
		\end{array}\right) \\
			\left(\begin{array}{c}
		x \\ y
		\end{array}\right) = \left(\begin{array}{c}
		1 \\ 2
		\end{array}\right) + \left(\begin{array}{cc}
		\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
		-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
		\end{array}\right)\left(\begin{array}{c}
		x'' \\ y''
		\end{array}\right) 
	\end{align*}
	En $R''$, vemos que estamos ante el caso de un hipérbola en posición vertical con lo que podemos calcular los cortes con el eje $y$ para obtener la distancia $a$. Los cortes nos dan $(0, 2)$ y $(0, -2)$ con lo que $2a = 4, a = 2$. Sabiendo $a$ ya sabemos $b = 1$ (por qué?) y con la relación en la hipérbola $c^2 = a^2 + b^2$ obtenemos la semidistancia focal $c = \sqrt{5}$ y los focos $[F_1]_{R''} = (0, \sqrt{5}), [F_2]_{R''} = (0, -\sqrt{5})$. Por otro lado, el eje principal, el que contiene a los focos, es el eje $x'' = 0$ y el eje secundario, perpendicular al principal por el centro, que en $R''$ es $(0,0)$ es la recta $y''=0$. Para la asíntotas, lo más seguro es hacer los límites. Despejamos $x''$ (aunque perfectamente podríamos despejar $y''$) y nos queda:
	\begin{align*}
		x'' = \pm\sqrt{\left(\frac{y'' - 2}{2}\right)\left(\frac{y'' + 2}{2}\right)} \\
		\lim_{y'' \to \infty} \frac{\sqrt{\left(\frac{y'' - 2}{2}\right)\left(\frac{y'' + 2}{2}\right)}}{\lambda y''} = 1 \implies \lambda = \frac{1}{2} \to 2x''  - y'' = 0 \\
		\lim_{y'' \to -\infty} \frac{\sqrt{\left(\frac{y'' - 2}{2}\right)\left(\frac{y'' + 2}{2}\right)}}{\lambda y''} = 1 \implies \lambda = -\frac{1}{2} \to 2x''  + y'' = 0 \\
	\end{align*}
	
	Ya tenemos todos los elementos geométricos respecto de la referencia $R''$ pero ahora necesitamos hacer el cambio de cada uno de ellos para obtenerlos respecto de la referencia $R$. Para ello utilizaremos el cambio de $R''$ a $R$ que hemos obtenido antes. Nos quedan:
	\begin{itemize}
		\item centro $[C]_R = (1, 2)$,
		\item focos $[F_1]_R = (\frac{2 + \sqrt{10}}{2}, \frac{4 + \sqrt{10}}{2}),\ [F_2]_R =  (\frac{2 - \sqrt{10}}{2}, \frac{4 - \sqrt{10}}{2})$,
		\item ejes principal $\frac{\sqrt{2}}{2} + \frac{1}{\sqrt{2}}x - \frac{1}{\sqrt{2}}y = 0$ y secundario $\frac{-3\sqrt{2}}{2} + \frac{1}{\sqrt{2}}x + \frac{1}{\sqrt{2}}y = 0$,\footnote{Otra manera de encontrar los ejes es hacer la recta que pasa por los focos o utilizar los vectores directores $u_1$ y $u_2$ que nos daban el cambio de $R$ a $R'$, de manera que nos quedaría ejes principal y secundario $(1,2) + \langle u_2 \rangle,\, (1, 2) + \langle u_1 \rangle$, respectivamente.}
		\item y asíntotas que podemos obtener con un punto (el centro) y haciendo un cambio de base de los vectores directores de las asíntotas que nos han salido en $R''$\footnote{Se trata solo de un cambio utilizando la matriz de cambio de base $M_{BB''}$, sin la traslación que hacemos cuando se trata de puntos}: $A_1 \equiv (1, 2) + \langle (3, 1) \rangle,\ A_2 \equiv (1, 2) + \langle (1, 3) \rangle$.
	\end{itemize}
	Para terminar hacemos un dibujo aproximado.
\end{ej}

\section{Otras cuestiones sobre las formas cuadráticas}

\begin{dfn}
	Sea $Q: V \to \mathbb{R}$ una forma cuadrática. Diremos que:
	\begin{enumerate}
		\item $Q$ es definida positiva $\iff \forall u \in V,\ Q(u) \geq 0$ y $Q(u) = 0 \iff u = \vec{0}$\footnote{Esto enlaza con los productos escalares: si $\varphi$ es la forma bilineal asociada a $Q$, $Q$ es definida positiva si y solo sí $\varphi$ define un producto escalar}
		\item $Q$ es semidefinida positiva si $\forall u \in V,\ Q(u) \geq 0$.
		\item $Q$ es definida negativa si $\forall u \in V,\ Q(u) \leq 0$ y $Q(u) = 0 \iff u = \vec{0}$.
		\item $Q$ es semidefinida negativa si $\forall u \in V,\ Q(u) \leq 0$.
		\item $Q$ es indefinida si no está en ninguno de los casos anteriores.
	\end{enumerate}
\end{dfn}

\begin{ej}
	Veamos ejemplos de algunas formas de los tipos anteriores. Fijamos $B = \{e_1, e_2\}$ para $V = \mathbb{R}^2$:
	\begin{enumerate}
		\item Si $M_B(Q) = \left(\begin{array}{cc}
		\lambda & 0 \\ 0 & \mu
		\end{array}\right),\ \lambda, \mu > 0$ entonces $Q$ es definida positiva.
		\item Si $M_B(Q) = \left(\begin{array}{cc}
		\lambda & 0 \\ 0 & 0
		\end{array}\right),\ \lambda > 0$ entonces $Q$ es semidefinida positiva.
		\item Si $M_B(Q) = \left(\begin{array}{cc}
		\lambda & 0 \\ 0 & \mu
		\end{array}\right),\ \lambda, \mu < 0$ entonces $Q$ es definida negativa.
		\item Si $M_B(Q) = \left(\begin{array}{cc}
		\lambda & 0 \\ 0 & 0
		\end{array}\right),\ \lambda < 0$ entonces $Q$ es semidefinida negativa.
	\end{enumerate}
\end{ej}

\begin{dfn}
	Se dice que una forma cuadrática $Q$ es degenerada si $\exists u \in V,\ u ≠ \vec{0}: Q(u) = 0$.
\end{dfn}

\begin{tm}[Criterio de Sylvester para formas cuadráticas definidas positivas]
	Sea $V$ un espacio vectorial, $B = \{v_1, \dots, v_n\}$ una base tal que $\dim V = n$. Definimos $M_B(Q)_r$ como la submatriz de $M_B(Q)$ formada por las $r$ primeras filas y las $r$ primeras columnas de $M_B(Q)$ con $r = 1, \dots, n$.
	
	$Q$ es definida positiva $\iff$ Rango $Q = n = $ índice de inercia positivo $\iff \det M_B(Q)_r > 0, \forall r = 1, \dots, n$.
\end{tm}

\begin{proof}
	Ya hemos visto la primera equivalencia, lo que queremos ver es la segunda.
	
	Definimos $B_r = \{v_1, \dots, v_r\}$ como la base formada por los $r$ primeros vectores de $B$ que genera el subespacio $F_r = \langle r_1, \dots, v_r \rangle, r = 1, \dots, n$. Sea $\varphi : V\times V \to \mathbb{R}$ la forma bilineal simétrica asociada a $Q$ de la que obtenemos $M_B(Q)$ de la siguiente manera:
	\begin{align*}
	M_B(Q) = \left(\begin{array}{cccc}
	\varphi(v_1, v_1) & \varphi(v_1, v_2) & \dots & \varphi(v_1, v_n) \\
	\varphi(v_2, v_1) & \varphi(v_2, v_2) & \dots & \varphi(v_2, v_n) \\
	\vdots & \vdots & \ddots & \vdots \\
	\varphi(v_n, v_1) & \varphi(v_n, v_2) & \dots & \varphi(v_n, v_n) \\
	\end{array}\right)
	\end{align*}
	Por como está construida la matriz $M_B(Q)$ tenemos que $M_B(Q)_r = M_{B_r}(Q\vert_{F_r})$ donde $Q\vert_{F_r} : F_r \to \mathbb{R}$. Partiendo de esto probaremos los dos sentidos de la definición:
	
	$(\implies)$ Supongamos que $Q$ es definida positiva, tenemos que ver si $\forall r = 1, \dots, n,\ \det M_B(Q)_r > 0$. Para cada $r = 1, \dots, n$ sabemos que $Q\vert_{F_r}$ también es definida positiva, es decir que tiene rango e índice de inercia positivo $r$. De hecho, $\exists$ una BON de $F_r$, $B_r'$ respecto de la cual
	\begin{align*}
		M_{B_r'}(Q\vert_{F_r}) = \left(\begin{array}{ccc}
		\lambda_1 & 0 & 0 \\
		0 & \ddots & 0 \\
		0 & 0 & \lambda_r
		\end{array}\right),\quad \lambda_i > 0, i = 1, \dots, r
	\end{align*}
	
	Esta matriz viene dada por $M_{B_r'}(Q\vert_{F_r}) = P_r^TM_{B_r}(Q\vert_{F_r})P_r$. Y como el determinante no se ve afectado por un cambio de base sabemos que el signo de $\det M_{B_r}(Q\vert_{F_r})$ es igual al signo de $\lambda_1 \cdot \dots \cdot \lambda_r > 0$ por hipótesis.
	
	$(\impliedby)$. Supongamos que $\det M_B(Q)_r > 0,\ r = 1,\dots, n$ tenemos que ver que $Q$ es definida positiva. Como $\det M_B(Q)_1 > 0$, sabemos que $\varphi(v_1, v_1) > 0$. Ahora vamos a usar que $\det M_B(Q)_2 > 0$ y poco a poco iremos construyendo una base en la que $Q$ esté en forma canónica y que tenga todos los coeficientes positivos y por lo tanto sea evidente que es definida positiva. Sea $F_2 = \langle v_1, v_2 \rangle = \langle w_1, w_2 \rangle$ (donde $B_2 = \{w_1, w_2\}$ es nuestra nueva base). Cogemos $w_1 = v_1, w_2 = v_2 - \lambda v_1$ de modo que $\varphi(w_2, w_1) = 0$. Desarrollamos: $\varphi(w_2, w_1) = \varphi(v_2 - \lambda v_1, v_1) = \varphi(v_2, v_1) - \lambda\varphi(v_1, v_1) = 0$, donde $\varphi(v_1, v_1) ≠ 0$, luego la ecuación tiene solución $\lambda = \frac{\varphi(v_2, v_1)}{\varphi(v_1, v_1)}$. Observemos que en $F_2$, utilizando nuestra nueva base $B_2' = \{w_1, w_2)\}$ podemos escribir
	\begin{align*}
	M_{B_2'}(Q\vert_{F_2}) = \left(\begin{array}{cc}
		\lambda & 0 \\ 0 & \mu
		\end{array}\right) = P_2^T M_{B_2}(Q\vert_{F_2}) P_2
 	\end{align*}
 	donde sabemos que $\lambda > 0$ y por la conservación del determinante entre cambios de base podemos decir que $\lambda\mu > 0 \implies \mu > 0$.
 	
 	Ahora bien, si $n = 2$ ya habríamos acabado. En caso contrario, supongamos que hemos construido una base $B_r' = \{w_1, \dots, w_r\}$ de $F_r$ tal que
 	\begin{align*}
 	M_{B_r'}(Q\vert_{F_r}) = \left(\begin{array}{ccc}
 		\lambda_1 & 0 & 0 \\
 		0 & \ddots & 0 \\
 		0 & 0 & \lambda_r
 	\end{array}\right) = P_r^T M_{B_r}(Q\vert_{F_r}) P_r
	 \end{align*}
	 con $\lambda_1, \dots, \lambda_r > 0$. Ahora contruiremos $B_{r+ 1}'$. Sea $F_{r+1} = \langle v_1, \dots, v_r, v_{r+1}\rangle= \langle w_1, \dots, w_r, w_{r+1}\rangle$ donde $w_{r+1} = v_{r+1} - \mu_1w_1 - \mu_2 w_2 - \dots - \mu_r w_r$. Buscamos cada $\mu_i$ para que se cumpla $\varphi(w_{r+1}, w_i) = 0$ para $i = 1, \dots, r$. Como antes podemos obtener $\mu_i = \frac{\varphi(w_{r+1}, w_i)}{\varphi(w_i, w_i)}$, luego $B_{r+1}' = \{w_1, \dots, w_r, w_{r+1}\}$ es también una base de $F_{r+1}$ en la que
	 \begin{align*}
	 	M_{B_{r+1}'}(Q\vert_{F_{r+1}}) = \left(\begin{array}{cccc}
	 	\lambda_1 & 0 & 0 & 0\\
	 	0 & \ddots & 0 & 0\\
	 	0 & 0 & \lambda_r & 0 \\
	 	0 & 0 & 0 & \lambda_{r+1}
	 	\end{array}\right) = P_{r+1}^T M_{B_{r+1}}(Q\vert_{F_{r+1}}) P_{r+1}
	 \end{align*}
	 donde cada $\lambda_i > 0$.
	 
	 Conclusión: hemos probado por inducción que $\exists$ una base de $V$, $B_n' = \{w_1, \dots, w_n\}$ tal que la matriz $M_{B_n}(Q)$ se escribe en forma canónica y todos los coeficientes de la diagonal son positivos.
\end{proof}

\begin{tm}
	Sea $Q: V \to \mathbb{R}$ una forma cuadrática. Entonces $Q$ es definida negativa $\iff$ tanto el rango como el índice de inercia negativo es $n \iff \forall B$ base de $V,\ \det M_B(Q)_r$ alternan el signo: $\det M_B(Q)_1 < 0, \det M_B(Q)_2 > 0, \dots, \det M_B(Q)_r$ tiene el signo de $(-1)^r$.
\end{tm}

\begin{proof}
	Usando que $Q$ es definida negativa $\iff -Q$ es definida positiva $\iff \det M_B(Q)_r > 0,\ \forall r = 1, \dots, r$ y sabiendo que $\det M_B(Q)_r = (-1)^r\det M_B(Q)_r$ ya estaría.
\end{proof}

\chapter{Superficies cuádricas}

% TODO: faltan apuntes del 2017-12-11

En la referencia $R = \{(0,0,0); e_1, e_2, e_3\}$ de $\mathbb{A}_\mathbb{R}^3$ tenemos una ecuación del tipo $a_{11}x^2 + a_{22}y^2 + a_{33}z^2 + a_{12}xy + a_{13}xz + a_{23}yz + a_1x + a_2y + a_3z + a = 0$. Podemos representar los coeficientes de la parte principal (forma cuadrática asociada) con la matriz
\begin{align}
	A = \left(\begin{array}{ccc}
	a_{11} & a_{12}/2 & a_{13}/2 \\
	a_{12}/2 & a_{22} & a_{23}/2 \\
	a_{13}/2 & a_{23}/2 & a_{33}
	\end{array}\right)
\end{align}

Nos interesa ver los autovalores $\lambda_1, \lambda_2, \lambda_3$ de $Q$. Distinguimos casos dependiendo del rango de $Q$:
\begin{enumerate}
	\item Rango 3: $\lambda_1, \lambda_2, \lambda_3 ≠ 0$.
	\item Rango 2: $\lambda_1, \lambda_2 ≠ 0,\ \lambda_3 = 0$.
	\item Rango 1: $\lambda_1 ≠ 0,\ \lambda_2, \lambda_3 = 0$. $\lambda_1x_1^2 + b_1x_1 + b_2y_1 + b_3z_1 + b = 0$. Traslación: $\lambda_1x_2^2 + b_-2y_2 + b_3z_3 + b' = 0$. Ahora distinguimos más casos:
	\begin{enumerate}
		\item $b_2 = b_3 = 0$ con lo que nos queda $x_2^2 = \frac{-b'}{\lambda_1}$. En este casos pueden ocurrir varias cosas:
		\begin{enumerate}
			\item Si $\frac{-b'}{\lambda_1} < 0$ nos queda el vacío.
			\item Si $\frac{-b'}{\lambda_1} = 0$ nos quedan dos planos coincidentes $x_2 = 0$.
			\item Si $\frac{-b'}{\lambda_1} > 0$ nos quedan dos planos paralelos $x_2 = \pm \sqrt{\frac{-b'}{\lambda_1}}$.
		\end{enumerate}
		\item Si $b_2^2 + b_3^2 ≠ 0$ esto es, $b_2 ≠ 0$, o $b_3 ≠ 0$, o $b_2, b_3 ≠ 0$. Hacemos el cambio $x_2= x_3, y_2 = \frac{b_2y_3 + b_3z_3}{\sqrt{b_2^2 + b_3^2}},\ z_2 = \frac{b_3y_3 - b_2z_3}{\sqrt{b_2^2 + b_3^2}}$ que es ortonormal. Lo podemos ver fijándonos en la matriz
		\begin{align}
			\left(\begin{array}{ccc}
			1 & 0 & 0 \\
			0 & \frac{b_2}{\sqrt{b_2^2 + b_3^2}} & \frac{b_3}{\sqrt{b_2^2 + b_3^2}} \\
			0 & \frac{b_3}{\sqrt{b_2^2 + b_3^2}} & \frac{- b_2}{\sqrt{b_2^2 + b_3^2}}
			\end{array}\right)
		\end{align}
		Obtenemos $\lambda_1x_3^2 + \left(\sqrt{b_2^2 + b_3^2}\right)y_3 + b' = 0$ que es la ecuación de un cilindro parabólico.
	\end{enumerate}
\end{enumerate}

En general hacemos siempre lo mismo
\begin{enumerate}
	\item Hacemos un cambio de variable para eliminar los términos cruzados y quedarnos con una matriz diagonal.
	\item Luego hacemos un cambio de base que nos permite eliminar los términos lineales que también aparecen al cuadrado. Esto equivale a hacer una traslación.
	\item me falta un paso
\end{enumerate}

Ahora que ya tenemos la clasificación, vamos a buscar alternativas que nos permitan clasificar cuádricas sin tener que hacer tantos cálculos, que muchas veces, como en el caso de hallar los autovalores, son imposibles.

\section{Invariantes para superficies cuádricas}
\begin{align*}
	a_{11}x^2 + a_{22}y^2 + a_{33}z^2 + a_{12}xy + a_{13}xz + a_{23}yz + a_1x + a_2y + a_3z + a = 0
\end{align*}
Podemos escribir la ecuación de arriba en forma matricial pensando que hay una cuarta variable que siempre es $1$:
\begin{align}
(x, y, z, 1)\bar{A}\left(\begin{array}{c}
x \\ y \\ z \\ 1
\end{array}\right) \\
(x, y, z, 1)\left(\begin{array}{cccc}
a_{11} & a_{12}/2 & a_{13}/2 & a_1/2 \\
a_{12}/2 & a_{22} & a_{23}/2 & a_2/2\\
a_{13}/2 & a_{23}/2 & a_{33} & a_3/2 \\
a_1/2 & a_2/2 & a_3/2 & a
\end{array}\right)\left(\begin{array}{c}
x \\ y \\ z \\ 1
\end{array}\right)
\end{align}
Para encontrar los autovalores haríamos $\det(\bar{A} - \lambda I_3) = -\lambda_3 + s_1\lambda_2 - s_2\lambda + \delta$. Vamos a ver qué relación tienen los coeficientes de ese polinomio con la matriz $\bar{A}$:
\begin{itemize}
	\item $s_1 = a_{11} + a_{22} + a_{33}$
	\item $s_2 = \det\left(\begin{array}{cc}
	a_{11} & a_{12}/2 \\
	a_{12}/2 & a_{22}
	\end{array}\right) + \det\left(\begin{array}{cc}
	a_{11} & a_{13}/2 \\
	a_{13}/2 & a_{33}
	\end{array}\right) + \det\left(\begin{array}{cc}
	a_{22} & a_{23}/2 \\
	a_{23}/2 & a_{33}
	\end{array}\right)$
	\item $\delta = \det A$ (no $\det \bar{A}$).
\end{itemize}

Utilizaremos los siguientes resultados para hacer esto: 
\begin{tm}
	Los números $s_1, s_2, \delta$ y $\det \bar{A} = \Delta$ no cambian por cambios de BON.
\end{tm}

\begin{tm}[Regla de descartes de los signos]
	Sea $p(x) = a_nx^n + a_{n - 1}x^{n-1} + \dots + a_1x + a_0 \in \mathbb{R}[x]$. Si todas la raíces del polinomio son reales y no nulas, entonces el número de raíces positivas de $p(x)$ es igual al número de cambios de signo en la sucesión $a_n, a_{n - 1}, \dots, a_1, a_0$ donde no incluimos los términos nulos.
\end{tm}

\begin{dfn}[Signatura de un polinomio]
	Sea $p(x) = a_nx^n + a_{n - 1}x^{n-1} + \dots + a_1x + a_0 \in \mathbb{R}[x]$, definimos la signatura de $p(x)$ como el valor absoluto del número de raíces positivas menos el número de raíces negativas:
	\begin{align*}
		\text{signatura }p(x) = | \text{nº raíces positivas} - \text{nº raíces negativas}|
	\end{align*}
\end{dfn}

\begin{tm}
	Cuando $\delta = \Delta = 0$,
	\begin{align*}
		s_3 = \left|\begin{array}{ccc}
		a_{22} & a_{23}/2 & a_2/2 \\
		a_{23}/2 & a_{33} & a_3/2 \\
		a_{2}/2 & a_3/2 & a
		\end{array}\right| + \left|\begin{array}{ccc}
		a_{11} & a_{13}/2 & a_1/2 \\
		a_{13}/2 & a_{33}  & a_3/2 \\
		a_{1}/2 & a_3/2 & a
		\end{array}\right| + \left|\begin{array}{ccc}
		a_{11} & a_{12}/2 & a_1/2 \\
		a_{12}/2 & a_{22}  & a_2/2 \\
		a_{1}/2 & a_2/2 & a
		\end{array}\right|
	\end{align*}
	es invariante por cambios de BON.
\end{tm}

\hrule
Distinguiremos los siguientes casos
\begin{enumerate}
	\item Rango 3: $\lambda_1, \lambda_2, \lambda_3 ≠ 0 \iff \delta ≠ 0$, es decir que hemos llegado a la siguiente situación: $\lambda_1 x^2 + \lambda_2 y^2 + \lambda_3 z^2 = D$. Lo que nos dice el teorema es que podemos obtener $s_1, s_2, \delta$ utilizando esta nueva forma. La nueva matriz $\bar{A}$ tendría esta forma:
	\begin{align*}
		\bar{A}' = \left(\begin{array}{cccc}
		\lambda_1 &0 & 0 & 0 \\
		0 & \lambda_2 & 0 & 0 \\
		0 & 0 & \lambda_3 & 0 \\
		0 & 0 & 0 & -D
		\end{array}\right)
	\end{align*}
	donde $\Delta = -D\lambda_1\lambda_2\lambda_3$ y por tanto $\lambda_1x^2 + \lambda_2y^2 + \lambda_3z^2 = \frac{-\Delta}{\delta}$. De esta manera podemos distinguir subcasos en función de $\Delta$:
	\begin{enumerate}
		\item Si $\Delta ≠ 0$ distinguimos aún más casos en función del signo de $\Delta$ y la signatura de XXXX:
		\begin{enumerate}
			\item Si $\Delta < 0$ con signatura 3. Tenemos un elipsoide $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$
			\item Si $\Delta > 0$ con signatura 1, $\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 1$
			\item Si $\Delta < 0$ con signatura 1, $\frac{x^2}{a^2} - \frac{y^2}{b^2} - \frac{z^2}{c^2} = 1$
			\item Si $\Delta > 0$  con signatura 3, tenemos el conjunto vacío $-\frac{x^2}{a^2} - \frac{y^2}{b^2} - \frac{z^2}{c^2} = 1$
		\end{enumerate}
		\item Si $\Delta = 0$, distinguimos entre 3 casos aunque el primero y el último son similares.
		\begin{enumerate}
			\item Con signatura 3, estamos ante un punto, $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 0$
			\item cons signatura 1, estamos ante un cono, $\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 0$
			\item Con signatura 3, estamos ante un punto, $-\frac{x^2}{a^2} - \frac{y^2}{b^2} - \frac{z^2}{c^2} = 0$
		\end{enumerate}
	\end{enumerate}
	\item Rango 2: $\lambda_1, \lambda_2 ≠ 0,\ \lambda_3 = 0 \iff \delta = 0$.\footnote{Ojo que $\delta = 0$ no es una condición suficiente para garantizar que estamos en el caso 2 2, puesto que también aparece en el caso 3 y viceversa. Para el caso 2 podemos hacer lo siguiente $\lambda_1 x^2 + \lambda_2 y^2 + b_3z = c$. Utilizando el teorema anterior obtenemos $s_2 = \lambda_1\lambda_2$ de manera que ya tenemos una condición suficiente para decir que estamos en este caso: $\delta = 0$ y $s_2 = \lambda_1\lambda_2 ≠ 0$.} Es decir, $\delta = 0$ y $s_2 = \lambda_1\lambda_2 ≠ 0$. Tenemos que distinguir ahora entre dos subcasos. Lo hacemos calculando $\Delta = -\lambda_1\lambda_2\frac{b_3^2}{4}$ (lo hacemos a partir de la matriz $\bar{A}$).
	\begin{enumerate}
		\item $b_3 ≠ 0 \iff \Delta ≠ 0$ de la que podemos distinguir dos casos más:
		\begin{enumerate}
			\item $s_2 > 0$, que nos da un paraboloide $\frac{x^2}{a^2} + \frac{y^2}{b^2} = z$
			\item $s_2 < 0$, que nos da $\frac{x^2}{a^2} - \frac{y^2}{b^2} = z$
		\end{enumerate}
		\item $b_3 = 0 \iff \Delta = 0$
		\begin{enumerate}
			\item $s_2 > 0$, $\frac{x^2}{a^2} + \frac{y^2}{b^2} = c$
			\begin{enumerate}
				\item Si $c = 0$, es decir cuando $s_3 = 0$, tenemos una recta,
				\item si $c > 0$, es decir cuando $s_3 < 0$, tenemos un cilindro elíptico, y 
				\item si $c < 0$, es decir cuando $s_3 > 0$, tenemos el vacío.
			\end{enumerate}
			\item $s_2 < 0$, $\frac{x^2}{a^2} - \frac{y^2}{b^2} = c$
			\begin{enumerate}
				\item Si $c = 0$, es decir $s_3 = 0$, tenemos dos planos secantes, y
				\item si $c ≠ 0$, es decir $s_3 ≠ 0$, tenemos un cilindro hiperbólico.
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

% --------------------------------------------------------

\chapter{Apéndice A: exámenes resueltos}
\section{Segundos parciales}
\subsection{Noviembre 2017 - Ana Bravo}
Jueves, 19 de octubre de 2017. Espacios vectoriales euclídeos y unitarios; espacio afín.
\paragraph{Problema 1.}
Decide si son verdaderas o falsas las siguientes afirmaciones:

\subparagraph{(a) (5 puntos)} En $\mathbb{A}_\mathbb{R}^4$ hay dos planos $\pi_1$ y  $\pi_2$ que se intersecan exactamente en un punto.

\textbf{VERDADERO:} Sean $\pi_1 \equiv P + \langle e_1 = (1, 0, 0, 0), e_2 = (0, 1, 0, 0) \rangle, \pi_2 \equiv P + \langle e_3 = (0,0,1,0), e_4 = (0,0,0,1) \rangle$. Sea $A \in \mathbb{A}^4_\mathbb{R}$. Sabemos que $A \in \pi \iff \overrightarrow{PA} \in \langle u, v \rangle$. Por tanto tiene que cumplirse que para un punto $A = (x, y, z, w)$:
\begin{align*}
\begin{cases}
	\overrightarrow{PA} \in \langle e_1, e_2 \rangle \\
	\overrightarrow{PA} \in \langle e_3, e_4 \rangle
\end{cases}	\iff
\begin{cases}
(x, y, z, t) = \lambda_1 e_1  + \lambda_2 e_2\\
(x, y, z, t) = \lambda_3 e_3  + \lambda_4 e_4
\end{cases} \iff
\begin{cases}
z = w = 0 \\
x = y = 0
\end{cases}
\end{align*}

\subparagraph{(b) (5 Puntos)} Si $S_1, S_2 : \mathbb{R}^2 \to \mathbb{R}^2$ son dos simetrías ortogonales respecto a dos rectas distintas, entonces $S_1 \circ S_2$ es una rotación.

\paragraph{Problema 2.} En $\mathbb{A}_\mathbb{R}^2$ se consideran los puntos $Q_0 = (1, 0),\ Q_1 = (1,1),\ Q_2 = (0,1)$.

\subparagraph{(a) (3 puntos)} Demuestra que $S = \{Q_0, Q_1, Q_2\}$ es una referencia baricéntrica.
\subparagraph{(b) (3 puntos)} Sea $R = \{P_0 = (0,0); e_1 = (1, 0), e_2 = (0,1)\}$ la referencia cartesiana estándar de $\mathbb{A}_\mathbb{R}^2$ y sea $S' = \{Q_0; \overrightarrow{Q_0 Q_1}, \overrightarrow{Q_0 Q_2}\}$. Escribe las ecuaciones de cambio de coordenadas cartesianas de $S'$ a $R$.
\subparagraph{(c) (4 puntos)} Calcula las coordenadas baricéntricas del punto $T = (2,2)$ respecto a $S$.

\paragraph{Problema 3.} Consideramos en $V = \mathbb{R}^3$ el producto escalar usual. Sea $B = \{e_1, e_2, e_3\}$ la base estándar, que consideramos positivamente orientada, y sea $f : \mathbb{R}^3 \to \mathbb{R}^3$ la aplicación lineal cuya matriz respecto a $B$ es
\begin{align*}
	\left(\begin{array}{ccc}
	2/3 & 1/3 & 2/3 \\
	1/3 & 2/3 & -2/3 \\
	2/3 & -2/3 & -1/3
	\end{array}\right)
\end{align*}

\subparagraph{(a) (3 puntos)} Demuestra que $f$ es ortogonal.
\subparagraph{(b) (2 puntos)} Decide de manera razonada si $f$ conserva la orientación
\subparagraph{(c) (5 puntos)} Clasifica $f$ y determina sus elementos geométricos.
%\renewcommand{\listtheoremname}{Lista de definiciones}
%\listoftheorems[ignoreall,onlynamed={dfn}]
%\renewcommand{\listtheoremname}{Lista de teoremas}
%\listoftheorems[ignoreall,onlynamed={tm,pro}]

\end{document}
